[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Preface\nThese are notes taken while reading Statistical Rethinking by Richard McElreath and watching his lectures. Most of the code is sourced or adapted from Solomon Kurz’s book which translates the {rethinking} code used by McElreath to {brms}.\nThese notes were transferred from Evernote to Quarto, so some of notes may be difficult to read as the editing of the note format is still an ongoing process.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html",
    "href": "qmd/chapter-summaries.html",
    "title": "Chapter Summaries",
    "section": "",
    "text": "Chapter 2",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch2",
    "href": "qmd/chapter-summaries.html#sec-summ-ch2",
    "title": "Chapter Summaries",
    "section": "",
    "text": "Using counts\n\nGarden of forking paths - all the potential ways we can get our sample data (sequence of marbles drawn from a bag) given a hypothesis is true (e.g. 1 blue, 3 white marbles in a bag)\n“Conjectures” are potential outcomes (1 blue and 3 white marbles in a bag) and each conjecture is a path in the garden of forking paths\nWith data, we count the number of ways (forking paths) the data is consistent with each conjecture (aka likelihood)\nWith new data, old counts become a prior, and updated total path counts = prior * new path counts (i.e. prior * likelihood)\n\nAs components of a model\n\nThe conjecture (aka parameter value), p,  with the most paths is our best guess at the truth. They’re converted to probabilities (i.e. probability of a parameter value) and now called “relative plausibilities”.\nThe likelihood is a function (e.g. dbinom) that gives the probability of an observation given a parameter value (conjecture)\nPrior probabilities (or relative plausibilities) must be assigned to each unknown parameter.\nThe updated relative plausibility of a conjecture, p, is the posterior probability\nposteriorp1 = (priorp1 * likelihoodp1) / sum(all prior*likelihood products for each possible value of parameter, p)\n\nThe denominator normalizes the updated plausability so that the sum of  updated plausabilities for all the parameter values is 1 (necessary to formally be a proability density)\n\n\ndbinom(x, size, prob) - probability density function for the binomial distribution\n\nx = # of observations of the event (e.g. hitting water on the globe)\nsize = sample size (N) (number of tosses)\nprob = parameter value (conjecture)(i.e. hypothesized proportion of water on the earth) (p)",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch3",
    "href": "qmd/chapter-summaries.html#sec-summ-ch3",
    "title": "Chapter Summaries",
    "section": "Chapter 3 - Sampling",
    "text": "Chapter 3 - Sampling\n\nSampling from the posterior\n\nrbinom - random variable generator of the binomial distribution\n\nSummarizing the posterior - mean, median, MAP, HPDI\nposterior prediction distribution\n\nGOF Question: If we were to make predictions about the most probable p using this model, how consistent is our model with the data?\n\nAnswer: if the shape of the PPD matches the shape of the sampled posterior distribution, then the model is consistent with the data. (i.e. good fit)",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch4",
    "href": "qmd/chapter-summaries.html#sec-summ-ch4",
    "title": "Chapter Summaries",
    "section": "Chapter 4 - Linear Models",
    "text": "Chapter 4 - Linear Models\n\nThe posterior distribution is the probability density of every combination of all the parameter values\n\nposterior distribution: after considering every possible combination of the parameters, it’s the assigned relative plausibilities to every combination, conditional on this model and these data. (from Ch.6)\n\nThe posterior is the joint distribution of all combinations of the parameters at the same time, Pr(parameters|outcome, predictors)\n\nMany posterior distributions are approximately gaussian/multivariate gaussian\nExample: intercept model\n\nit’s a density of every combination of value of mean and sd\n\nExample: Height ~ Weight\n\nThe posterior is Pr(α, β , σ | H, W)\n\nwhich is proportional to Normal(W|μ,σ) ⨯ Normal(α|178,100) ⨯ LogNormal(β|0,1) ⨯ Uniform(σ|0,10)\n\n\n\nIntro to priors\nModel Notation\n\nExample single variable regression where height is the response and weight the predictor\n\nhi ~ Normal(μ, σ)  # response\nμi = α + βxi  # response mean (deterministic, i.e. no longer a parameter)\nα ~ Normal(178, 100) # intercept\nβ ~ Normal(0, 10) # slope\nσ ~ Uniform(0, 50) # response s.d.\nparameters are α, β, and σ\n\n\nCentering/standardization of predictors can remove correlation between parameters\n\nWithout this transformation, parameters and their uncertainties will co-vary within the posterior distribution\n\ne.g. high intercepts will often mean high slopes\n\nWithout independent parameters\n\nThey can’t be interpreted independently\nEffects on prediction aren’t independent\n\n\nCIs, PIs\nbasis splines (aka b-splines)",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch5",
    "href": "qmd/chapter-summaries.html#sec-summ-ch5",
    "title": "Chapter Summaries",
    "section": "Chapter 5 - Multi-Variable Linear Models",
    "text": "Chapter 5 - Multi-Variable Linear Models\n\nModel Notation\n\nDivorceRatei ~ Normal(μi, σ)\nμi = α + β1MedianAgeMarriage_si + β2MarriageRate_si\nα ~ Normal(10, 10)\nβ1 ~ Normal(0, 1)\nβ2 ~ Normal(0, 1)\nσ ~ Uniform(0, 10)\n\nInterpretation\nDAGs\nInferential Plots\n\npredictor residual, counter-factual, and posterior prediction\n\nmasking\n\ncorrelation between predictors and opposite sign correlation of each with the outcome variable can lead increased estimated effects in a multi-regression as compared to individual bivariable regressions\n\ncategorical variables (not ordinals)\n\nUsing an index variable is preferred to dummy variables\n\nthe index method allows the priors for each category to have the same uncertainty\n\nno intercepts used in the model specifications\nContrasts",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch6",
    "href": "qmd/chapter-summaries.html#sec-summ-ch6",
    "title": "Chapter Summaries",
    "section": "Chapter 6 - Colliders, Multicollinearity, Post-Treatment Bias",
    "text": "Chapter 6 - Colliders, Multicollinearity, Post-Treatment Bias\n\nmulticollinearity\n\nConsequences\n\nthe posterior distribution will seem to suggest that none of the collinear variables is reliably associated with the outcome, even if all of the collinear variables are in reality strongly associated with the outcome.\n\nThe posterior distributions of the parameter estimates will have very large spreads (std.devs)\ni.e. parameter mean estimates shrink and their std.devs inflate as compared to the bivariate regression results.\n\npredictions won’t be biased but interpretation of effects will be impossible\n\nsolutions\n\nThink causally about what links the collinear variables and regress using that variable instead of the collinear ones\nUse data reduction methods\n\n\npost-treatment bias\n\nmistaken inferences arising from including variables that are consequences of other variables\n\ni.e. the values of the variable are a result after treatment has been applied\ne.g. using presence of fungus as a predictor even though it’s value is determined after the anti-fungus treatment has been applied\n\nConsequence: it can mask or unmask treatment effects depending the causal model (DAG)\n\ncollider bias\n\nWhen you condition on a collider, it induces statistical—but not necessarily causal— associations.\nConsequence:\n\nThe statistical correlations/associations are present in the data and may mislead us into thinking they are causal.\n\nAlthough, the variables involved may be useful for predictive models as the backdoor paths do provide valid information about statistical associations within the data.\n\nDepending on the causal model, these induced effects can be inflated\n\nA more complicated demonstration of Simpson’s Paradox (see My Appendix)\n\nApplications of Backdoor Criterion\n\nSee Causal Inference &gt;&gt; Statistical Rethinking\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch7",
    "href": "qmd/chapter-summaries.html#sec-summ-ch7",
    "title": "Chapter Summaries",
    "section": "Chapter 7 - Information Theory, Prediction Metrics, Model Comparison",
    "text": "Chapter 7 - Information Theory, Prediction Metrics, Model Comparison\n\nRegularizing prior (type of skeptical prior)\n\nTypicall priors with smaller sd values\nFlat priors result in a posterior that encodes as much of the training sample as possible. (i.e. overfitting)\nWhen tuned properly, reduces overfitting while still allowing the model to learn the regular features of a sample.\n\nToo skeptical (i.e. sd too small) results in underfitting\n\n\nInformation Entropy\n\nThe uncertainty contained in a probability distribution is the average log-probability of an event.\n\nKullback-Leibler Divergence (K-L Divergence)\n\nThe additional uncertainty induced by using probabilities from one distribution to describe another distribution.\n\nLog Pointwise Predictive Density (lppd)\n\nSum of the log average probabilities\n\nlarger is better\n\nThe log average probability is an approximation of information entropy\n\nDeviance\n\n-2*lppd\n\nsmaller is better\n\nAn approximation for the K-L divergence\n\nPredictive accuracy metrics Can’t use any information criteria prediction metrics to compare models with different likelihood functions\n\nPareto-Smoothed Importance Sampling Cross-Validation (PSIS)\n\nEstimates out-of-sample LOO-CV lppd\n\nloo pkg\n\n“elpd_loo” - larger is better\n“looic” - is just (-2 * elpd_loo) to convert it to the deviance scale, therefore smaller is better\n\nRethinking pkg: smaller is better\n\nWeights observations based on influence on the posterior\nUses highly influential observations to formulate a pareto distribution and sample from it(?)\n\nWidely Applicable Information Criterion (WAIC)\n\nDeviance with a penalty term based on the variance of the outcome variable’s observation-level log-probabilities from the posterior\nEstimates out-of-sample deviance\n\nloo pkg:\n\n“elpd_waic”: larger is better\n“waic”:  is just (-2 * elpd_waic) to convert it to deviance scale, therefore smaller is better\n\nRethinking pkg: smaller is better\n\n\nBayes Factor\n\nThe ratio (or difference when logged) of the average likelihoods (the denominator of bayes theorem) of two models.\n\n\nModel Comparison\n\nTo judge whether two models are “easy to distinguish” (i.e. kinda like whether their scores are statistically different), we look at the differences between the model with the best WAIC and the WAICs of the other models along with the standard error of the difference of the WAIC scores\nLeave-one-out cross-validation (LOO-CV)\n\nHas serious issues, I think (see Vehtari paper for recommendations, (haven’t read it yet))\n\n\nOutliers\n\nDetection - High p_waic (WAIC) and k (PSIS) values can indicate outliers\nSolutions\n\nMixture Model\nRobust Regression using t-distribution for outcome variable. As shape parameter, v, approaches 1+, tails become thicker.",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch8",
    "href": "qmd/chapter-summaries.html#sec-summ-ch8",
    "title": "Chapter Summaries",
    "section": "Chapter 8 - Interactions",
    "text": "Chapter 8 - Interactions\n\ncontinuous:categorical interaction\n\nCoded similarly to coding categoricals (index method)\n\ncontinuous:continuous interaction\n\nCoded very similar to the traditional R formula\nInteraction prior is the same as the variables used in the interaction\n\nPlots\n\nA counterfactual plot can be used to show the reverse of the typical interaction interpretation (i.e. association of continuous predictor conditioned on the categorical)\nTriptych plots are a type of facetted predictor (one of the interaction variables) vs fitted graph where you facet by bins, quantiles, levels of the other interaction variable",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch9",
    "href": "qmd/chapter-summaries.html#sec-summ-ch9",
    "title": "Chapter Summaries",
    "section": "Chapter 9 - MCMC",
    "text": "Chapter 9 - MCMC\n\nGibbs\n\nOptimizes sampling the joint posterior density by using conjugate priors\nInefficient for complex models\nCan’t discern bad chains as well as HMC\n\nHamiltonian Monte Carlo (HMC)\n\nUses Hamiltonian differential equations in a particle physics simulation to sample the joint posterior density\n\nMomentum and direction are randomly chosen\n\nHyperparameters\n\nUsed to reduce autocorrelation of the sampling (sampling is sequential) (U-Turn Problem)\nDetermined during warm-up in HMC\n\nStan uses NUTS2\n\nLeapfrog steps (L) - paths between sampled posterior value combinations are made up of leapfrog steps\nStep Size (ε) - The length of a leapfrog step is the step size\n\n\nDiagnostics\n\nEffective Sample Size (ESS) - Measures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample\n\nBulk_ESS - effective sample size around the bulk of the posterior (i.e. around the mean or median)\n\nWhen value is much lower than the actual number of iterations (minus warmup) of your chains, it means the chain is inefficient, but possibly still okay\n\nTail_ESS - effective sample size in the tails of the posterior\n\nNo idea what’s good here.\n\n\nRhat (Gelman-Rubin convergence diagnostic) - estimate of the convergence of Markov chains to the target distribution\n\nIf converges, Rhat = 1+\nIf value is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples.\n\nEarly versions of this diagnostic can fail for more complex models (i.e. bad chains even when value = 1)\n\n\nTrace Plots\n\nMulti-line plot depicting the sampling of parameter values in the joint posterior\nlazy, fat caterpillars = good chains\nNot recommended since 1 pathological chain can remain hidden in the plot\n\nTrank plots\n\nA layered histogram method that is easier to discern each chain’s health than using trace plots\n\n\nSet-up\n\nWarm-up samples\n\nMore complex models require more warm-up\nStart will default and adjust based on ESS values\n\nPost-Warmup samples\n\n200 for mean estimates using not-too-complex regression models\nMuch moar required for\n\nComplex models\nFiner resolution of the tails\nNon-Gaussian distributions\n\n\nChains\n\ndebugging: 1\n\nSome stan errors only display when 1 chain is used\n\nValidation of chains: 3 or 4\nFinal Run: only need 1 but can use more depending on compute power/# of vCPUs\n\n\nProblems with ugly chains in trace/trank plots\n\nSolutions for the 2 examples were to use weakly informative priors ¯\\_(ツ)_/¯",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch10",
    "href": "qmd/chapter-summaries.html#sec-summ-ch10",
    "title": "Chapter Summaries",
    "section": "Chapter 10 - GLM Concepts",
    "text": "Chapter 10 - GLM Concepts\n\nThe principle of maximum entropy provides an empirically successful way to choose likelihood functions. Information entropy is essentially a measure of the number of ways a distribution can arise, according to stated assumptions. By choosing the distribution with the biggest information entropy, we thereby choose a distribution that obeys the constraints on outcome variables, without importing additional assumptions. Generalized linear models arise naturally from this approach, as extensions of the linear models in previous chapters.\nThe maximum entropy distribution is the one with the greatest information entropy (i.e. log number of ways per event) and is the most plausible distribution.\n\nNo guarantee that this is the best probability distribution for the real problem you are analyzing. But there is a guarantee that no other distribution more conservatively reflects your assumptions.\nmaximum entropy also provides a way to generate a more informative prior that embodies the background information, while assuming as little else as possible.\n\nOmitted variable bias can have worse effects with GLMs\nGaussian\n\nA perfectly uniform distribution would have infinite variance, in fact. So the variance constraint is actually a severe constraint, forcing the high-probability portion of the distribution to a small area around the mean.\nThe Gaussian distribution gets its shape by being as spread out as possible for a distribution with fixed variance.\nThe Gaussian distribution is the most conservative distribution for a continuous outcome variable with finite variance.\n\nThe mean µ doesn’t matter here, because entropy doesn’t depend upon location, just shape.\n\n\nBinomial\n\nBinomial distribution has the largest entropy of any distribution that satisfies these constraints:\n\nonly two unordered events (i.e. dichotomous)\nconstant expected value (i.e. exp_val = sum(prob*num_events))\n\nIf only two un-ordered outcomes are possible and you think the process generating them is invariant in time—so that the expected value remains constant at each combination of predictor values— then the distribution that is most conservative is the binomial.\n\nOther distributions",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-summaries.html#sec-summ-ch11",
    "href": "qmd/chapter-summaries.html#sec-summ-ch11",
    "title": "Chapter Summaries",
    "section": "Chapter 11",
    "text": "Chapter 11\n\nLogistic Regression models a 0/1 outcome and data is at the case level\n\nExample: Acceptance, A; Gender, G; Department, D Ai ~ Bernoulli(pi) logit(pi) = α[Gi, Di]\n\nBinomial Regression models the counts of a Bernoulli variable that have been aggregated by some group variable(s)\n\nExample: Acceptance counts that have been aggregated by department and gender Ai ~ Binomial(Ni, pi) logit(pi) = α[Gi, Di]\n\nResults are the same no matter whether you choose to fit a logistic regression with case-level data or aggregate the case-level data into counts and fit a binomial regression\nbrms models\n\nLogistic\n\nfamily = bernouilli\nformula: outcome_var|trials(1)\n\nBinomial:\n\nfamily = binomial\nformula\n\nbalanced: outcome_var|trials(group_n)\nunbalanced: outcome_var|trials(vector_with_n_for_each_group)\n\n\n\nrstanarm models specified just like using glm\nFlat Normal priors for logistic or binomial do NOT have high sds. High sds say that outcome event probability is always close to 0 or close 1.\n\nFor flat intercept: sd = 1.5\nFor flat slope: sd = 1.0\nSee Bayes, priors for details on other options\n\nEffects\n\nTypes\n\nAbsolute Effects - The effect of a (counter-factual) change in predictor value (type of treatment) has on the outcome (probability of an event)\n\nContrast of the predicted values (e.g. marginal means)\n\nRelative Effects - The effect of a (counter-factual) change in predictor value (type of treatment) has on the outcome (odds of an event)\n\n\nUC Berkeley gender discrimination analysis\n\nTypical pipe DAG for many social science analyses where unobserved confounders are often an issue\nAlso see\n\nCausal Inference &gt;&gt; Misc &gt;&gt; Partial Identification\nCausal Inference &gt;&gt; Mediation Analysis\n\n\nPoisson - when N is very large and probability of an event, p, is very small, then expected value and variance are approximately the same. The shape of this distribution is the Poisson\nFlat Normal priors for Poisson also do NOT have high sds\n\nNot sure if these are standard flat priors, but the priors in the example were\n\nIntercept sd = 0.5\nSlope sd = 0.2",
    "crumbs": [
      "Chapter Summaries"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html",
    "href": "qmd/chapter-2.html",
    "title": "Chapter 2",
    "section": "",
    "text": "Counting the Ways",
    "crumbs": [
      "Chapter 2"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html#sec-ch2-count",
    "href": "qmd/chapter-2.html#sec-ch2-count",
    "title": "Chapter 2",
    "section": "",
    "text": "One blue marble is drawn from the bag and replaced. The bag is shaken, and a white marble is drawn and replaced. Finally, the bag is shaken, and a blue marble is drawn and replaced\n\n\nEach ring is a iid observation (bag shaken and a marble drawn and replaced)\nIn this example, the “garden of forking paths” is set of all potential draws (consisting of 3 observations), given the conjecture of there being 1 blue and 3 white marbles in the bag\n\nIf we actually do draw a marble, record the observation, replace the marble, repeat 2 more times, and the result is blue, white, blue, then these are the number of paths in each conjecture’s garden that are consistent with that outcome\n\n\nFor conjecture 1 blue, 3 white, (1, 3, 1) is the number of paths in each ring, respectively, that remain consistent with the sequence of recorded observations.\n\nWhen multiplied together, the product equals the total consistent paths.\n\n\nAfter the bag is shaken, a new marble is drawn (new data) — it’s blue. Previous counts are now the prior counts.\n\n\nThe ways this new blue marble can be drawn, given a conjecture, is used to update each prior count through multiplication.\n\nThis is equivalent to starting over and drawing another marble after the previous 3 iid observations.",
    "crumbs": [
      "Chapter 2"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html#sec-ch2-plaus",
    "href": "qmd/chapter-2.html#sec-ch2-plaus",
    "title": "Chapter 2",
    "section": "Plausibilities",
    "text": "Plausibilities\n\nThe plausibility of a conjecture (p1) is the (prior plausibility given p1) ⨯ (“New Count” given p1). Then, that product is standardized into a probability so that it is comparable to other conjectures.\n\nplausibilityp1 = (prior_plausibilityp1 ⨯ new_countsp1) / the sum of all (new_countspi ⨯ prior_plausibilitypi) products of the other conjectures\nIt’s the probability of the conjecture given the new data\n\nThe plausibility of a conjecture, p, after seeing new evidence, Dnew, is proportional to the ways the conjecture, p, can produce the new evidence, Dnew, times the prior plausibility of the conjecture, p.\n\nEquivalent Notations:\n\nPlausibility of p after Dnew ∝ ways p can produce Dnew ⨯ prior plausibility of p\nPlausibility of p after \\(D_{new} = \\frac {\\text{Ways} \\; p \\; \\text{can produce}\\ D_{new}\\ \\times\\ \\text{Prior plausibility}\\ p}{\\text{Sum of products}}\\)\n\nSum of products = sum of the WAYS of each conjecture. For\n\nConjecture 0 blues = 0 ways\nConjecture 1 blue   = 3 ways (current example)\nConjecture 2 blues = 8 ways\nConjecture 3 blues = 9 ways\nConjecture 4 blues = 0 ways\nTherefore sum of products = 20\n\nIf the prior plausibility of conjecture, p, of 1 blue marble = 1 (and the rest of the conjectures, i.e. flat prior), then plausibility of conjecture 1 blue = (3 ⨯ 1)/20 = 0.15. The plausibility calculation normalizes the counts to be between 0 and 1.",
    "crumbs": [
      "Chapter 2"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html#sec-ch2-bayescomp",
    "href": "qmd/chapter-2.html#sec-ch2-bayescomp",
    "title": "Chapter 2",
    "section": "In Bayesian Language",
    "text": "In Bayesian Language\n\nA conjectured proportion of blue marbles, p, is usually called a parameter value. It’s just a way of indexing possible explanations of the data\n\nIn the example below, the proportion of surface water is the unknown parameter, but the conjecture could also be other things like sample size, treatment effect, group variation, etc.\nThere can also be multiple unknown parameters for the likelihood to consider.\nEvery parameter must have a corresponding prior probability assigned to it.\n\nThe relative number of ways that a value p can produce the data is usually called a likelihood.\n\nIt is derived by the enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data (i.e. paths_consistent_with_data / total_paths).\nAs a model component, the likelihood is a function that gives the probability of an observation given a parameter value (conjecture)\n\n“How likely your sample data is out of all sample data of the same length?”\n\nExample: The proportion of water to land on the earth:\n\nW is distributed Binomially with N trials and a probability of p for W in each trial, \\(W \\sim \\mbox{Binomial}(N, p)\\)\n“The count of ‘water’ observations (globe is tossed and finger lands on water), W, is distributed binomially, with probability p of ‘water’ on each toss of a globe and N tosses in total.”\nNotation: \\(L(p \\ | \\ W, N)\\)\nAssumptions:\n\nObservations are independent of each other\nThe probability of observation of W (water) is the same for every observation\n\ndbinom(x, size, prob)\n\nFinds the probability (i.e. likelihood) of getting a certain number of successes (x) in a certain number of trials (size) where the probability of success on each trial is fixed (prob).\nArgs\n\nx = # of observations of water (W)\nsize = sample size (N) (number of tosses)\nprob = parameter value (conjecture)(i.e. hypothesized proportion of water on the earth) (p)\n\n\n\n\nThe prior plausibility of any specific p is usually called the prior probability.\n\nA distribution initial plausibilities for every value of a parameter\nExpresses prior knowledge about a parameter and constrains estimates to reasonable ranges\nUnless there’s already strong evidence for using a particular prior, multiple priors should be tried to see how sensitive the estimates are to the choice of a prior\nExample where the prior is a probability distribution for the parameter:\n\np is distributed Uniformly between 0 and 1, (i.e. each conjecture is equally likely), \\(p \\sim \\mbox{Uniform}(0, 1)\\)\n\nWeakly Informative or Regularizing priors: conservative; guards against inferences of strong association\n\nMathematically equivalent to penalized likelihood\n\n\nThe new, updated relative plausibility of a specific p is called the posterior probability.\nThe set of estimates, aka relative plausibilities of different parameter values, aka posterior probabilities, conditional on the data — is known as the posterior distribution or posterior density (e.g. \\(Pr(p \\ | \\ N, W)\\)).\nThoughts\n\nThe likelihood, prior, and posterior densities are probability densities each with an area = 1. Looking at the marble tables it looks like the individual posterior probabilities sum to 1. So, the sum (we’re talking densities so this “sum” = integration) of all the products of the multiiplication of the prior and likelihood densities must not have an area = 1. Therefore, the denominator (i.e. sum of products) then standardizes each of these products so the posterior density does have an area of 1.",
    "crumbs": [
      "Chapter 2"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html#sec-ch2-numsolv",
    "href": "qmd/chapter-2.html#sec-ch2-numsolv",
    "title": "Chapter 2",
    "section": "Numerical Solvers for the Posterior Distribution",
    "text": "Numerical Solvers for the Posterior Distribution\n\nGrid Approximation - compute the posterior distribution from only a portion of potential values (the grid of parameter values) for a set of unknown parameters\n\nDoesn’t scale well as the number of parameters grows\nSteps:\n\nDecide how many values you want to use in your grid (e.g. seq( from = 0, to = 1, len = 1000) )\n\nNumber of parameter values in your grid is equal to the number of points in your posterior distribution\n\nCompute the prior value for each parameter value in your grid (e.g. rep(1, 1000) , uniform prior)\nCompute the likelihood (e.g. using dbinom(x, size, p = grid)) for each grid value\nMultiply the likelihood times the prior which is the unstandardized posterior\nStandardize that posterior by dividing by sum(unstd_posterior)\n\n\nQuadratic approximation - the posterior distribution can be represented by the Gaussian distribution quite well. The log of a Gaussian (posterior) distribution is quadratic.\n\nSteps:\n\nFind the mode of the posterior. Uses quadratic approximation. With a uniform prior this is equivalent to MLE\nEstimate the curvature of the posterior using another numerical method\n\nNeeds larger sample sizes. How large is model dependent.\n{rethinking} function, quap( )\n\nInputs are likelihood function (e.g. dbinom) and prior function (e.g. punif), and data for the likelihood function\nOutputs mean posterior probability and the std dev of the posterior distribution\n\n\nMCMC only briefly mentioned",
    "crumbs": [
      "Chapter 2"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html",
    "href": "qmd/chapter-3.html",
    "title": "Chapter 3",
    "section": "",
    "text": "Posterior Density",
    "crumbs": [
      "Chapter 3"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html#sec-ch3-postdens",
    "href": "qmd/chapter-3.html#sec-ch3-postdens",
    "title": "Chapter 3",
    "section": "",
    "text": "Sampling from the posterior distribution means we can work with counts (easier, more intuitive) instead of working with the density which is working with integrals. Plus MCMC results are count based.\nSampling from the posterior density\nsamples &lt;-\n  sample(parameter values vector,\n         prob = posterior density from output of model,\n         size = # of samples you want,\n         replace = T)\n\nParameter values are the conjectures,\nPosterior is all the parameter values’ likelihood x prior (i.e. updated relative plausibilities of the conjectures)\n\nGrid Approximation of a posterior for 1 parameter:\np_grid &lt;- seq( from=0, to=1, length.out=1000) # parameter values vector\nprior &lt;- rep(1,1000) # repeat 1 a thousand times to create a flat prior\nlikelihood &lt;- dbinom(3, size = 3, prob=p_grid) # plausibility of 3 events out of 3 observations for each each conjectured parameter value\nposterior_unstd &lt;- likelihood * prior\nposterior &lt;- posterior_unstd / sum(posterior_unstd)\n\n# sampling the posterior density\nsamples &lt;- sample(p_grid, prob = posterior, size = 10000, replace = T)\n\n“size” is how many samples we’re taking\n(See stat.rethinking brms recode bkmk for ggplot graphs and dplyr code)\n\n\nsum() of posterior samples gives probability for a specified parameter value\nquantile() of posterior samples gives the parameter value for a specified probability\n\nCommon questions to ask about the posterior distribution\n\nHow much of the posterior probability lies below some parameter value?\n\nExample: Whats the posterior probability that the proportion of water on the globe is below 0.50?\nAnswer: sum(samples &lt; 0.50) / total_samples (= 1e4, see above)\n\n“/total sample” is the procedure if there are 1 or more parameters represented in the posterior density\n\n\nHow much of the posterior probability is between two parameter values?\n\nExample: The posterior probability that the proportion of water is between 0.50 and 0.75?\nAnswer: sum(samples &gt; 0.50 & samples &lt; 0.75) / total_samples\n\nWhich parameter value marks the lower 5% of the posterior probability?\n\ni.e. Which proportion of water has a 5% probability?\nAnswer: quantile(samples, 0.05)\n\nWhich range of parameter values contains the middle 80% of the posterior probability?\n\nDefined as the [Percentile Interval (PI)\n\nFrequentist CIs are normally percentile intervals, just of the sampling distribution instead of the posterior\nFine in practice for the most part, but bad for highly skewed posterior distributions as it’s not guaranteed to contain the most probable value.. In such cases use HPDI (see below).\n\nIf you do have an a posterior that’s highly skewed, make sure to also report the entire distribution.\n\n\n\nWhich range of proportions of water is the true value likely to be in with a 80% probability?\n\nAnswer: quantile(samples, c( 0.10, 0.90))\n\nWhich parameter value has the highest posterior probability?\n\nAnswer: the MAP (see below)",
    "crumbs": [
      "Chapter 3"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html#sec-ch3-summpost",
    "href": "qmd/chapter-3.html#sec-ch3-summpost",
    "title": "Chapter 3",
    "section": "Summarizing the Posterior",
    "text": "Summarizing the Posterior\n\nCredible Interval - General Bayesian term that is interchangeable with confidence interval. Instead an interval of probability density or mass, it’s based on an interval of the posterior probability. If choice of interval (percentile or hdpi) affects inferences being made, then also report entire posterior distribution.\nHighest Posterior Density Interval (HPDI) - The narrowest interval containing the specified probability mass. Guaranteed to have the value with the highest posterior probability.\n\n\nLeft: 50% percentile interval assigns equal mass (25%) to both the left and right tail. As a result, it omits the most probable parameter value, p = 1.\nRight: 50% HPDI finds the narrowest region with 50% of the posterior probability. Such a region always includes the most probable parameter value.\nDisadvantages:\n\nExpensive\nSuffers from Simulation Variance (i.e. sensitive to number of samples).\nLess familiar to most audiences\n\nSee Chapter 4 &gt;&gt; Uncertainty for brms code\n\nMaximum a posteriori (MAP) - value with the highest posterior probability, aka mode of the posterior.\nThe choice of type of point estimate (MAP, median, mean) should depend on a loss function (e.g. L1, squared –&gt; median, L2 aka quadratic –&gt; mean, etc).\n\nIf the loss is asymmetrical, e.g. cost rises exponentially as loss increases, then I think he’s saying that’s a good time to use MAP.",
    "crumbs": [
      "Chapter 3"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html#sec-ch3-dumdat",
    "href": "qmd/chapter-3.html#sec-ch3-dumdat",
    "title": "Chapter 3",
    "section": "Dummy Data",
    "text": "Dummy Data\n\nDummy Data are simulated data (aka fake data) to take the place of real data.\n\nUses\n\nChecking model fit and model behavior\nChecking the effectiveness of a research design (e.g. power analysis)\nForecasting\n\ndbinom(0:2, size = 2 , prob = 0.70) gives likelihoods for observing 0Ws, 1W, 2Ws for a trial with 2 flips given conjecture, p = 0.70\nrbinom(10, size = 2, prob = 0.70) is generating 10 data points given the conjecture (aka parameter value), p = 70%\n\n10 experiments (or trials) of flipping the globe twice\n\noutput being how many Ws were observed in each experiment\nthe output are dummy observations\n\nrbinom means its a binomial distribution, so only two possible outcomes in each trial\nsize = 2 means there are 2 flips per trial and means there are to be either 2 events, 1 event, or 0 events.\n\nIn the globe flipping Example where the event is landing on Water: 2 Water - 0 Land, 1 W - 1 L, 0 W - 2 L\n\nprob = 0.70 is probability (plausibility of an event) (e.g. landing on Water)\ntable(rbinom(100000, 2, 0.70) / 100000) is very similar to the output of dbinom above.\nIncreasing the size from 2 to 9 yields a distribution where the mode is around the true proportion (which would be 0.70 * 9 = 6.3 Ws)\n\n\nThe sampling distribution is still wide though, so the majority of experiments don’t result in the around 6 Ws.\nRepresents the “garden of forking paths” from Ch. 2",
    "crumbs": [
      "Chapter 3"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html#sec-ch3-ppd",
    "href": "qmd/chapter-3.html#sec-ch3-ppd",
    "title": "Chapter 3",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\n\nThe PPD is the (simulated) distribution of outcome values we expect to see in the wild given this model and these data.\nEquivalent to computing all the sampling distributions for each p and then averaging (by using the posterior distribution) them together (or integrating over the posterior density). This propagates the uncertainty about p of the posterior distribution into the PPD.\n\n(Intuitive) Steps in creating the PPD for the globe example\n\nA probability of hitting water (aka parameter value) is sampled from the posterior density (top pic)\n\nMore likely parameter values get sampled more often\n\nThat sampled probability is associated with a count distribution (aka sampling distribution histogram) (middle pic) where the x-axis values are possible total counts of the event after each trial.\n\nEach p’s sampling distribution is the distribution we’d expect to see if that p was true. (e.g. bell curve for p = 0.50)\ne.g. rbinom(10, size = 9, prob = 0.70) where the sampled probability is 0.70, trials = 10, and 9 is the number of globe tosses per trial\n\nTherefore, there’s a maximum of 9 events (i.e. hitting the water) possible per trial\n\n\nThis count distribution (middle pic) gets sampled, and that sampled count is tallied in the PPD (bottom pic)\n\ne.g. if a 6 is sampled from the count distribution, it’s tallied in the PPD for the “6” on the x-axis\n\nRepeat (e.g. 1e4 times)\n\nComputing a PPD:\n\nrbinom(number of trials, size = number of flips per trial, prob = samples from posterior density)\nThese are counts unlike the posterior density\n\ntherefore maybe the shape is dependent on “size”\n\nThe posterior distribution is used as weights to calculate a weighted, average frequency of W observations for each trial.\nExample: rbinom(10000, 9, samples)\n\nFor trial 1, a coin is flipped 9 times.\n\nThe total number of heads for each trial is determined by sampling the posterior density\n\nRepeat another 999 times\nA vector is returned where each value is the total number of heads for that trial (e.g. length in example = 10000)\n\n\nExample: PPD for tossing the globe\np_grid &lt;- seq( from=0, to=1, length.out=1000) # parameter values vector\nprior &lt;- rep(1,1000) # repeat 1 a thousand times to create a flat prior\nlikelihood &lt;- dbinom(6, size = 9, prob=p_grid) # plausibility of 3 events out of 3 observations for each each conjectured parameter value\nposterior_unstd &lt;- likelihood * prior\nposterior &lt;- posterior_unstd / sum(posterior_unstd)\n\nsamples &lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) # sample from posterior\n\nppd &lt;- rbinom(10000, size = 9, prob = samples) # simulate observations to get the PPD\nhist(ppd)\nExample: brms way\nb3.1 &lt;-\n  brm(data = list(w = 6), \n      family = binomial(link = \"identity\"),\n      w | trials(9) ~ 0 + Intercept,\n      # this is a flat prior\n      prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      iter = 5000, warmup = 1000,\n      seed = 3,\n      file = \"fits/b03.01\")\n\n# sampling the posterior\nf &lt;-\n  fitted(b3.1, \n        summary = F,            # says we want simulated draws and not summary stats\n        scale = \"linear\") %&gt;%    # linear outputs probabilities\n  as_tibble() %&gt;% \n  set_names(\"p\")\n\n# posterior probability density (top pic)\nf %&gt;% \n  ggplot(aes(x = p)) +\n  geom_density(fill = \"grey50\", color = \"grey50\") +\n  annotate(geom = \"text\", x = .08, y = 2.5,\n          label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                    breaks = c(0, .5, 1),\n                    limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n# ppd\nf &lt;-\n  f %&gt;% \n  mutate(w = rbinom(n(), size = n_trials,  prob = p))\n\n# ppd histogram (bottom pic)\nf %&gt;% \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"number of water samples\",\n                    breaks = seq(from = 0, to = 9, by = 3)) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\nSame process is used for a Prior Predictive Distribution\n\nThe prior distribution takes the place of the posterior density in this case\n\nFor a continuous distribution, see example in Chapter 4 &gt;&gt; Prior Predictive Distribution\nModel fit\n\n2 types of uncertainty:\n\nPredicted Observation Uncertainty\n\nEven if we did know the correct conjecture, p, we wouldn’t know with certainty the outcome of the next trial, unless the correct p = 0 or p = 1.\nSee histogram in previous section\n\nEven if the “true” conjecture (e.g. p = 0.70) is known, there is uncertainty in the count of W observations the next trial will yield.\n\nSounds like this is quantified by predictive intervals (PI) given some α\n\nUncertainty About p\n\nThe posterior distribution over p embodies this uncertainty.\nAnd this is quantified by credible intervals (CI) given some α\n\n\nGOF Question: If we were to make predictions about the most probable p using this model, how consistent is our model with the data?\n\nAnswer: If the shape of the PPD matches the shape of the sampled posterior distribution, then the model is consistent with the data. (i.e. good fit)",
    "crumbs": [
      "Chapter 3"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html",
    "href": "qmd/statistical-rethinking.html",
    "title": "1  Statistical Rethinking",
    "section": "",
    "text": "1.1 Chapter 2\nTOC\nSummaries\nQuestions\nGeneral Notes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-2",
    "href": "qmd/statistical-rethinking.html#chapter-2",
    "title": "1  Statistical Rethinking",
    "section": "",
    "text": "1 blue marble is drawn from the bag and replaced. The bag is shaken, and a white marble is drawn and replaced. Finally, the bag is shaken, and a blue marble is drawn and replaced\n\n\neach ring is a iid observation (bag shaken and a marble drawn and replaced)\nIn this Example, the “garden of forking paths” is set of all potential draws (consisting of 3 observations), given the conjecture of there being 1 blue and 3 white marbles in the bag\n\n\n\nIf we actually do draw a marble, record the observation, replace the marble, repeat 2 more times, and the result is blue, white, blue, then these are the number of paths in each conjecture’s garden that are consistent with that outcome\nFor conjecture 1 blue, 3 white,  1, 3, 1 is the number of paths in each ring, respectively, that remain consistent with the sequence of recorded observations.\n\nWhen multiplied together, the product equals the total consistent paths.\n\n\n\n\nAfter the bag is shaken, a new marble is drawn (new data) — it’s blue. Previous counts are now the prior.\nThe ways this new blue marble can be drawn, given a conjecture, is used to update each prior count through multiplication.\n\nThis is equivalent to starting over and drawing another marble after the previous 3 iid observations.\n\n\nThe plausibility of a conjecture, p1, is the prior plausibility given p1 * new count given p1 and then that product is standardized into a probability so that it is comparable.\n\nplausibilityp1 =  (new_countsp1 *prior_plausibilityp1) / sum of all the (prior*new) products of the other conjectures\nIt’s the probability of the conjecture given the new data\n\nThe plausibility of a conjecture, p, after seeing new evidence, Dnew, is proportional to the ways the conjecture, p, can produce the new evidence, Dnew, times the prior plausibility of the conjecture, p.\n\nEquivalents:\n\nplausibility of p after Dnew    ways p can produce Dnew x prior plausibility of p\n\n\nsum of products = sum of the WAYS of each conjecture. For Conjecture 0 blues = 0 ways, Conjecture 1 blue = 3 ways (see above), Conjecture 2 blues = 8 ways, Conjecture 3 blues = 9 ways, Conjecture of 4 blues = 0. Therefore sum of products = 20.\nif the prior plausibility of p for conjecture of 1 blue marble = 1 (and the rest of the conjectures, i.e. flat prior), then plausibility of conjecture 1 blue = (3*1)/20 = 0.15. The plausibility is a way to normalize the counts to be between 0 and 1.\n\n\n\n\nA conjectured proportion of blue marbles, p,  is usually called a parameter value. It’s just a way of indexing possible explanations of the data\n\nHere p, proportion of surface water, is the unknown parameter, but the conjecture could also be other things like sample size,  treatment effect, group variation, etc.\nThere can also be multiple unknown parameters for the likelihood to consider.\nEvery parameter must have a corresponding prior probability assigned to it.\n\nThe relative number of ways that a value p can produce the data is usually called a likelihood.\n\nIt is derived by the enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data  (i.e. paths_consistent_with_data / total_paths).\nAs a model component, the likelihood is a function that gives the probability of an observation given a parameter value (conjecture)\n\n“how likely your sample data is out of all sample data of the same length?”\n\nExample (the proportion of water to land on the earth):\n\nW is distributed Binomially with N trials and a probability of p for W in each trial,  \n“The count of ‘water’ observations (finger landing on water), W, is distributed binomially, with probability p of ‘water’ on each toss of a globe and N tosses in total.”\nL(p | W, N) is another likelihood notation\nAssumptions:\n\nObservations are independent of each other\nThe probability of observation of W (water) is the same for every observation\n\ndbinom(x, size, prob)\n\nx = # of observations of water (W)\nsize = sample size (N) (number of tosses)\nprob = parameter value (conjecture)(i.e. hypothesized proportion of water on the earth) (p)\n\n\n\nThe prior plausibility of any specific p is usually called the prior probability.\n\nA distribution initial plausibilities for every value of a parameter\nExpresses prior knowledge about a parameter and constrains estimates to reasonable ranges\nUnless there’s already strong evidence for using a particular prior, multiple priors should be tried to see how sensitive the estimates are to the choice of a prior\nExample where the prior is a probability distribution for the parameter:\n\np is distributed Uniformly between 0 and 1,  (i.e. each conjecture is equally likely)  \n\nWeakly Informative or Regularizing priors: conservative; guards against inferences of strong association\n\nmathematically equivalent to penalized likelihood\n\n\nThe new, updated relative plausibility of a specific p is called the posterior probability.\nThe set of estimates, aka relative plausibilities of different parameter values, aka posterior probabilities, conditional on the data — is known as the posterior distribution or posterior density (e.g. Pr(p | N, W)).\nThoughts\n\nThe likelihood, prior, and posterior densities are probability densities each with an area = 1. Looking at the marble tables it looks like the individual posterior probabilities sum to 1. So, the sum (we’re talking densities so this “sum” = integration actually) of all the products of the multiiplication of the prior and likelihood densities must not have an area = 1. Therefore, the denominator (i.e. sum of products) then standardizes each of these products so the posterior density does have an area of 1.\n\nNumerical Solvers for the posterior distribution:\n\nGrid Approximation - compute the posterior distribution from only a portion of potential values (the grid of parameter values) for a set of unknown parameters\n\nDoesn’t scale well as the number of parameters grows\nSteps:\n\nDecide how many values you want to use in your grid (e.g. seq( from = 0, to = 1, len = 1000) )\n\nnumber of parameter values in your grid is equal to the number of points in your posterior distribution\n\nCompute the prior value for each parameter value in your grid (e.g. rep(1, 1000) , uniform prior)\nCompute the likelihood (e.g. using dbinom(x, size, p = grid))  for each grid value\nMultiply the likelihood times the prior which is the unstandardized posterior\nStandardize that posterior by dividing by sum(unstd_posterior)\n\n\nQuadratic approximation - the posterior distribution can be represented by the Gaussian distribution quite well. The log of a Gaussian (posterior) distribution is quadratic.\n\nSteps:\n\nFind the mode of the posterior. Uses quadratic approximation. With a uniform prior this is equivalent to MLE\nEstimate the curvature of the posterior using another numerical method\n\nNeeds larger sample sizes. How large is model dependent.\nRethinking pkg function, quap( )\n\ninputs are likelihood function (e.g. dbinom) and prior function (e.g. punif), and data for the likelihood function\noutputs mean posterior probability and the std dev of the posterior distribution\n\n\nMCMC only briefly mentioned",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-3---sampling",
    "href": "qmd/statistical-rethinking.html#chapter-3---sampling",
    "title": "1  Statistical Rethinking",
    "section": "1.2 Chapter 3 - Sampling",
    "text": "1.2 Chapter 3 - Sampling\n\nSampling from the posterior distribution means we can work with counts (easier, more intuitive) instead of working with the density which is working with integrals. Plus MCMC results are count based.\n\nSampling from the posterior density: samples &lt;- sample(parameter values vector, prob = posterior density from output of model,  size = # of samples you want, replace = T)\n\nparameter values are the conjectures,\nposterior is their likelihood x prior,\n\nGrid Approximation for 1 parameter:\n\n\n\n\np_grid &lt;- seq( from=0, to=1, length.out=1000) # parameter values vector\nprior &lt;- rep(1,1000) # repeat 1 a thousand times to create a flat prior\nlikelihood &lt;- dbinom(3, size = 3, prob=p_grid) # plausibility of 3 events out of 3 observations for each each conjectured parameter value\nposterior_unstd &lt;- likelihood * prior\nposterior &lt;- posterior_unstd / sum(posterior_unstd)\n\n# sampling the posterior density\nsamples &lt;- sample(p_grid, prob = posterior, size = 10000, replace = T)\n\nsize is how many samples we’re taking \n(See stat.rethinking brms recode bkmk for ggplot graphs and dplyr code)\nCommon questions to ask about the posterior distribution\n\nHow much of the posterior probability lies below some parameter value?\n\nExample: Whats the posterior probability that the proportion of water on the globe is below 0.50?\nAnswer: sum(samples &lt; 0.50) / total_samples  (= 1e4, see above)\n\n“/total sample” is the procedure if there are 1 or more parameters represented in the posterior density\n\n\nHow much of the posterior probability is between two parameter values?\n\nExample: The posterior probability that  the proportion of water is between 0.50 and 0.75?\nAnswer: sum(samples &gt; 0.50 & samples &lt; 0.75) / total_samples\n\n** sum() of posterior samples gives probability for a specified parameter value **\n** quantile() of posterior samples gives the parameter value for a specified probability **\nWhich parameter value marks the lower 5% of the posterior probability?\n\ni.e. which proportion of water has a 5% probability?\nAnswer: quantile(samples, 0.05)\n\nWhich range of parameter values contains the middle 80% of the posterior probability?\n\nDefined as the Percentile Interval (PI)\n\nFrequentist CIs are normally percentile intervals, just of the sampling distribution instead of the posterior\nFine in practice for the most part, but bad for highly skewed posterior distributions as it’s not guaranteed to contain the most probable value.. In such cases use HPDI (see below).\n\nIf you do have an a posterior that’s highly skewed, make sure to also report the entire distribution.\n\n\n\nWhich range of proportions of water is the true value likely to be in with a 80% probability?\n\nAnswer: quantile(samples, c( 0.10, 0.90))\n\nWhich parameter value has the highest posterior probability?\n\nAnswer: the MAP (see below)\n\n\nCredible Interval - General Bayesian term that is interchangeable with confidence interval. Instead an interval of probability density or mass, it’s based on an interval of the posterior probability. If choice of interval (percentile or hdpi) affects inferences being made, then also report entire posterior distribution.\nHighest Posterior Density Interval (HPDI) - The narrowest interval containing the specified probability mass. Guaranteed to have the value with the highest posterior probability.\n\nLeft: 50% percentile interval assigns equal mass (25%) to both the left and right tail. As a result, it omits the most probable parameter value, p = 1.\nRight: 50% HPDI finds the narrowest region with 50% of the posterior probability. Such a region always includes the most probable parameter value.\nDisadvantages:\n\nExpensive\nSuffers from Simulation Variance (i.e. sensitive to number of samples).\nLess familiar to most audiences\n\nSee Ch 4. uncertainty section for brms code\n\nMaximum a posteriori (MAP) - value with the highest posterior probability, aka mode of the posterior.\nThe choice of type of point estimate (MAP, median, mean) should depend on a loss function (e.g. L1, squared –&gt; median, L2 aka quadratic –&gt; mean, etc).\n\nIf the loss is asymmetrical, e.g. cost rises exponentially as loss increases, then I think he’s saying that’s a good time to use MAP.\n\nDummy Data are simulated data (aka fake data) to take the place of real data.\n\nUses\n\nChecking model fit and model behavior\nChecking the effectiveness of a research design (e.g. power analysis)\nForecasting\n\ndbinom(0:2, size = 2 , prob = 0.70)  gives likelihoods for observing 0Ws, 1W, 2Ws for a trial with 2 flips given conjecture, p = 0.70\nrbinom(10, size = 2, prob = 0.70)  is generating 10 data points given the conjecture (aka parameter value), p = 70%\n\n10 experiments (or trials) of flipping the globe twice\n\noutput being how many Ws were observed in each experiment\nthe output are dummy observations\n\nrbinom means its a binomial distribution, so only two possible outcomes in each trial\nsize = 2 means there are 2 flips per trial and means there are to be either 2 events, 1 event, or 0 events.\n\nIn the globe flipping Example where the event is landing on Water: 2 Water - 0 Land, 1 W - 1 L, 0 W - 2 L\n\nprob = 0.70 is probability (plausibility of an event) (e.g. landing on Water)\ntable(rbinom(100000, 2, 0.70) / 100000)  is very similar to the output of dbinom above.\nIncreasing the size from 2 to 9 yields a distribution where the mode is around the true proportion (which would be 0.70 * 9 = 6.3 Ws) \n\nThe sampling distribution is still wide though, so the majority of experiments don’t result in the around 6 Ws.\nRepresents the “garden of forking paths” from Ch. 2\n\n\n\nPosterior Predictive Distribution (PPD)\n\nThe PPD is the (simulated) distribution of outcome values we expect to see in the wild given this model and these data.\nEquivalent to computing all the sampling distributions for each p and then averaging (by using the posterior distribution) them together (or integrating over the posterior density). This propagates the uncertainty about p of the posterior distribution into the PPD.\n(Intuitive) Steps in creating the PPD for the globe Example\n\nA  probability of hitting water (aka parameter value) is sampled from the posterior density (top pic)\n\nMore likely parameter values get sampled more often\n\nThat sampled probability is associated with a count distribution (aka sampling distribution histogram) (middle pic) where the x-axis values are possible total counts of the event after each trial.\n\nEach p’s sampling distribution is the distribution we’d expect to see if that p was true. (e.g. bell curve for p = 0.50)\ne.g. rbinom(10, size = 9, prob = 0.70) where the sampled probability is 0.70, trials = 10, and 9 is the number of globe tosses per trial\n\nTherefore, there’s a maximum of 9 events (i.e. hitting the water) possible per trial\n\n\nThis count distribution (middle pic) gets sampled, and that sampled count is tallied in the PPD (bottom pic)\n\ne.g. if a 6 is sampled from the count distribution, it’s tallied in the PPD for the “6” on the x-axis\n\nRepeat (e.g. 1e4 times)\n\nComputing a PPD:\n\nrbinom(number of trials, size = number of flips per trial, prob = samples from posterior density)   \nthese are counts unlike the posterior density\n\ntherefore maybe the shape is dependent on “size”\n\nthe posterior distribution is used as weights to calculate a weighted, average frequency of W observations for each trial.\nExample: rbinom(10000, 9, samples)\n\nFor trial 1, a coin is flipped 9 times.\n\nThe total number of heads for each trial is determined by sampling the posterior density\n\nRepeat another 999 times\nA vector is returned where each value is the total number of heads for that trial (e.g. length in Example = 10000)\n\n\nExample: PPD for tossing the globe\n\n\np_grid &lt;- seq( from=0, to=1, length.out=1000) # parameter values vector\nprior &lt;- rep(1,1000) # repeat 1 a thousand times to create a flat prior\nlikelihood &lt;- dbinom(6, size = 9, prob=p_grid) # plausibility of 3 events out of 3 observations for each each conjectured parameter value\nposterior_unstd &lt;- likelihood * prior\nposterior &lt;- posterior_unstd / sum(posterior_unstd)\n\nsamples &lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) # sample from posterior\n\nppd &lt;- rbinom(10000, size = 9, prob = samples) # simulate observations to get the PPD\nhist(ppd)\n\nExample: brms way\n\nb3.1 &lt;-\n  brm(data = list(w = 6), \n      family = binomial(link = \"identity\"),\n      w | trials(9) ~ 0 + Intercept,\n      # this is a flat prior\n      prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      iter = 5000, warmup = 1000,\n      seed = 3,\n      file = \"fits/b03.01\")\n\n# sampling the posterior\nf &lt;-\n  fitted(b3.1, \n        summary = F,            # says we want simulated draws and not summary stats\n        scale = \"linear\") %&gt;%    # linear outputs probabilities\n  as_tibble() %&gt;% \n  set_names(\"p\")\n\n# posterior probability density (top pic)\nf %&gt;% \n  ggplot(aes(x = p)) +\n  geom_density(fill = \"grey50\", color = \"grey50\") +\n  annotate(geom = \"text\", x = .08, y = 2.5,\n          label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                    breaks = c(0, .5, 1),\n                    limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n# ppd\nf &lt;-\n  f %&gt;% \n  mutate(w = rbinom(n(), size = n_trials,  prob = p))\n\n# ppd histogram (bottom pic)\nf %&gt;% \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"number of water samples\",\n                    breaks = seq(from = 0, to = 9, by = 3)) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\n\nSame process is used for a Prior Predictive Distribution\n\nThe prior distribution takes the place of the posterior density in this case\n\nFor a continuous distribution, see Ch 4 &gt;&gt; Prior Predictive Distribution Example\nModel fit\n\n2 types of uncertainty:\n\npredicted observation uncertainty\n\neven if we did know the correct conjecture, p, we wouldn’t know with certainty the outcome of the next trial, unless the correct p = 0 or p = 1.\nSee histogram in previous section\n\nEven if the “true” conjecture (e.g. p = 0.70) is known, there is uncertainty in the count of W observations the next trial will yield.\n\nSounds like this is quantified by predictive intervals (PI) given some α\n\nuncertainty about p\n\nthe posterior distribution over p embodies this uncertainty.\nAnd this is quantified by credible intervals (CI) given some α\n\n\nGOF Question: If we were to make predictions about the most probable p using this model, how consistent is our model with the data?\n\nAnswer: if the shape of the PPD matches the shape of the sampled posterior distribution, then the model is consistent with the data. (i.e. good fit)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-4---linear-models-cis-splines",
    "href": "qmd/statistical-rethinking.html#chapter-4---linear-models-cis-splines",
    "title": "1  Statistical Rethinking",
    "section": "1.3 Chapter 4 - Linear Models, CIs, Splines",
    "text": "1.3 Chapter 4 - Linear Models, CIs, Splines\n\nGaussian Distribution\n\nGood toy Example that explains the shape of a Normal distribution\n\n1000 people stand at the 50 yd line and flip a coin 16 times (doesn’t seem like a lot). Tails move one step left, heads move one step right. The shape of the people after the 16 flips will be a bell curve. If you picture people zigging left and right after flips, there are more possible sequences of flips that end with the person back where they started at the 50 yd line than say, for Example, at the 30 yd line (either goal). Therefore, ending up at the 50 yd line is most probable.\n\nReminds me of how there is only one way for your ear bud wires not to be tangled and many, many more ways for them to be tangled.\n\n\nAdditive effects make convergence to a Gaussian inevitable - Each flip can be thought of as a fluctuation from the average of the distribution with large positive fluctuations cancelling out large negative fluctuations. The more samples taken (flips), the more opportunities for fluctuations of varying sizes and sign to be paired with a fluctuation of equal size and opposite sign , resulting in a cancellation and a final sum of zero. Therefore, normality emerges as sample sizes get larger. The speed at which the sampling distribution converges to normality depends on the underlying data generating distribution.\nSmall multiplicative effects converge to a Gaussian - if the effect of interaction is a small percent (e.g. between 0 and 10% increase/decrease) then the resulting product of fluctuations converges to a Gaussian distribution as well. Small multiplicative effects can be well approximated additively.\nThe log of large multiplicative effects converge to a Gaussian - if the effect of interactions is a large percent (e.g. 10-50% increase/decrease), then the product of the fluctuations are Gaussian on the log-scale (Log-Normal).\n\nSee General Notes -&gt; Notation:  for the notation and syntax used in model specification\nIt’s a good idea to plot your priors to get a sense of the assumptions in your model.\n\nExample: curve(dunif( x, 0, 50), from = -10,  to = 60)  for σ ~ Uniform( 0, 50 ) constrains the σ to have a positive probability from 0 to 50 cm for a height outcome variable\n\nPrior Predictive Distribution\n\nPriors can be previous posterior distributions, so they can be sampled in order to see the expected distribution, averaged over the priors, which is a distribution of relative plausibilities before seeing the data.\nExample: height ~ 1\n\n\nsample_mu &lt;- rnorm(1e4, 178, 20) # where mean = 178 and sigma = 20\nsample_sigma &lt;- runif(1e4, 0, 50) # bounded from 0 to 50\nprior_predictive_distribution &lt;- rnorm(1e4, sample_mu, sample_sigma) # averaging over the priors, PPD\ndens(prior_predictive_distribution) # relative plausibilities before seeing data\n\nWhat our parameter priors (mean, sd) say that we expect the height distribution to look like in the wild.\nSampling means and std devs from posterior\n\nregardless of sample size, a gaussian likelihood * a gaussian prior will always produce a gaussian posterior distribution for the mean\nthe std.dev posterior will have a longer right tail. Smaller sample sizes = longer tail\n\nvariances are always positive. Therefore if the estimate of the variance is near zero, There isn’t much uncertainty about how much smaller it can be because its bounded by zero, but there is no bound on the right, so the uncertainty is larger.\n**brms handles these skews fine ** because it’s posterior sampling function samples from Hamiltonian Monte Carlo (HMC) chains and not the multivariate Gaussian distribution. See Ch.8. Also Ch.9 in the brms version for modeling σ using distributional models in case of heterogeneous variances.\n\nSimilar grid approx. code to top of Ch.3. Instead of one parameter, p, it’s the mean and standard deviation.\n\nEach value (a row number) that’s sampled represents one combination of potential values of the mean and standard deviation and is selected based on the posterior probability density.\nThis posterior density is the joint posterior density of the mean and standard deviation\n\n\n\n# sample posterior probability but output the row indices and not the values\nsample_rows &lt;- sample(1:nrow(posterior), size = 1e4, replace = TRUE, prob = posterior$prob)\n# use sampled row indices to sample the grid values for mu and sigma\nsample_mu &lt;- posterior$mu[sample_rows]\nsample_sigma &lt;- posterior$sigma[sample_rows]\ndens(sample_mu) # Often Guassian\nHDPI(sample.sigma) # usually right skewed, extent depends on sample size, may want to use HDPI for CIs\n\n(See stat.rethinking brms recode bkmk for ggplot graphs and dplyr code)\nUsing quadratic approximation to compute the MAP for the height data\n\nThis is actually HMC instead of quadratic. He uses a MAP function from his package in the book.\nWe’re trying to find the maximum a posteriori (MAP) for μ and σ (i.e. the maximum of their posterior probabilities)\n\n\n# no predictors yet, so just an intercept model\nb4.1 &lt;-  brm(data = height_dat, family = gaussian,\n              height ~ 1,\n              prior = c(prior(normal(178, 20), class = Intercept),\n                        # can use fewer iter and warm-up with a half cauchy prior\n                        # prior(cauchy(0, 1), class = sigma)\n                        prior(uniform(0, 50), class = sigma)),             \n              # iter = 2000, warmup = 1000, chains = 4, cores = 4\n              iter = 31000, warmup = 30000, chains = 4, cores = 4,\n              seed = 4, backend = \"cmdstanr\",\n)\nsummary(b4.1)\nposterior_summary(b4.1)\npost &lt;- posterior_samples(b4.1)\nplot(b4.1)\n\nOnly things to pay attention to at this point are the family, formula, and prior arguments, Think the rest of this gets covered in Ch.8.\n\nflat priors can require a large value in the warm-up arg because of it’s weakness\n\nsummary shows the estimates(means) of mu (intercept) and standard deviation (sigma), standard errors, CIs (percentile) along with the model specifications\n\nThese values are the Gaussian  estimates of each parameter’s “marginal” distribution\n\nThe marginal distribution of sigma is the distribution of the residuals\nThe marginal distribution for mu means the plausibility of each value of mu, after averaging over the plausibilities of each value of sigma.\n\nThe “averaging over” effect can be seen in the marginal distribution of sigma if you change the the prior for mu to normal(178, 0.1). The posterior estimate for mu will hardly move off the prior, but the estimate for sigma will move quite a bit from before, since the plausibilities for each value of mu changed. \n\nSo the posterior density for sigma gets more squatted and widens with that narrow mu prior. The right tail lengthens as well.\nI think this is an Example of how the marginal distribution of sigma is affected by mu since it has been averaged over the values of mu.\n\n\n\n\nposterior_summary  just gives the estimates, errors, CIs in a matrix object\nposterior_samples  extracts samples from the posterior for each parameter\n\nextracts samples HMC chains\nalso outputs the log posterior, lp__ . For details:\n\nhttps://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2\nhttps://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient \n\n\nfitted gives the fitted values\n\nnewdata + summary = F takes new data and interpolates from the model.\n\nStrength of priors can be judged by the size of n that it implies using the formula for Gaussian sample std.dev.\n\n\n\nEquivalent to saying you’ve observed 100 heights (1/0.01) that had a mean = 178 cm, sd = 0.01. Considered to be a pretty strong prior.\n\n\n\nExtremely weak prior\n\n\nModel\n\nExample single variable regression where height is the outcome and weight the predictor\n\nhi ~ Normal(μ, σ)\nμi = α + βxi\nα ~ Normal(178, 100)\nβ ~ Normal(0, 10)\nσ ~ Uniform(0, 50)\n\nhi is the likelihood and ui is the linear model (now deterministic, not a parameter). The rest of the definitions are for priors.\nA Gaussian prior for β with mu = 0 is considered conservative because it will drag the probability mass towards zero, and a β parameter at 0 is equivalent to saying, as a prior, that no relation exists between the predictor and outcome\n\nIn this case, the sigma for this prior is 10, so while it’s conservative, it’s also weak, and therefore it’s pull on β towards zero will be small. The smaller sigma is, the greater the pull of the prior.\n\nThis pull towards 0 is also called shrinkage.\n\n\nStarting values don’t effect the posterior while priors do.  (Also see Ch 9 &gt;&gt; Set-up Values)\n\ninits argument is for starting values in brms\n\nEither “random” or “0”. If inits is “random” (the default), Stan will randomly generate initial values for parameters.\n\nIf “0”,  then all parameters are initialized to be zero.\n“random” randomly selects the starting points from a uniform distribution ranging from -2 to 2\n\nThis option is sometimes useful for certain families, as it happens that default (“random”) inits cause samples to be essentially constant.\nWhen everything goes well, the MCMC chains will all have traversed from their starting values to sampling probabilistically from the posterior distribution once they have emerged from the warmup phase. Sometimes the chains get stuck around their starting values and continue to linger there, even after you have terminated the warmup period. When this happens, you’ll end up with samples that are still tainted by their starting values and are not yet representative of the posterior distribution.\n\nsetting inits = “0” is worth a try, if chains do not behave well.\nAlternatively, inits can be a list of lists containing the initial values, or a function (or function name) generating initial values. The latter options are mainly implemented for internal testing but are available to users if necessary. If specifying initial values using a list or a function then currently the parameter names must correspond to the names used in the generated Stan code (not the names used in R).\n\nSee this thread which uses a custom function to create starting values for a parameter\n\n\nIn many cases a weak prior for the intercept is appropriate as it is often uninterpretable. Meaning it should have a wide range given that we can’t intelligibly estimate what range of values it might fall in-between.\n\ne.g. if the model estimate for the intercept in the height ~ weight model was 113.9. This would mean that the average height for a person with zero weight is 113.9 cm which is nonsense.\n\nCentering predictors makes the intercept interpretable. The interpretation becomes “when the predictors are at their average value, the expected value of the outcome is the value of the intercept.”\n\nβ and σ estimates aren’t affected by the transformation\n\n\n\n\n\nb4.3 &lt;-  brm(data = dat, family = gaussian,\n      height ~ 1 + weight,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 41000, warmup = 40000, chains = 4, cores = 4,\n      seed = 4, backend = \"cmdstanr\",\n)\n\n# scatter plot of observed pts with the regression line from the model\n# Defined the by the alpha and beta estimate\ndat %&gt;%\n  ggplot(aes(x = weight, y = height)) +\n  geom_abline(intercept = fixef(b4.3)[1],\n              slope    = fixef(b4.3)[2]) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\nThe posterior density is a joint density of α (intercept), β, and σ.\nSince μ is a function of α and β, it also has a joint density even though it is no longer a parameter (i.e. deterministic and no longer stochastic)\nCorrelation between parameters\n\nposterior_samples(b4.4) %&gt;%\n  select(-lp__) %&gt;% # removing log posterior column\n  cor() %&gt;%\n  round(digits = 2)\n\nCorrelated parameters are potentially a problem for interpretation of those parameters.\nCentering the predictors can reduce or eliminate the correlation between predictor and intercept parameter estimates\n\nalso makes the intercept interpretable (see 2 bullets above)\n\nUncertainty\n\n2 types of uncertainty (also see end of Ch.3)\n\nThe posterior distribution includes uncertainty in the parameter estimation\nThe distribution of simulated outcomes (PI) includes sampling variation (i.e. uncertainty)\n\nThe posterior distribution is a distribution about all possible combinations of α and β  with an assigned plausibility for each combination.\n\nEach combo can fit a line. A plot of these regression lines is an intuitive representation of the uncertainty. The CI for a regression line. \n\nInterpretation of HDPI when x = x0\n\nExample: Given a person’s weight = 50kg, what does an 89% HDPI around our model’s height prediction mean?\n\n“The central 89% of the ways for the model to produce the data place the average height between about 159cm and 160cm (conditional on the model and data), assuming a weight of 50kg.”\n\n\n\n\n# posterior distribution for x = x0 where x0 = 50 kg\nmu_at_50 &lt;- posterior_samples(b4.4) %&gt;%\n  transmute(mu_at_50 = b_Intercept + b_weight * 50)\n\n# hdmi at 89% and 95% intervals\nmean_hdi(mu_at_50[,1], .width = c(.89, .95))\n\nmu_at_50 %&gt;%\n  ggplot(aes(x = mu_at_50)) +\n  geom_density(size = 0, fill = \"royalblue\") +\n  tidybayes::stat_pointintervalh(aes(y = 0),\n                      point_interval = mode_hdi, .width = .95) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = expression(mu[\"height | weight = 50\"])) +\n  theme_classic()\n\nConfidence Interval - repeating the calculation for every x0 will give you the regression line CI\n\nExample brms regression line CI\n\n\n# x value (weight) range we want for the CI of the line\nweight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1))\n\n# predicted values (height) for each x value\n# 95% CIs generated by default\nmu_summary &lt;-  fitted(b4.3, newdata = weight_seq) %&gt;%\n    as_tibble() %&gt;%\n    # let's tack on the `weight` values from `weight_seq`\n    bind_cols(weight_seq)\n\n# regression line against data with CI shading\ndat %&gt;%\nggplot(aes(x = weight, y = height)) +\ngeom_smooth(data = mu_summary,\n          aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n          stat = \"identity\",\n          fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\ngeom_point(color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\ncoord_cartesian(xlim = range(d2$weight)) +\ntheme(text = element_text(family = \"Times\"),\n    panel.grid = element_blank())\n\nA narrow CI doesn’t necessarily indicate an accurate model. If the model assumption of a linear relationship is wrong, then the model is wrong regardless of the narrowness of the CI. The regression line represents the most plausible line and the CI is the bounds of that plausibility.\nPrediction Interval\n\nProcedure\n\nGenerate the range/number of weights (predictor variable) you want a PI for\nSample σ, α, and β from the posterior distribution the same number of times as the number of weights in your range\nsimulate the heights from a gaussian distribution (think rnorm) using the weights (along with α and β) in a linear expression for mean argument and the standard deviations from the posterior (see details in book)\n\nExample PI and CI around regression line\n\n\n# 95% PIs generated by default\npred_height &lt;-  predict(b4.3,\n      newdata = weight_seq) %&gt;%\n    as_tibble() %&gt;%\n    bind_cols(weight_seq)\n\n# includes regression line, CI, and PI\ndat %&gt;%  ggplot(aes(x = weight)) +\n  # PIs\n  geom_ribbon(data = pred_height, aes(ymin = Q2.5,\n              ymax = Q97.5), fill = \"grey83\") +\n  # CIs\n  geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\", fill = \"grey70\", color = \"black\",\n              alpha = 1, size = 1/2) +\n  geom_point(aes(y = height), color = \"navyblue\", shape = 1,\n            size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\nSplines (2nd Edition)\n\na smooth function built out of smaller, component functions\n\nThese smaller functions are influenced locally (i.e. by the data immediately proximal to area the component function is responsible for)\n\nThis contrasts with polynomial functions where all the parameters are influenced globally (i.e. all the data)\n\nThis makes splines much more flexible than polynomials\n\n\n\nB-Spline: “basis” spline, non-linear function components (i.e. “basis” functions) that are built on top of each other.\n\nSounds like fourier series\nPredictor is divided up into sections. For each section, basis components are either switched on or off\nµi = α + w1Bi,1 + w2Bi,2 + w3Bi,3 + …\n\nwhere Bi,n is the nth basis function’s for row i\n\nB values turn on/off weights in different regions\n\nw is the parameter estimate (negative or positive) used to adjust the influence of the basis function on µ\n\nThis value adjusts the height of each basis function in its region relative to an intercept\n\nThe intercept is depicted in fig in the Example below as horizontal solid black line (y = 0) in the first 2 facets and a dotted line in the bottom facet.\n\n\n\n\nExample: Temperature vs Time\n\n“+” indicates a knot (pivot point); there is a (linear in this case) basis function for each knot\nAt year 1306, only basis functions B2 and B3 are turned on. w2 and w3 weight the value of each basis function to best fit the mean temperature of that year\n\nUnlike polynomial regression, where parameters influence the entire shape of the curve.\n\n\nThe fit in the Example isn’t very good. 2 ways to improve:\n\nAdd more knots which essentially adds more basis functions\n\nMight be useful to place knots at different evenly-spaced quantiles of the predictor variable. For Example, you might use 0%, 25%, 50%, 75%, and 100%.\n\ne.g. using 15 knots would mean 3 knots at each quantile\n\n\nUse polynomial basis functions\n\nThe higher the degree of polynomial means the more basis functions turned of a particular point\n\ndegree 1 (linear): 2 basis functions turned on per point, 1 basis function per knot\ndegree 2 (quadratic): 3 basis functions turned on per point, 1 basis function per knot + 1\ndegree 3 (cubic): 4 basis functions turned on per point, 1 basis function per knot + 2\n\n\n\nExample: 15 knots at 5 quantiles, cubic basis functions\n\nModel specification\n\n\n\ndata(cherry_blossoms, package = \"Rethinking\")\nd2 &lt;- cherry_blossoms %&gt;%\n        tidyr::drop_na(temp)\n\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2,\n                family = gaussian,\n                doy ~ 1 + s(year))\n##                  prior    class    coef group resp dpar nlpar bound      source\n##                  (flat)        b                                          default\n##                  (flat)        b syear_1                            (vectorized)\n##  student_t(3, 105, 5.9) Intercept                                          default\n##    student_t(3, 0, 5.9)      sds                                          default\n##    student_t(3, 0, 5.9)      sds s(year)                            (vectorized)\n##    student_t(3, 0, 5.9)    sigma                                          default\n\n# multi-level method\nb4.11 &lt;- brm(data = d2,\n            family = gaussian,\n            # k = 19, corresponds to 17 basis functions I guess ::shrugs::\n            # The default for s() is to use what’s called a thin plate regression spline\n            # bs uses a basis spline\n            temp ~ 1 + s(year, bs = \"bs\", k = 19),\n            prior = c(prior(normal(100, 10), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(student_t(3, 0, 5.9), class = sds),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 4, backend = \"cmdstanr\",\n            control = list(adapt_delta = .99))\nprint(b4.11)\nb4.11_fits &lt;- fitted(b4.11) %&gt;% \n      data.frame() %&gt;% \n      bind_cols(select(d2, year, temp))\n\nggplot(b4.11_fits, aes(x = year, y = temp, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = fixef(b4.11)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  geom_ribbon(fill = \"white\", alpha = 2/3) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"#4f455c\"))\n\nThe multi-level method is used in the code block, but a single level method is possible. SEs are large in the single-level and about half the point estimates differ in sign from the multi-level.\nMost if not all of the point estimates from the multi-level are covered by the CIs of the single-level, so I don’t think either model is “wrong” based on the estimates.\nI’m more comfortable with the single-level method, but I’d probably choose the multi-level method based on the SEs.\nSee scrapsheet.R in Statistical Rethinking directory for code. A comparison between the two methods estimates shown in a chart is also there.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-5---multivariable-linear-models-diagnostic-plots-categoricals-dags",
    "href": "qmd/statistical-rethinking.html#chapter-5---multivariable-linear-models-diagnostic-plots-categoricals-dags",
    "title": "1  Statistical Rethinking",
    "section": "1.4 Chapter 5 - Multivariable Linear Models, Diagnostic Plots, Categoricals, DAGs",
    "text": "1.4 Chapter 5 - Multivariable Linear Models, Diagnostic Plots, Categoricals, DAGs\n\nUseful because:\n\nStatistical “control” for confounds. A confound is something that misleads us about a causal influence. They can hide real important variables just as easily as they can produce false ones.\n\nFrom section 6.4, confounding is “any context in which the association between an outcome Y and a predictor of interest X is not the same as it would be, if we had experimentally determined the values of X”\n\nMultiple causation. A phenomenon may arise from multiple causes and estimating each with the same data is useful.\nStratification (the intercept is dependent on the value of the predictor)\n\nbinary or categorical - adding the adjustment variable to the model formula subsets the data and allows the model to estimate a different association for each category\ncontinuous - adding the adjustment variable to model formula allows the model to estimate a different association for each value of the continuous predictor\nInteractions - The importance of one variable may depend upon another. Effective inference about one variable will often depend upon consideration of others.\n\n\nmain effects are additive combinations of variables\nMultivariate notation with Divorce Rate as the outcome and state median age at marriage and state marriage rate as predictors\n\nDivorceRatei ~ Normal(μi, σ)\nμi = α + β1MedianAgeMarriage_si + β2MarriageRate_si\nα ~ Normal(10, 10)\nβ1 ~ Normal(0, 1)\nβ2 ~ Normal(0, 1)\nσ ~ Uniform(0, 10)\n\nSuper flat priors here, see below for better ones\n\n\nQuestion answered from a multivariate regression with two predictors\n\nWhat is the predictive value of this variable, once I know the other predictor variable(s)?\nExample: Outcome = Divorce rate (D),a Predictor 1 = state’s median marriage age (A), Predictor 2 = state’s marriage rate (M)\n\nAfter already knowing the marriage rate, what additional value is there in knowing the median age at marriage?\nAfter already knowing the median age of marriage, what additional value is there in knowing the marriage rate?\n\n\nμi = α + β1MedianAgeMarriage_si + β2Marriage_si\n\n(the “_s” was added to signify that these are the standardized versions of these variables)\nSays, A State’s divorce rate can be a function of its marriage rate or its median age at marriage\n\n“+” is an “or” which indicates independent associations, which may be purely statistical or rather causal.\n\n\n\n\nb5.3 &lt;-  brm(data = dat, family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\",\n      file = \"fits/b05.03\")\nprint(b5.3)\npost &lt;- posterior_samples(b5.3)\n\n# Coefficient plot\nlibrary(tidybayes)\n\npost %&gt;%\n  select(-lp__) %&gt;% # removing log posterior column\n  gather() %&gt;%\n\nggplot(aes(x = value, y = reorder(key, value))) +\n  # note how we used `reorder()` to arrange the coefficients\n  geom_vline(xintercept = 0, color = \"firebrick4\", alpha = 1/10) +\n  stat_pointintervalh(point_interval = mode_hdi, .width = .95,\n                      size = 3/4, color = \"firebrick4\") +\n  labs(title = \"My tidybayes-based coefficient plot\",\n      x = NULL, y = NULL) +\n  theme_bw() +\n  theme(panel.grid  = element_blank(),\n        panel.grid.major.y = element_line(color = alpha(\"firebrick4\", 1/4), linetype = 3),\n        axis.text.y  = element_text(hjust = 0),\n        axis.ticks.y = element_blank())\n\nm5.3:  D ~ α + βMM + βAA\n\nb5.3 (above) using brms\n\nm5.2: D ~ α + βM\nm5.1: D ~ α + βA\n\nThe positive effect of marriage rate disappears after adjusting for median age\nInterpretation: Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State.\n\n2nd ed explains DAGs and conditional independencies using this Example. See Causal Inference &gt;&gt; Statistical Rethinking\n3 types of inferential plots\n\nPredictor Residual plots - Shows outcome vs residual predictor values\n\nIt’s a clever multi-variable version of the typical observed response vs observed predictor scatter plot plus fitted regression line.\n\nShows the linear relationship between the outcome and a predictor after adjusting for the other the predictors.\nThe fact that residuals are either positive or negative does add a little something different to the interpretation\n\nA predictor variable residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest.\n\ni.e. the x1 predictor residual is the residual of this regression -  x1 ~ α + βx2 + βx3 + …\n\nThe regression removes the variation that can be attributed to the other predictor variables\n\nCalculating the predictor residuals assumes the relationships between the predictors are additive\nInterpretation of the residual: When a residual is positive , that means the predictor of interest was in excess of what we’d expect, given the value of the other predictors.\n\nThe slope of the regression line in this plot will be the coefficient estimate in the multi-variable regression.\n\nMost charts for regression coefficients are just error bar plots (like above) or tables. This seems much better.\n\nYou get the distribution of the residuals (shape, spread, etc.), a visual of the slope and it’s CI, an interpretation of how the slope relates values of the predictor and the response, and partial correlation.\nresidual patterns might indicate a nonlinear transformation should be used\nperhaps coloring by category levels might produce a pattern that indicates an interaction should be used\nI think the spread of residuals and the linearity of the shape of this plot gives an idea of the partial correlation with the response. The tighter the spread, the larger the partial correlation\n\n\n\n\n\ndata(WaffleDivorce, package = \"rethinking\")\nd &lt;- WaffleDivorce\nrm(WaffleDivorce)\nd &lt;- d %&gt;% \n  mutate(d = rethinking::standardize(Divorce),\n        m = rethinking::standardize(Marriage),\n        a = rethinking::standardize(MedianAgeMarriage))\n\n# model state marriage rate using state median marriage age\n# only one other predictor here, but we'd also include others if we had more.\nb5.4 &lt;- brm(data = d, \n      family = gaussian,\n      m ~ 1 + a,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\",\n      file = \"fits/b05.04\")\n\n# \"predictor residuals\" for marriage rate\nr &lt;- residuals(b5.4) %&gt;%\n  # to use this in ggplot2, we need to make it a tibble or data frame\n  as_tibble() %&gt;%\n  bind_cols(d)\n\n# annotation\ntext &lt;-tibble(Estimate = c(- 0.5, 0.5),\n        Divorce = 14.1,\n        label = c(\"slower\", \"faster\"))\n# Predictor Residual plot\nr %&gt;%\n  ggplot(aes(x = Estimate, y = Divorce)) +\n      stat_smooth(method = \"lm\", fullrange = T,\n              color = \"firebrick4\", fill = \"firebrick4\",\n              alpha = 1/5, size = 1/2) +\n      geom_vline(xintercept = 0, linetype = 2, color = \"grey50\") +\n      geom_point(size = 2, color = \"firebrick4\", alpha = 2/3) +\n      geom_text(data = text,\n            aes(label = label)) +\n      scale_x_continuous(\"Marriage rate residuals\", limits = c(-2, 2)) +\n      coord_cartesian(xlim = range(r$Estimate),\n                  ylim = c(6, 14.1)) +\n      theme_bw() +\n      theme(panel.grid = element_blank())\n\n\n\nThe predictor residuals are either positive or negative, so this bifurcates the X-axis in the bottom-row plots into predictor values that are “lower than expected” and “higher than expected.”\nThe regression line is then used to interpret the association between States with higher/lower expected  values and their Divorce Rate.\nSee Kurz to see how to include uncertainty (error bars) with the predictor residual points\nTop row are observed response (predictor of interest) vs  another observed predictor (both standardized)\n\nLine is the fitted regression line with the segments showing the magnitude of the residual\nInterpretation for marriage rate as the response (top left): When a residual is positive (i.e segment above the line), that means the observed proportion of married adults (marriage rate) was in excess of what we’d expect given the median age at marriage in that State\n\nBottom row are the Predictor Residual plots\n\npoints are the standardized divorce rate vs residuals; line is estimated mean of divorce rate (i.e. d ~ α + β&lt;predictor_residuals&gt;)\nInterpretation focuses on the regression line and the positive or negative values of the residuals on the x-axis\nInterpretation of marriage rate residuals as the predictor (left): States with a higher than expected marriage rate (pos x values) have almost the same divorce rates on average as those states with lower than expected (neg x values) marriage rates (i.e. regression line is flat)\nInterpretation of age at marriage residuals as the predictor (right):  States with an older than expected  (pos x values) median age at marriage have a much lower divorce rate on average than states with a younger than expected (neg x values) median age of marriage.\nReminder: The slope of lines are coefficients in the multi-regression model. These charts illustrate how those models are measuring the left-over variation of a predictor’s linear relationship with the response after the other predictors’ values are known.\n\n\nCounterfactual plots - Shows the implied predictions for imaginary experiments in which the different predictor variables can be changed independently of one another\n\nShows how predictions change as you change only one predictor at a time.\n2-stage process (See Example): Input each value of the predictor of interest into an assumed causal model to get the values of the other predictors. Then, fit the multi-regression model to get the predictions.\n\n1st Stage: Association of predictor variable of interest is regressed out of each predictor variable and predictions are calculated from these models.\n2nd Stage:  The final regression model uses these predicted values as inputs for all the predictors except the predictor of interest\n\nSimilar to a partial dependence plot, except for the causal model being used to calculate values of the other predictors.\n\npdp keeps the value of the predictor of interest constant and fits multiple models for each combination of the values of the other predictors, then takes the average of the predictions. This average is estimated response value that corresponds to predictor of interest value in the plot.\nBoth probably end up in a very similar place but the counterfactual plots sounds much less computationally intensive.\n\nThe scale is that of the data unlike the Predictor Residual plot\nPlot can visualize unrealistic scenarios, so must be careful with interpretation.\nExample: How does Divorce Rate (D) (outcome) change when we change Median Age at Marriage (predictor) values? Assumed causal model: \n\nm_model is the relationship between Median Age of Marriage (A) and State Marriage Rate (M) in the causal model. The values of the predictor of interest, A, will be used in this model to get the values of the other predictor, M.\nd_model uses the values of A and the modeled values of M to calculate the estimated values of D.\n\n\n\n# the multi-regression model\n# models influence of a on m\nm_model &lt;- bf(m ~ 1 + a)\n\n# models influence of m and a on d. Values of m determined by m_model\nd_model &lt;- bf(d ~ 1 + a + m)\n\n\nb5.3_A &lt;- brm(data = d, \n              family = gaussian,\n              # set_rescor = F says don't add the residual correlation between the two submodels\n              d_model + m_model + set_rescor(FALSE),\n                        # resp arg says which submodel the prior belongs to\n                        # d-model priors\n              prior = c(prior(normal(0, 0.2), class = Intercept, resp = d),\n                        prior(normal(0, 0.5), class = b, resp = d),\n                        prior(exponential(1), class = sigma, resp = d),\n                        # m-model priors\n                        prior(normal(0, 0.2), class = Intercept, resp = m),\n                        prior(normal(0, 0.5), class = b, resp = m),\n                        prior(exponential(1), class = sigma, resp = m)),\n              iter = 2000, warmup = 1000, chains = 4, cores = 4,\n              seed = 5, backend = \"cmdstanr\",\n              file = \"fits/b05.03_A\")\nprint(b5.3_A)\n\n# Counterfactual predictions for Divorce Rate, d\n# range of values for the predictor of interest, a\n# m = 0 is for the \"average state of m\" which is zero since it's standardized\nnd &lt;- tibble(a = seq(from = -2, to = 2, length.out = 30),\n            m = 0)\n# resp = \"d\" says we want the counterfactual predictions for d\np1 &lt;- predict(b5.3_A,\n              resp = \"d\",\n              newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;%   \n  ggplot(aes(x = a, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  labs(subtitle = \"Total counterfactual effect of A on D\",\n      x = \"manipulated A\",\n      y = \"counterfactual D\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n# Modelled values for Marriage Rate, m (note that there's no m = 0)\nnd &lt;- tibble(a = seq(from = -2, to = 2, length.out = 30))\np2 &lt;- predict(b5.3_A,\n              resp = \"m\",\n              newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;%   \n  ggplot(aes(x = a, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  labs(subtitle = \"Counterfactual effect of A on M\",\n      x = \"manipulated A\",\n      y = \"counterfactual M\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n# patchwork\np1 + p2 + plot_annotation(title = \"Counterfactual plots for the multivariate divorce model\")\n\nThis predicted trend in D include both paths: A → D and A → M → D. (left side) As the multi-regression showed, marriage rate, m, has little influence on d. Might be why we see little difference in this graphs.\nExample: What is the expected causal effect of increasing median age at marriage from 20 to 30?\n\nnd &lt;- tibble(a = (c(20, 30) - 26.1) / 1.24,\n            m = 0)\npredict(b5.3_A,\n        resp = \"d\",\n        newdata = nd,\n        summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(\"a20\", \"a30\") %&gt;% \n  mutate(difference = a30 - a20) %&gt;% \n  summarise(mean = mean(difference))\n\nExample: How does Divorce Rate (outcome) change when we change values of Marriage Rate (predictor)? The arrow A → M is deleted, because if we control the values of M, then A no longer influences it \n\n# see comments above\nnd &lt;- tibble(m = seq(from = -2, to = 2, length.out = 30),\n            a = 0)\npredict(b5.3_A,\n        resp = \"d\",\n        newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;%   \n  ggplot(aes(x = m, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  labs(subtitle = \"Total counterfactual effect of M on D\",\n      x = \"manipulated M\",\n      y = \"counterfactual D\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\nPosterior Prediction plots - Show model-based predictions against raw data, or otherwise display the error in prediction.\n\nAlso see end of Ch.3, which uses a density plot and discusses GOF\nIt’s your standard Predicted vs Outcome plot\n\nab-line looks funky in the book because of the axis labels but it just a standard, 45-degree, y-intercept=0 line\n\nline represents perfect prediction\n\nused to determine which values of the outcome variable the model under/over predicts, which values that model completely fails at predicting (values far away from the ab-line), and those values the model performs well on.\n\nwhich values are “higher than we’d expect” given these data and this model.\n\n\n\n\nfitted(b5.3) %&gt;%\n  data.frame() %&gt;%\n  # unstandardize the model predictions\n  mutate_all(~. * sd(d$Divorce) + mean(d$Divorce)) %&gt;% \n  bind_cols(d) %&gt;%\n\n  ggplot(aes(x = Divorce, y = Estimate)) +\n  geom_abline(linetype = 2, color = \"grey50\", size = .5) +\n  geom_point(size = 1.5, color = \"firebrick4\", alpha = 3/4) +\n  geom_linerange(aes(ymin = Q2.5, ymax = Q97.5),\n                size = 1/4, color = \"firebrick4\") +\n  geom_text(data = . %&gt;% filter(Loc %in% c(\"ID\", \"UT\", \"RI\", \"ME\")),\n            aes(label = Loc), \n            hjust = 1, nudge_x = - 0.25) +\n  labs(x = \"Observed divorce\", y = \"Predicted divorce\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\nPrediction error per group variable\n\nFrom Kurz’s 1st ed\n\n\nresiduals(b5.3) %&gt;%\n  as_tibble() %&gt;%\n  rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;%\n  bind_cols(predict(b5.3) %&gt;%\n              as_tibble() %&gt;%\n              transmute(p_ll = Q2.5, p_ul = Q97.5),\n            d) %&gt;%\n  # here we put our `predict()` intervals into a deviance metric\n  mutate(p_ll = Divorce - p_ll,\n        p_ul = Divorce - p_ul) %&gt;%\n  # now plot!\n  ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) +\n      geom_hline(yintercept = 0, size = 1/2, color = \"firebrick4\", alpha = 1/10) +\n      geom_pointrange(aes(ymin = f_ll, ymax = f_ul),\n                  size = 2/5, shape = 20, color = \"firebrick4\") +\n      geom_segment(aes(y    = Estimate - Est.Error,\n                  yend = Estimate + Est.Error,\n                  x    = Loc, xend = Loc),\n                  size = 1, color = \"firebrick4\") +\n      geom_segment(aes(y = p_ll, yend = p_ul,\n                  x    = Loc, xend = Loc),\n                  size = 3, color = \"firebrick4\", alpha = 1/10) +\n      labs(x = NULL, y = NULL) +\n      coord_flip(ylim = c(-6, 5)) +\n      theme_bw() +\n      theme(panel.grid  = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y  = element_text(hjust = 0))\n\nMasking\n\nA special case where you have two (or more) predictors that when each is fit in a bivariable regression with the outcome, no (or only a very small effect) can be estimated. Then, when fitting all the predictors in a multivariate regression with the outcome, a measurable/increased effect is estimated.\nThis is a special combination of both predictors being mildly, positively correlated with each other while each are oppositely correlated with the outcome.\nSituation with an outcome variable and two predictors\n\ncorrelation structure\n\npred_var1 and pred_var1 are positively correlated\npred_var1 is negatively correlated with the outcome\npred_var2 is positively correlated with the outcome\n\nmulti-variable regression result\n\npred_var1’s negative estimated effect on the outcome will increase as compared to its estimated effect in a bivariable regression\npred_var2’s positive estimated effect on the outcome will increase as compared to its estimated effect in a bivariable regression\n\nExample\n\nK (outcome) = mother’s milk kilocalories\nM (pred_var1) = log(Body Mass)\nN (pred_var2) = Neocortex percent\nDAGs that are consistent with the data associations where U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies (e.g. a single path DAG) and therefore no testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\nAlso see Causal Inference &gt;&gt; Statistical Rethinking &gt;&gt; Testable Implications\n\n\n\nHow does including both variables in the regression reveal the effect?\n\nThe model uses the opposite relationship of predictors with the outcome to find the most i_nteresting_ (think variation) cases in order to measure the effects of each predictor.\n\nThe closer the size of the correlation between the two pred_vars gets to 1, the larger the change in the estimated effect. Very close to 1 means both have very similar information and using both is redundant (see multicollinearity below)\n\nCloser to zero = smaller increase in effect (i.e. masking diminishes)(independent variables)\n\nLess correlation between variables –&gt; less masking\n\n\nFor Discussion on masking and multicollinearity see My Appendix\n\nCategorical\n\nMisc\n\nWhy we don’t split data by a categorical and run separate regressions? (from 8.1 pg 245)\n\nYou make 2 worse estimations of the variance (sigma) by using smaller amounts of data instead of one estimate using all the data\nIf you want parameter estimates of the splitting variable and uncertainty of that estimate, it needs to be in the model\nModels must use the same data, if you use information criteria based metrics for comparison\nMulti-level models can use information across categories\n\nIncluding a categorical (no interaction) allows the model to predict different means for each category/level.\n\nBinary\n\n❌ Example with the categorical variable as an indicator variable\n\nhi ∼ Normal(µi, σ) µi = α + βmmi α ∼ Normal(178, 20) βm ∼ Normal(0, 10) σ ∼ Uniform(0, 50)\n\nh = height, m = sexmale\nβm represents the expected difference between males and females in height\nα is the average female height\n\nThe problem with this method is that male has two parameters, α and βm, associated with it and therefore twice the uncertainty as compared to female which only has α.\n\n✔ Example with the categorical variable as an index variable\n\nhi ∼ Normal(µi , σ) µi = αsex[i] αj ∼ Normal(178, 20) , for j = 1 & 2 σ ∼ Uniform(0, 50)\n\nresults in two α parameters, one for each sex, and both can use the same prior. Therefore, both sexes have the same uncertainty. \n\nAlways been taught that this approach inherently orders the categories, e.g. since 2 &gt; 1, but I guess this is kosher is bayesian models\nNote: there is no intercept used in these models with only categorical predictors\n\n\n\n\ndata(Howell1, package = \"rethinking\")\nd &lt;- Howell1\nd &lt;-  d %&gt;% \n  mutate(sex = ifelse(male == 1, 2, 1), # create index variable\n        sex = factor(sex)) # transforming it into a factor tells brms it's indexed\n\nb5.8 &lt;-  brm(data = d, \n            family = gaussian,\n            height ~ 0 + sex, # \"0 +\" notation means calculate separate intercepts for each category\n            prior = c(prior(normal(178, 20), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 5, backend = \"cmdstanr\",\n            file = \"fits/b05.08\")\nprint(b5.8)\n## Population-Level Effects: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sex1  134.85      1.58  131.82  137.92 1.00    4036    3015\n## sex2  142.61      1.71  139.29  145.91 1.00    3244    2615\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    26.79      0.77    25.33    28.38 1.00    3510    2929\n\nEach intercept is the avg height for each category (aka marginal means)\n\nAlso see Ch. 8\n\nNominal\n\nUse the index variable method\nSame as binary in the model notation — instead of j = 1, 2 for binary, j = 1,2,3,4 for a categorical with 4 levels\nIncrease the s.d. of the α prior (as compared to the binary prior) “to allow the different  to disperse, if the data wants them to.”\n\n“I encourage you to play with that prior and repeatedly re-approximate the posterior so you can see how the posterior differences among the categories depend upon it.”\n\n\n\ndata(milk, package = \"rethinking\")\nd &lt;- milk %&gt;% \n  mutate(kcal.per.g_s = as.numeric(scale(kcal.per.g)))\n\nb5.9 &lt;-  brm(data = d, \n            family = gaussian,\n            kcal.per.g_s ~ 0 + clade,\n            prior = c(prior(normal(0, 0.5), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4,\n            seed = 5, backend = \"cmdstanr\",\n            file = \"fits/b05.09\")\nprint(b5.9)\n\nCoefficient (error-bar) plots using brms, bayesplot, and tidybayes\n\n# coef plots with CI bars\n\nbrms::mcmc_plot(b5.9, pars = \"^b_\")\n# bayesplot outputs a ggplot obj\nlibrary(bayesplot)\ncolor_scheme_set(\"red\")\npost &lt;- posterior_samples(b5.9)\npost %&gt;% \n  select(starts_with(\"b_\")) %&gt;% \n  mcmc_intervals(prob = .5,\n                point_est = \"median\") +\n  labs(title = \"My fancy bayesplot-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\nlibrary(tidybayes) \npost %&gt;% \n  select(starts_with(\"b\")) %&gt;% \n  set_names(distinct(d, clade) %&gt;% arrange(clade) %&gt;% pull()) %&gt;% \n  pivot_longer(everything()) %&gt;%   \n  ggplot(aes(x = value, y = reorder(name, value))) +  # note how we used `reorder()` to arrange the coefficients\n  geom_vline(xintercept = 0, color = \"firebrick4\", alpha = 1/10) +\n  stat_pointinterval(point_interval = mode_hdi, .width = .89, \n                    size = 1, color = \"firebrick4\") +\n  labs(title = \"My tidybayes-based coefficient plot\",\n      x = \"expected kcal (std)\", \n      y = NULL) +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\nMultiple nominal predictors\n\nb5.11 &lt;-  brm(data = d, \n      family = gaussian, \n      # bf() is an alias for brmsformula() that lets you specify model formulas\n      bf(kcal.per.g_s ~ 0 + a + h, \n        a ~ 0 + clade, \n        h ~ 0 + house,\n        # tells brm we're using non-linear syntax\n        nl = TRUE),\n      prior = c(prior(normal(0, 0.5), nlpar = a),\n                prior(normal(0, 0.5), nlpar = h),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\",\n      file = \"fits/b05.11\")\n\nposterior_samples(b5.11) %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  mutate(name = str_remove(name, \"b_\") %&gt;% \n          str_remove(., \"clade\") %&gt;% \n          str_remove(., \"house\") %&gt;% \n          str_replace(., \"World\", \" World \")) %&gt;% \n  separate(name, into = c(\"predictor\", \"level\"), sep = \"_\") %&gt;% \n  mutate(predictor = if_else(predictor == \"a\", \"predictor: clade\", \"predictor: house\")) %&gt;% \n\n  ggplot(aes(x = value, y = reorder(level, value))) +  # note how we used `reorder()` to arrange the coefficients\n  geom_vline(xintercept = 0, color = \"firebrick4\", alpha = 1/10) +\n  stat_pointinterval(point_interval = mode_hdi, .width = .89, \n                    size = 1, color = \"firebrick4\") +\n  labs(x = \"expected kcal (std)\", \n      y = NULL) +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        strip.background = element_blank()) +\n  facet_wrap(~predictor, scales = \"free_y\")\n\n(As of May 2022) When using the typical formula syntax with more than one categorical variable, {brms} drops a category from every categorical variable except for the first one in the formula.\n\n{brms} was orginally designed to wrap Stan multi-level models, so maybe that has something do with it.\nKurz has links and discussion in Section 5.3.2 of his ebook\n\nContrasts: The difference between one categorical level estimate and the reference level estimate is called a contrast. (aka mean difference)\n\nNever legitimate to compare the overlap in the predictive distributions of categories\n\nMe: I have no idea what “compare the overlap” actually means.\n\nAssume its the intersection of the category ppds or posterior means distributions.\nNot sure how you can “compare” the intersection. It’s a singular thing. Maybe it would make sense if there are more than 2 categories.\n\nIt’s bad because parametersare correlated with each other (?)\n\nShould always compute the contrast distribution\n\nAlso applies to comparing CIs and p-values\nAlways compute the difference first, then the uncertainty of that difference\n\nposterior mean contrast formula: contrast between β1 and β2 (assuming they are uncorrelated) = (β1 - β2) ± √(sdβ12 + sdβ22)\n\nβ is used because the model specification doesn’t have an intercept\n\nPosterior Mean Contrast Calculation\n\n\nlibrary(tidybayes)\n# Compute contrast between male and female estimates\nposterior_samples(b5.8) %&gt;% \n  mutate(diff_fm = b_sex1 - b_sex2) %&gt;%  # Female - Male\n  gather(key, value, -`lp__`) %&gt;%  # not including log posterior column\n  group_by(key) %&gt;% \n  tidybayes::mean_qi(value, .width = .89)\n\n##  key      value .lower .upper .width .point .interval\n##  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n## 1 b_sex1  135.    132.  137.    0.89 mean  qi       \n## 2 b_sex2  143.    140.  145.    0.89 mean  qi       \n## 3 diff_fm  -7.76  -11.5  -3.89  0.89 mean  qi       \n## 4 sigma    26.8    25.6  28.1    0.89 mean  qi\n\ni.e. the difference in the posterior densities for the parameter values of each category\nThe contrast, “diff_fm”, is computed in mutate and mean_qi adds the CIs\nVisualizing contrasts\n\nExample: Posterior Mean Contrast (1 predictor) (From Lecture 4) \n\nWeight was the response with height and sex as predictors\n\nContrast is Male - Female\n\nCalculation is show above\nModel: W ~ S in order the measure the total causal effect (also see Causal Inference &gt;&gt; Statistical Rethinking &gt;&gt; Misc)  of Sex on Weight\nInterpretation: The difference in mean weights between the sexes is likely around 6.8 kg but could potentially be from ~5 to ~8.5\nPosterior means for each sex\n\nExample: Posterior Predictive Contrast (1 predictor) (From Lecture 4)\n\nContrast of the individual gender category PPDs\nWeight was the response with height and sex as predictors\n\nContrast is Male - Female\n\n** the code below won’t result in the same numbers as the plot because b5.8 is H ~ S instead of W ~ S like in the lecture **\n\n\n\n\n\nps5.8 &lt;- posterior_samples(b5.8)\nW1 &lt;- rnorm(1000, ps5.8$b_sex1, ps5.8$sigma) # ppd for womens' weights\nW2 &lt;- rnorm(1000, ps5.8$b_sex2, ps5.8$sigma) # ppd for mens' weights\nW_contrast &lt;- W2 - W1\nmale_prop &lt;- sum(W_contrast &gt; 0) / 1000\nfemale_prop &lt;- sum(W_contrast &lt; 0) / 1000\n\nModel: W ~ S in order the measure the total causal effect (also see Causal Inference &gt;&gt; Statistical Rethinking &gt;&gt; Misc)  of Sex on Weight\nInterpretation: The ppd contrast shows that being a male results in a greater causal effect on weight than being female 82% of the time.\n\nThe reason for the conflicting results in the posterior is that there are likely other causal influences that can affect difference in weights between men and women\n\nThis is indicated by the overlap of the individual ppds\nThese are distributions of female and male weights we expect to see in the wild given the model and these data\n\n\nExample: Posterior Mean Contrast vs Predictor (2 predictors) (From Lecture 4)\n\nWeight was the response with height and sex as predictors\n\nContrast is Female - Male\n\nModel: W ~ S + H in order the measure the direct causal effect (also see Causal Inference &gt;&gt; Statistical Rethinking &gt;&gt; Misc)  of Sex on Weight\nSee Lecture 4 around 39:23 for the code\nInterpretation\n\nChart shows the relationship between height and the gender difference in weight\n\nAs height increases women tend to be heavier than men\n\nTilt suggests “most of the causal influence of sex on weight goes through height” (not sure why this is the case)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-6---colliders-multicollinearity-post-treatment-bias",
    "href": "qmd/statistical-rethinking.html#chapter-6---colliders-multicollinearity-post-treatment-bias",
    "title": "1  Statistical Rethinking",
    "section": "1.5 Chapter 6 - Colliders, Multicollinearity, Post-Treatment Bias",
    "text": "1.5 Chapter 6 - Colliders, Multicollinearity, Post-Treatment Bias\n\nComparing the parameter of interest’s posterior density to its prior will tell you how much information your data is providing\n\nIf the posterior density is similar to the prior, then the data is not providing much information.\n\n3 of the potential problems are discussed\n\nMulticollinearity\n\nA *very strong* relationship between 2 or more of the predictors\nConsequence of it is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome.\n\nThe posterior distributions of the parameter estimates will have very large spreads (std.devs)\ni.e. parameter mean estimates shrink and their std.devs inflate as compared to the bivariate regression results.\n\n\n\nWhen two predictors are highly correlated (&gt;0.90), they contain almost the same information. So, you’re essentially using the same variable and therefore there are infinite combinations of values of β1 and β2 with each equally plausible that result in the same prediction for y. So, the uncertainty of the estimates for those parameters is infinite.\n\nresults in  very large standard deviations\npotentially breaks the model function and you’ll get a “non-identifiable” error\n\nNon-identifiability: the structure of the data and model do not make it possible to estimate the parameter’s value. Multicollinearity is a type of non-identifiability problem.\n\n\n\nThe predictions estimated by the model will be fine, but there can be no inference drawn from the parameter estimates.\nIn the case of strongly, negatively correlated predictors that are oppositely correlated with the outcome, the effect estimates for each predictor shrink to/near zero and therefore each may be a good predictor, but including both is redundant. (sect 5.3.2 1st ed; 6.1.2 2nd ed).\n\n“And what matters isn’t just the correlation between a pair of variables. Rather, what matters is the correlation that remains after accounting for any other predictors.”\n\n** Also see Ch 11 &gt;&gt; (last one) Example: “aggregated binomial” regression (unbalanced number of trials) &gt;&gt; over-parameterization\nSolutions\n\nIdentifying and automatically dropping highly correlated predictors is a mistake. Pairwise correlations are not the problem. It is the conditional associations—not correlations—that matter. And even then, the right thing to do will depend upon what is causing the collinearity. The associations within the data alone are not enough to decide what to do.\n\nExample: Thinking the situation through causally in order to deal with multicollinearity\n\nL, F, and K are highly correlated with each other and therefore have a multicollinearity issue when used in a multivariate regression.\nUnobserved Density, D, influences both lactose, L, and Fat, F.\nRegressing Kilocalorie, K, only on D and dropping L and F solves the multicollinearity problem.\n\n\nFeature reduction methods such as PCA and forms of regularized regression can be used\nRunning different models that include one variable of multicollinear group and showing that the results produce the same interpretation is another option\n\n\nFor Discussion on masking and multicollinearity see My Appendix\nPost-Treatment Bias\n\nmistaken inferences arising from including variables that are consequences of other variables\n\ni.e. the values of the variable are a result after treatment has been applied\n\nBad for causation but not necessarily bad for prediction (sect. 7.5, pg 232)\n\n“…a useful measure of the expected improvement in prediction that comes from conditioning on the fungus. Although the treatment works, it isn’t 100% effective, and so knowing the treatment is no substitute for knowing whether fungus is present.”\n\nExample (adjusting for F masks the treatment effect)\n\nYou want to know the difference in growth under different anti-fungal soil treatments, because fungus on the plants tends to reduce their growth.\n\nPlants are initially seeded and sprout.\nTheir heights are measured.\nThen different soil treatments are applied.\nFinal measures are the height of the plant and the presence of fungus.\n\nThere are four variables of interest here: initial height, final height, treatment, and presence of fungus.\nFinal height is the outcome of interest.\n\nIf your goal is to make a causal inference about the treatment, you shouldn’t include the presence of fungus, because it is a post-treatment effect.\nYou should also NOT drop cases based on post-treatment criteria\n\nStatistical model h1,i ∼ Normal(µi , σ) µi = h0,i × p p = α + βTTi + βFFi α ∼ Log-Normal(0, 0.25) βT ∼ Normal(0, 0.5) βF ∼ Normal(0, 0.5) σ ∼ Exponential(1)\n\nh0 is the initial height at t = 0; (Pre-Treatment variable)\nh1 is the final height at t = 1;\np is a proportional factor;\nT is the anti-fungal treatment indicator,\nF is an indicator if fungus was present (Post-Treatment variable)\n\n\n\n\n\n# nonlinear model syntax\nb6.7 &lt;- \nbrm(data = d, \n  family = gaussian,\n  bf(h1 ~ h0 * (a + t * treatment + f * fungus),\n    # shorthand for fitting all of these parameters as intercepts (i.e. a ~ 1, t ~ 1, f ~ 1)\n    a + t + f ~ 1,\n    # tells brm to fit this as a nonlinear model\n    nl = TRUE),\n  # lb arg - Lower bound for parameter restriction; log-normal is always positive (height will never be negative)\n  prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n            prior(normal(0, 0.5), nlpar = t),\n            prior(normal(0, 0.5), nlpar = f),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6, backend = \"cmdstanr\",\n  file = \"fits/b06.07\")\n\n## Population-Level Effects: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## a_Intercept    1.48      0.03    1.43    1.53 1.00    1565    1952\n## t_Intercept    0.00      0.03    -0.06    0.06 1.00    1669    1940\n## f_Intercept    -0.27      0.04    -0.34    -0.19 1.00    1923    2305\n## \n## Family Specific Parameters: \n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    1.45      0.10    1.26    1.67 1.00    3162    2588\n\nThe fungus variable (f-intercept)(aka post treatment effect) completely masks the effect of the treatment (t-intercept), effectively zeroing it out. \n\nWhen we include F, the post-treatment effect, in the model, we end up blocking the path from the treatment to the outcome. This is the DAG way of saying that learning the treatment tells us nothing about the outcome, once we know the fungus status. Conditioning on F induces d-separation.\nd-separation (directional separation): some variables on a directed graph are independent of others. There is no path connecting them.\n\ne.g. In this case, H1 is d-separated from T, but only when we condition on F, H1 _||_ T|F (conditional independency). Also, F _||_ H0 and H0 _||_ T (collider, H1, no conditioned on)\n\n\nAllowing treatment to influence H1\n\nAdjusting for F blocks the pipe, T-F-H1. You can calculate the direct causal effect of T on H1, but it will prevent the calculation of the total causal effect of T on H1.\nCorrect Model\n\n**In this situation, the correct model is that we do NOT adjust for F (i.e. do NOT include this variable in your model)**\nH1 ~ Normal (μi, σ) μi = H0 ⨯ pi pi = α + βTTi\nSee above for the brms model specification (although includes F) and definition of terms\n\n\nExample (adjusting for F unmasks the treatment effect)\n\nSame but Moisture, M,  (unobserved) influences both Fungus, F, and Final Height, H\nA regression of H1 on T will show no association between the treatment and plant growth. But if we include F in the model, suddenly there will be an association.\nFungus confounds inference about the treatment, this time by making it seem like having Fungus helped the plants, even though it had no effect.\n\nCollider bias is a type of confounding.\n\n\nCollider Bias\n\nWhen you condition on a collider, it induces statistical—but not necessarily causal— associations among its causes.\n\nAlthough, the variables involved may be useful for predictive models as the backdoor paths do provide valid information about statistical associations within the data\n\nExample (non-misleading)\n\nAdjusting for S (collider) induces an association between T and N.\nOnce you learn that a grant proposal has been selected (S), then learning its trustworthiness (T) (i.e. the rigorousness of the experimental design) also provides information about its newsworthiness (N) (i.e. importance of the experiment to the public).\n\nWe only see the selected grants not the failed grant proposals (i.e. selection bias, adjusting by S), so while there is no association between T and N in the population, if we look at the subset of selected grants (S), we will see an association.\n\n\nExample (misleading)\n\nColliders, by inducing a statistical association, can also lead to misleading inferences.\n(fictitious scenario) Happiness, H, and Age, A, are independent of one another, but are influences of Marriage, M.\nAdjusting for M (collider) induces a spurious association between H and A which would lead to the false inference that Happiness changes with Age\nOnce we know whether someone is married or not, then their age does provide information about how happy they are. But this is a statistical association and not a causal association.\n\n\nMore DAGS (sections 6.3 and 6.4)\n\nSee My Appendix for discussion on section 6.3’s more advance Simpson’s Paradox Example\n\nSimpson’s Paradox is a statistical phenomenon where adding a conditioning variable(s) changes the effect and interpretation of the variable of interest. The variables that are causing the change are adding backdoor paths. So, the scientific question and its DAG determine causality. An Example of why you just can’t keep adding control variables.\n\nSee Causal Inference note\n\nStatistical Rethinking &gt;&gt; Shutting the Backdoor - details and Examples for “shutting the backdoor” to confounding paths\nHandling unobserved confounds\n\nMisc &gt;&gt; Partial Identification\nStructural Causal Models &gt;&gt; (Bayesian) Examples\n\n\nSee the book’s 2nd Example (Waffle House and Divorce) for good walkthrough on designing a DAG\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong.**\n\ne.g. testing conditional independencies of a DAG with the data can invalidate a DAG.\nMe: Experiment and theory are needed to also prove a DAG is right.\n\nI think that’s right — that “proving a DAG right” and making a valid causal inference are two different things\n\nfrom 6.3, “If you don’t have a causal model, you can’t make inferences from a multiple regression. And the regression itself does not provide the evidence you need to justify a causal model. Instead, you need some science”\nFrom summary: Although the DAG and data modeling “allow you to make valid casual inferences.”\nProving the causal model right is the endgame, but making a valid causal inference is just saying, “The evidence from the data doesn’t dispute the causal model (i.e. DAG) that X causes Y.”\n\n\n\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions.\n\nThese models can still be analyzed and have causal interventions designed from them.\n\nMultivariable regression models describe conditional associations, not causal influences. Therefore additional information, from outside the model, is needed to make sense of it (e.g. the DAG/causal model).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-7---information-theory-prediction-metrics-model-comparison",
    "href": "qmd/statistical-rethinking.html#chapter-7---information-theory-prediction-metrics-model-comparison",
    "title": "1  Statistical Rethinking",
    "section": "1.6 Chapter 7 - Information Theory, Prediction Metrics, Model Comparison",
    "text": "1.6 Chapter 7 - Information Theory, Prediction Metrics, Model Comparison\n\noverfitting and underfitting\n\nPolynomial fits can result in overfitting\nAdding variables always increases R2\nunderfitting produces bad results for in-sample and out-of-sample data\n\nTerms\n\ntarget: a criterion for model performance\n\nDetermine by:\n\nCost-benefit analysis.\n\nHow much does it cost when we’re wrong (false positives, false negatives)?\nHow much do we win when we’re right (true positives, true negatives)?\n\nAccuracy in context: a metric that generalizes GOF between models and datasets\n\n\ndeviance: an approximation of relative distance from perfect accuracy\nInformation: The reduction in uncertainty when we learn an outcome.\n\nInformation Theory seeks a measure of uncertainty that satisfies three criteria: \n\nThe measure should be continuous\nIt should increase as the number of possible events increases\nIt should be additive\n\nInformation Entropy - The uncertainty contained in a probability distribution is the average log-probability of an event.\n\nwhere there are n different possible events and each event i has probability pi\nThe smaller the probability of an event, pi, the larger the uncertainty, H, is.\nA precise definition of “uncertainty” with which we can provide a baseline measure of how hard something is to predict, as well as how much improvement is possible.\nExample (information entropy for the weather):\n\nIf true probabilities of rain and shine are p1 = 0.3 and p2 = 0.7, respectively H(p) = − (p1 log(p1) + p2 log(p2)) ≈ 0.6\nIf true probabilities of rain and shine are p1 = 0.01 and p2 = 0.99, respectively H(p) = − (p1 log(p1) + p2 log(p2)) ≈ 0.06\n\nWith the huge imbalance in probabilities there’s less uncertainty and therefore a lower entropy\n\n\n\nKullback-Leibler Divergence (K-L Divergence)\n\nThe additional uncertainty induced by using probabilities from one distribution to describe another distribution.\nThe average difference in log probability between the target (p) and model (q) (i.e. difference of two entropies in “units of entropy”)\n\np is the truth and q is an approximation\nWe don’t actually know the “truth” of course, so this whole thing is theoretical\n\nInformation criteria estimate this theoretical value to compare models for predictive accuracy.\nThe “truth”, p,  gets sorta gets subtracted out when we compare two models (see book for details)\n\nWhen p = q, we know the actual probabilities of the events\n\nThe divergence helps us contrast different approximations (i.e. models) of p as a distance measure\n\nThe smaller the divergence, DKL, the better the model.\n\nDKL can also be defined using cross-entropy: DKL(p, q) = H(p, q) − H(p)\n\nwhere cross-entropy, H(p, q) = − Σ pi log(qi)\n\nDKL(p,q) != DKL(q, p)\nside note (used in chapter 10): If we use a distribution with high entropy to approximate an unknown true distribution of events, we will reduce the distance to the truth and therefore the error.\n\nLog Pointwise Predictive Density (lppd)\n\nSum of the log average probabilities\n\nHigher is better\n\nSteps (per model)\n\nTake S samples of the posterior density for y, i.e. p(yi|Θ) where i ∈ S\nFor each observation, average the sampled probabilities, then take the log of the average\nSum the log of the averages for all observations\n\n\n\n# sample the posterior\nn_samples &lt;- 1000\npost &lt;- extract.samples(mod,n=n_samples)\n\n# output: 50 x 1000 matrix\n# for each set of sampled parameter values, the log-probability of each outcome obs, given that set of parameter values, is calculated. So 1000 log-probabilities for each observation.\nlogprob &lt;- sapply( 1:n_samples ,\n                  function(s) {\n                        mu &lt;- post$a[s] + post$b[s]*cars$speed\n                        dnorm( cars$dist , mu , post$sigma[s] , log=TRUE )\n    } )\nn_obs &lt;- nrow(cars)\n# output: vector, length(lppd) = 50\n# for each observation, the average log-probability is calculated\nlppd &lt;- sapply( 1:n_obs , function(i) log_sum_exp(logprob[i,]) - log(n_samples) )\n\n# The model's lppd is the sum of all the observation-level lppds\nsum(lppd)\n\nFor precision, all of the averaging done on the log scale.\n\nlog_sum_exp, computes the log of a sum of exponentiated terms\nSubtracting the log of n_samples is like dividing by n_samples for the typical average calculation.\n\nThe log average probability (most of the right side of the equation) is an approximation for E log (model q), aka the entropy\n\nSo the difference (i.e. comparing) between the lppd of two models is like the K-L divergence of two models\n\nBy itself, the lppd value isn’t interpretable. It only makes sense in comparison to another model’s lppd.\nDeviance = -2 lppd\n\nIt’s negative so that lower is better\nThe “2” is there for historical reasons. In non-Bayesian statistics, under somewhat general conditions, a difference between two deviances has a chi-squared distribution. The factor of 2 is there to scale it the proper way. Then, you can check an see if two models are statistically significantly different from each other.\n\nSaw this in discrete analysis with poisson and log-linear models where deviance uses log-likelihood instead of lppd\n\n** Has the same flaw as R2, in that the more complex the model is, the better the score. BUT unlike R2, if used with train/test splits, it will tend to select the better model in the test set.**\n\ni.e. it’s a predictive metric and not a GOF metric.\n\n\nSkeptical priors reduce overfitting\n\nWith flat or nearly flat priors, the model interprets every parameter value is equally plausible\n\nResults in a posterior that encodes as much of the training sample—as represented by the likelihood function—as possible. (i.e. really flexible models (e.g. polynomial) = overfitting)\n\nSkeptical priors slow the rate of learning from the sample.\nRegularizing prior\n\ntype of skeptical prior\nWhen tuned properly, reduces overfitting while still allowing the model to learn the regular features of a sample.\n\nToo skeptical results in underfitting\n\nExamples: (all sd values are considered to be skeptical)\n\nprior β ∼ Normal(0, 1) says the machine should be very skeptical of values above 2 and below −2\n\na Gaussian prior with a standard deviation of 1 assigns only 5% plausibility to values above and below 2 standard deviations.\nBecause the predictor variable x is standardized, you can interpret this as meaning that a change of 1 standard deviation in x is very unlikely to produce 2 units of change in the outcome.\nSince more probability is massed up around zero, estimates are shrunk towards zero—they are conservative.\n\n\n\nBehavior of Skeptical priors The true model has 3 parameters training set: blue; test set: black Dots and circles were models with flat priors\n\nFor a small sample (e.g. N = 20)\n\nTraining set: as the prior becomes more “skeptical” (sd = 1 –&gt; 0.2) the worse (i.e. higher) the deviance score\n\nit prevents the model from adapting completely to the sample\n\nTesting set: as the prior becomes more “skeptical” (sd = 1 –&gt; 0.2) the better (i.e. lower) the deviance score\n\nNot much difference in the score for the true model (3 parameters)\nThe most skeptical parameter’s score does stay flatter than the others for parameters &gt; 3 though\nThink I prefer N(0, 0.5)’s action. It has the same low score for the true model as the 0.2 prior and some score separation to make it less likely I might choose more parameters\n\nI might be looking at a combination of things when choosing a model, so the score separation might be helpful is dissuading me from chooseing the overfitting models.\n\n\n\nFor larger sample (e.g. N = 100)\n\nThe degree of skepticism of the prior has a much smaller, near negligible effect\n\nThe size of the effect of the skeptical prior is probably affected by how informative the predictors are as well. So sample size probably isn’t everything.\n\n\n\n\nPredictive accuracy metrics\n\nPareto-Smoothed Importance Sampling Cross-Validation (PSIS)\n\nAn out-of-sample,  LOO CV  lppd is calculated by using “importance” weighting.\n\nbrms::loo_compare or brms::loo - wrappers for loo::loo (docs)\n\n“elpd_loo” - larger is better\n“looic” - is just (-2 * elpd_loo) to convert it to the deviance scale, therefore smaller is better\n\nRethinking pkg: smaller is better\n\nThe posterior is sampled and “importance” weights are calculated for each observation based on how much the posterior distribution “would change” if that observation is removed.\n\nAn observation that is expected (i.e. more likely) is less important and given a smaller weight.\n\nAn “expected” observation is one that has a high probability in the posterior (i.e. towards the center of the mass). It’s less important, because if it’s removed, there are plenty of other points with similar information. Therefore, the posterior density wouldn’t change that much.\n\n\nThe Pareto distribution is used to make the weights calculation more reliable.\n\nObservations with very large weights can dominate the calculation of the lppd estimate\n\nThe largest weights are used to estimate a Pareto distribution. The distribution is used to “smooth” the large weights (somehow).\n\nThe shape parameter of the distribution, k, is estimated. When k &gt; 0.5, then the distribution has infinite variance. PSIS weights perform well as long as k &lt; 0.7. Large k values can be used to identify influential observations (i.e. rare observations/potential outliers).\n\nFor brms, warnings for high k values will show when using add_criterion(mod, \"loo\") \nOutliers make it tough to estimate out-of-sample accuracy, since rare values are unlikely to be in the new sample. (i.e. overfitting risk)\nAlso, warnings about high k values can occur when the sample size is small\n\nWhen looking at the posterior, keep in mind that “influential” data values might be significantly affecting the posterior distribution.\n\n\n\nResources\n\nSee Model Comparison &gt;&gt; Weight section (below) for code to compare models using PSIS\nVehtari video explaining Pareto k\nSee 11.2 pg 362 for labeling fitted vs predictor plot with high K values.\nPaper\nloo package has some nice CV workflows\n\n\nInformation Criteria\n\nInformation criteria construct a theoretical estimate of the relative out-of-sample K-L Divergence.\nAkaike Information Criterion (AIC) AIC = Dtrain + 2p = −2lppd + 2p\n\np = number of free parameters in the posterior distribution\nThe “2” is just for scaling\nSo, just the deviance + the number of (free) parameters\n\nSays the dimensionality (i.e. complexness) a model is directly proportional to it’s tendency to overfit.\n\nConditions for reliability\n\nThe priors are flat or overwhelmed by the likelihood (?).\nThe posterior distribution is approximately multivariate Gaussian.\nThe sample size, N, is much greater than the number of parameters, k.\n\n** Flat priors are rarely the best priors, so AIC not really useful. But it’s the inspiration for other metrics. **\n\nWidely Applicable Information Criterion (WAIC)\n\nPerformance is very similar to PSIS but doesn’t have as many diagnostic features as PSIS.\n\ny is the observations, Θ is the posterior distribution\nIt’s deviance + a penalty that’s proportional to the variance of posterior density\n\nloo pkg:\n\n“elpd_waic”: larger is better\n“waic”:  is just (-2 * elpd_waic) to convert it to deviance scale, therefore smaller is better\n\nRethinking pkg: smaller is better\n\nEffective number of parameters, pwaic (aka the penalty term or overfitting penalty)\n\nSays compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.\nCalled such because in ordinary linear regressions the sum of all penalty terms from all points tends to be equal to the number of free parameters in the model\n\nWhen the sum is larger than the number of free parameters, it can indicate an outlier is present which will increase the overfitting risk.\n\n\n\n\n\n\n\n# see lppd code above\n# takes the variance of each observation's row of log-probabilities\n# output: vector, length = 50\npWAIC &lt;- sapply( 1:n_cases , function(i) var(logprob[i,]) )\n# Subtracts the penalty term, pWAIC, from the model lppd; So, this looks like a penalyzed deviance score\nWAIC &lt;- -2*( sum(lppd) - sum(pWAIC) )\n\n# brms funciton\n# output: waic, p_waic, elpd_waic, and all their standard errors\nbrms::waic(mod)\n\n# pointwise waic\nbrms::waic(mod)$pointwise\n\nNo assumption about the shape of the posterior distribution\nOut-of-sample deviance approximation that asymptotically converges to the cross-validation approximation\nout-of-sample accuracy to be normally distributed with mean equal to the reported WAIC value and a standard deviation equal to the standard error\nStandard Error: \n\nN = sample size, var = variance, p = penalty term for observation i\n\n\n# see lppd and waic code above\nwaic_vec &lt;- -2*( lppd - pWAIC )\ns_waic &lt;- sqrt( n_cases*var(waic_vec) )\n\nBayes Factor\n\nThe ratio of the average likelihoods (the denominator of bayes theorem) of two models.\n\nOn the log scale, these ratios are differences\n\nSince the average likelihood has been averaged over the priors, it has a natural penalty for more complex models\nProblems\n\nEven when priors are weak and have little influence on posterior distributions within models, priors can have a huge impact on comparisons between models.\nNot always possible to compute the average likelihood\n\n\nModel Comparison\n\nFor inference, scientific considerations already select the relevant model. Instead comparison is about measuring the impact of model differences while accounting for overfitting risk.\n** Can’t use any information criteria prediction metrics to compare models with different likelihood functions ** (Ch 10)\n** Don’t compare models with different numbers of observations ** (Ch 11)\n\ne.g. 1/0 logistic regression model vs aggregated logistic regression model\n\nLeave-one-out cross-validation (LOO-CV)\n\nHas serious issues, I think (see Vehtari paper for recommendations, (haven’t read it yet))\n\nTo judge whether two models are “easy to distinguish” (i.e. kinda like whether their scores are statistically different), we look at the differences between the model with the best WAIC and the WAICs of the models (waic_diff) along with the standard error of the difference of the WAIC scores (waic_se_diff).\n\nOr the substantively same score, elpd_waic\nloo_compare lists the best model first, then the next best model, etc. according to the criterion arg\n\n\n\n# add waic calculation to brms model objs\nb6.7 &lt;- add_criterion(b6.7, criterion = \"waic\")\nb6.8 &lt;- add_criterion(b6.8, criterion = \"waic\")\n\n# compare the WAIC estimates  (can be more than 2 models)\nw &lt;- brms::loo_compare(b6.7, b6.8, criterion = \"waic\")\nprint(w, simplify = F)\n##      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  se_waic\n## b6.7    0.0      0.0  -180.7      6.7          3.5    0.5    361.5  13.4 \n## b6.8  -20.5      4.9  -201.2      5.4          2.5    0.3    402.5  10.8 \n\nelpd waic = lppd - p_waic. So it’s just WAIC without the conventional multiplication by -2.\n\nhigher is better now though\nInstead of waic se differences, elpd waic se differences are used, which is the substantively the same thing\n\nloo package authors aren’t fans of the frequentist -2* convention.\n\n“These standard errors, for all their flaws, should give a better sense of uncertainty than what is obtained using the current standard approach of comparing differences of deviances to a Chi-squared distribution, a practice derived for Gaussian linear models or asymptotically, and which only applies to nested models in any case.”\n\nTherefore, under se_diff, we see 4.9 is our comparison value to focus on.\n\n\nTo get the waic standard error differences used in the book, we do the following:\n\nwaic_diff = elpd_diff * -2\nwaic_se_diff = se_diff * 2\n\n\n# manual calcluation of waic_se_diff using rethinking::WAIC\nwaic_m6.7 &lt;- WAIC( m6.7 , pointwise=TRUE )$WAIC\nwaic_m6.8 &lt;- WAIC( m6.8 , pointwise=TRUE )$WAIC\nn &lt;- length(waic_m6.7)\ndiff_m6.7_m6.8 &lt;- waic_m6.7 - waic_m6.8\nsqrt( n*var( diff_m6.7_m6.8 ) )\n\n# brms\nn &lt;- length(b6.7$criteria$waic$pointwise[, \"waic\"]) \ntibble(waic_b6.7 = b6.7$criteria$waic$pointwise[, \"waic\"],\n      waic_b6.8 = b6.8$criteria$waic$pointwise[, \"waic\"]) %&gt;% \n  mutate(waic_diff = waic_b6.7 - waic_b6.8) %&gt;% \n  summarise(waic_diff_se = sqrt(n * var(diff)))\n\n# 99% interval (2.6 is 99% z-score) around waic_diff\nwaic_diff + c(-1,1)*waic_se_diff*2.6\n## 12.96 67.04\n\nThe lower end of the interval (12.96) is far above zero, so the models are substantially different.\nWeight - These weights can be a quick way to see how big the differences are among models.\n\nΔi is the difference in WAIC or PSIS between model i and the model with the best WAIC or PSIS score\n\nFor WIAC, the waic_diff scores above\n\nΔj is the Δ for model j\nAll the model weights being compared sum to 1.\nEach model weight is essentially a proportion of it’s WAIC or PSIS difference compared to the total of all the WAIC or PSIS differences.\n\nLarger is better\n\nUse case with PSIS weights from Ch 8\n\n\n# \"loo\" is for PSIS\nb8.1b &lt;- add_criterion(b8.1b, \"loo\")\nb8.2 &lt;- add_criterion(b8.2, \"loo\")\n# not sure why he included \"waic\" here; maybe for some other later calc\nb8.3 &lt;- add_criterion(b8.3, c(\"loo\", \"waic\")) \nloo_compare(b8.1b, b8.2, b8.3, criterion = \"loo\") %&gt;%\nprint(simplify = F)\n##      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\n## b8.3    0.0      0.0  129.6      7.3        5.0    0.9  -259.2  14.7 \n## b8.2    -3.5      3.2  126.1      7.4        4.1    0.8  -252.2  14.8 \n## b8.1b  -35.2      7.5    94.4      6.5        2.6    0.3  -188.8  13.0\n\nbrms::model_weights(b8.1b, b8.2, b8.3, weights = \"loo\") %&gt;%\n        round(digits = 2)\n## b8.1b  b8.2  b8.3 \n##  0.00  0.03  0.97\n\n# shows k values for all data points below 0.5 threshold\nloo::loo(b8.3) %&gt;% \n  plot()\n# K values\ntibble(k = b8.3$criteria$loo$diagnostics$pareto_k,\n      row = 1:170) %&gt;% \n  arrange(desc(k))\n# k value diagnostic table - shows how many are points have bad k values and that group's min n_eff\nloo(b8.3) %&gt;% loo::pareto_k_table()\n\nIn the comparison table, “looic” is the PSIS score\nb8.3 has more than 95% of the model weight. That’s very strong support for including the interaction effect, if prediction is our goal.\nb8.3 is a little overfit\n\nThe modicum of weight given to b8.2 suggests that the posterior means for the slopes in b8.3 are a little overfit.\nThe standard error of the difference in PSIS between the top two models is almost the same as the difference itself\n\nNot shown here. Calculation same as to what’s shown in waic_diff section.\n\n\nOutliers\n\nDetection\n\nHigh p_waic (WAIC) and k (PSIS) values can indicate outliers\n\nRare values have greater influence on the posterior. (somehow this translates into a larger p_waic)\n\nThe posterior changes a lot when they are dropped from the sample\n\nk values &gt; 0.5 will trigger a warning, but not until values exceed 0.7 do they indicate an outlier (See PSIS for details)\n\n\nSolutions\n\nIf there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay.\nIf there are several outliers, then a form of Robust Regression can be used or a Mixture Model.\n\nCommon to use a Student’s T distribution instead of a Gaussian for the outcome variable specification\n\nThe Student-t distribution arises from a mixture of Gaussian distributions with different variances. If the variances are diverse, then the tails can be quite thick.\nHas an extra shape parameter, ν, that controls how thick the tails are.\n\nν = ∞ is a Gaussian distribution\nAs v –&gt; 1+ , tails start becoming fat\nν can be estimated with very large datasets that have plenty of rare events",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-8---interactions",
    "href": "qmd/statistical-rethinking.html#chapter-8---interactions",
    "title": "1  Statistical Rethinking",
    "section": "1.7 Chapter 8 - Interactions",
    "text": "1.7 Chapter 8 - Interactions\n\nInteractions estimate the association between a predictor and an outcome when that association depends upon the value of another predictor.\n\nCan’t be shown in DAGs\n\nProcessing\n\nDemeaning a continuous interaction variable removes it’s collinearity with the main effects (also see Regression, Interactions &gt;&gt; Processing)\n\nSpecification\n\nMcElreath says his indexing approach and the traditional way of specifying an interaction are equivalent\n\nIndexing keeps the estimate’s uncertainty (i.e. std.errors) the same for each category (see ch 5 &gt;&gt; categoricals for details).\n\nSince the uncertainty will be the same for all the categories with indexing, it’s easier make sensible priors for the variable’s parameter\n\nAlso, 1 prior can be used for all categories.\n\n\nExample: ruggedness x gid (cont x cat) interaction (also see below)\n\nTraditional (sort of, it’s algabraically manipulated some)\n\nAfter moving (r-r_bar) inside the parenthesis, γ is the interaction coefficient\n\nIndexing\n\nThe CID subscript shows that both the intercept and the slope are indexed by CID.\n\n\nIndexing just reparameterizes the model output (See continuous x categorical Example for details)\n\nInstead of contrasts with the traditional specification, you get marginal means (intercepts) and marginal effects (interaction slopes) with the indexing specification\nSee Regression, Interactions &gt;&gt; Terms for definitions of marginal means and marginal effects\n\n\n\nInteraction between a continuous predictor and a categorical predictor\n\nCoding this type of interaction is done by indexing both intercept and slope (similar to coding categoricals in Ch 5)\noutcome: log GDP; predictors: ruggedness (continuous), cid (binary: africa/not africa)\nIndex Specification\n\n\ndata(rugged, package = \"rethinking\")\ndd &lt;- rugged %&gt;%\n    filter(complete.cases(rgdppc_2000)) %&gt;% \n  # re-scale variables\n    mutate(log_gdp = log(rgdppc_2000),\n          log_gdp_std = log_gdp / mean(log_gdp),\n          # scale as percent-of-max-value, [0,1] \n          rugged_std  = rugged / max(rugged),\n          rugged_std_c  = rugged_std - mean(rugged_std),\n          # index variable: Africa/not-Africa\n          cid = if_else(cont_africa == 1, \"1\", \"2\"))\n\n# same as coding for cat vars except adding a slope that is also conditioned on the index\nb8.3 &lt;- \n  brm(data = dd, \n      family = gaussian,\n      # bf = \"brms formula\"\n      bf(log_gdp_std ~ 0 + a + b * rugged_std_c, \n        a ~ 0 + cid, \n        b ~ 0 + cid,\n        # nl = \"nonlinear\" syntax\n        nl = TRUE),\n      prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 8, backend = \"cmdstanr\",\n      file = \"fits/b08.03\")\nb8.3\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cid1    0.89      0.02    0.86    0.92 1.00    4259    2991\na_cid2    1.05      0.01    1.03    1.07 1.00    4853    3142\nb_cid1    0.13      0.08    -0.02    0.28 1.00    4156    2613\nb_cid2    -0.14      0.05    -0.25    -0.03 1.00    4112    2957\n\nCan also use a syntax shortcut, a + b ~ 0 + cid instead of separate equations (see Ch. 11, Poisson Example)\nTraditional Specification\n\nlibrary(rstanarm); library(marginaleffects); library(dplyr)\nstan_mod &lt;- stan_glm(log_gdp_std ~ rugged_std_c * cid,\n                    data = dd,\n                    family = gaussian(link = \"identity\"),\n                    seed = 12345)\nsummary(stan_mod)\n                  mean  sd  10%  50%  90%\n(Intercept)        0.9    0.0  0.9  0.9  0.9 \nrugged_std_c      0.1    0.1  0.0  0.1  0.2 \ncid2              0.2    0.0  0.1  0.2  0.2 \nrugged_std_c:cid2 -0.3    0.1 -0.4  -0.3  -0.2 \nsigma              0.1    0.0  0.1  0.1  0.1\n\npredictions(stan_mod, variables = \"cid\")\n  rowid    type predicted rugged_std_c cid  conf.low conf.high\n1    1 response 0.8842526 1.558369e-17  1 0.8527271 0.9133909\n2    2 response 1.0510526 1.558369e-17  2 1.0309829 1.0716412\n\nmarginaleffects(stan_mod) %&gt;%\n  tidy(by = \"cid\")\n      type        term contrast cid  estimate    conf.low  conf.high\n1 response rugged_std_c    dY/dX  1  0.1365192 -0.01539749  0.29222466\n2 response rugged_std_c    dY/dX  2 -0.1477665 -0.25793153 -0.03938909\n3 response          cid    2 - 1  1  0.1773914  0.14153747  0.21552476\n4 response          cid    2 - 1  2  0.1630014  0.12717283  0.20008143\n\nThe marginal means, a_cid1 & a_cid2, from the index model are reproduced using the rstanarm model and marginaleffects::predictions\nThe marginal effects, b_cid1 & b_cid2, are the estimates given in the first 2 lines of the output from marginaleffects::marginaleffects using the rstanarm model\n\nThese values are the slopes of these regression lines (fig 8.2)\nResults if you were to fit a regression model for each subset of data according the value of “cid” (i.e. Africa/Non-Africa)\n\nInterpretation\n\nGiven a country is in Africa,\n\nand with ruggedness at it’s median, the average log GDP is 0.88\na 1 unit increase in ruggedness results in a 0.13 increase in log GDP on average.\n\n\nInteractions are symmetric\n\nNormally we interpret the interaction as the association of the continuous predictor varying by or conditioned on the categorical predictor, but the reverse is also valid.\nExample: the difference in expected log GDP between a nation in Africa and outside Africa, conditional on geographical ruggedness\n\ni.e. interpreting model output where it’s the association between log GDP and Africa/non-Africa that’s conditional on ruggedness\n\nCounterfactual plot\n\n\nnd &lt;- \n  crossing(cid        = 1:2,\n          rugged_std = seq(from = -0.2, to = 1.2, length.out = 30)) %&gt;% \n  mutate(rugged_std_c = rugged_std - mean(dd$rugged_std))\n\nfitted(b8.3, \n      newdata = nd,\n      summary = F) %&gt;%\n  data.frame() %&gt;%\n  pivot_longer(everything()) %&gt;% \n  bind_cols(expand(nd,\n                  iter = 1:4000,\n                  nesting(cid, rugged_std))) %&gt;% \n  select(-name) %&gt;% \n  pivot_wider(names_from = cid, values_from = value) %&gt;% \n  mutate(delta = `1` - `2`) %&gt;%   \n  ggplot(aes(x = rugged_std, y = delta)) +\n  stat_lineribbon(.width = .95, fill = palette_pander(n = 8)[8], alpha = 3/4) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  annotate(geom = \"text\",\n          x = .2, y = 0,\n          label = \"Africa higher GDP\\nAfrica lower GDP\",\n          family = \"Times\") +\n  labs(x = \"ruggedness (standardized)\",\n      y = \"expected difference log GDP\") +\n  coord_cartesian(xlim = c(0, 1),\n                  ylim = c(-0.3, 0.2))\n\n\nWe are seeing through the model’s eyes and imagining comparisons between identical nations inside and outside Africa, as if we could independently manipulate continent and also terrain ruggedness.\nBelow the horizontal dashed line, African nations have lower than expected GDP. This is the case for most terrain ruggedness values.\nAt the highest ruggedness values, a nation is possibly better off inside Africa than outside it. Really it is hard to find any reliable difference inside and outside Africa, at high ruggedness values. It is only in smooth nations that being in Africa is a liability for the economy.\nInteraction between continuous predictors Bi ∼ Normal(µi , σ) µi = α + βWWi + βSSi + βWSWiSi\n\nSee pgs 260-61 for details on why the interaction prior is the same as the main effects prior\n\nI’m not sure how this reasoning works if all the predictors do NOT have the same prior.\n\nFor a continuous interaction, you probably have to make sure all predictors involved in the interaction have been transformed so they have the same prior.\n\n\n\n\nb8.5 &lt;-\n  brm(data = d, \n      family = gaussian,\n      blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent,\n      prior = c(prior(normal(0.5, 0.25), class = Intercept),\n                prior(normal(0, 0.25), class = b, coef = water_cent),\n                prior(normal(0, 0.25), class = b, coef = shade_cent),\n                prior(normal(0, 0.25), class = b, coef = \"water_cent:shade_cent\"),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 8,\n      file = \"fits/b08.05\")\n\nTriptych Plots\n\nType of facetted predictor (one of the interaction variables) vs fitted graph where you facet by bins, quantiles, levels of the other interaction variable.\n\n\npoints &lt;-\n  d %&gt;%\n  expand(fit = c(\"b8.4\", \"b8.5\"),\n        nesting(shade_cent, water_cent, blooms_std)) %&gt;%\n  mutate(x_grid = str_c(\"shade_cent = \", shade_cent),\n        y_grid = fit)\n# redefine `nd`\nnd &lt;- crossing(shade_cent = -1:1, \n              water_cent = c(-1, 1))\n# use `fitted()`\nset.seed(8)\nrbind(fitted(b8.4, newdata = nd, summary = F, nsamples = 20),\n      fitted(b8.5, newdata = nd, summary = F, nsamples = 20)) %&gt;%\n  # wrangle\n  data.frame() %&gt;%\n  set_names(mutate(nd, name = str_c(shade_cent, water_cent, sep = \"_\")) %&gt;% pull()) %&gt;%\n  mutate(row = 1:n(),\n        fit = rep(c(\"b8.4\", \"b8.5\"), each = n() / 2)) %&gt;%\n  pivot_longer(-c(row:fit), values_to = \"blooms_std\") %&gt;%\n  separate(name, into = c(\"shade_cent\", \"water_cent\"), sep = \"_\") %&gt;%\n  mutate(shade_cent = shade_cent %&gt;% as.double(),\n        water_cent = water_cent %&gt;% as.double()) %&gt;%\n  # these will come in handy for `ggplot2::facet_grid()`\n  mutate(x_grid = str_c(\"shade_cent = \", shade_cent),\n        y_grid = fit) %&gt;%   \n  # plot!\n  ggplot(aes(x = water_cent, y = blooms_std)) +\n  geom_line(aes(group = row),\n            color = palette_pander(n = 6)[6], alpha = 1/5, size = 1/2) +\n  geom_point(data = points,\n            color = palette_pander(n = 6)[6]) +\n  scale_x_continuous(\"Water (centered)\", breaks = c(-1, 0, 1)) +\n  scale_y_continuous(\"Blooms (standardized)\", breaks = c(0, .5, 1)) +\n  ggtitle(\"Posterior predicted blooms\") +\n  coord_cartesian(xlim = c(-1, 1),\n                  ylim = c(0, 1)) +\n  theme(strip.background = element_rect(fill = alpha(palette_pander(n = 2)[2], 1/3))) +\n  facet_grid(y_grid ~ x_grid)\n\n\nModel without interaction (top row): shows that water (predictor) helps—there is a positive slope in each plot—and that shade hurts—the lines sink lower moving from left to right. But the slope with water doesn’t vary across shade levels. Without the interaction, it cannot vary.\nModel with interaction (bottom row): The slope is allowed to vary. At higher light levels, water can matter more, because the tulips have enough light to produce blooms. At very high light levels, light is no longer limiting the blooms, and so water can have a much more dramatic impact on the outcome.\nChoosing facet values: generally, you might use a representative low value, the median, and a representative high value or quantile values for the facets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-9---mcmc",
    "href": "qmd/statistical-rethinking.html#chapter-9---mcmc",
    "title": "1  Statistical Rethinking",
    "section": "1.8 Chapter 9 - MCMC",
    "text": "1.8 Chapter 9 - MCMC\n\nThe search for a sampling algorithm that can produce a good description of the posterior in the fewest amount of samples of the joint posterior distribution.\nQuadratic approximation, quap( ), assumes the posterior is a multivariate gaussian. These algorithms allow us to drop that assumption.\nGibbs sampling is more efficient than the Metropolis-Hastings algorithm.\n\nIt computes something called adaptive proposals_,_ which allows it to make smart jumps around the joint posterior distribution of all parameters. This results in a good image of the posterior but in fewer steps than the Metropolis-Hastings algorithm.\n\nadaptive proposals depend upon using particular combinations of prior distributions and likelihoods known as conjugate pairs.\n\nProposals are potential new parameter combinations in the posterior for the algorithm to jump to next.\n\nConjugate pairs are prior distributions that have the same posterior distribution.\n\nThey have analytical solutions which makes them faster.\nSome conjugate priors are actually pathological in shape, once you start building multilevel models and need priors for entire covariance matrices this becomes a problem.\n\n\nVery inefficient for complex models with hundreds or thousands of parameters\n\nThey gets stuck in small regions of the posterior because complex models have high correlation regions.\n\nSee Chapter 6, multicollinearity. When parameters value combinations contain roughly the same information, they become equally likely. This creates confusion for Gibbs and it gets stuck in these areas for longer periods of time.\nHandles U-turn problem (see below) by using a burn-in phase. Unlike HMC’s warm-up phase, burn-in uses actual samples from the joint posterior density.\n\n\n\nHamiltonian Monte Carlo (or Hybrid Monte Carlo, HMC)\n\nRequires more computation time for each sample of the joint posterior but needs fewer samples.\nKing Monty’s Royal Drive (Parable that describes how HMC samples the joint posterior) \n\nThe kingdom is valley that runs North and South. King must visit towns but the amount of visits must be in proportion to the towns population. Towns at higher elevations have lower population that towns at lower elevation. Y axis is elevation I think and +/- is North/South. A point is a town where the king visits.\nThe journey begins at time 1 on the far left. The vehicle is given a random momentum and a random direction, either north (top) or south (bottom). The thickness of the path shows momentum at each moment. The vehicle travels, losing momentum uphill or gaining it downhill. After a fixed amount of time, they stop and make a visit, as shown by the points (aka samples). Then a new random direction and momentum is chosen. In the long run, positions are visited in proportion to their population density\n\nThe valley bottom at zero represents the region of the joint posterior where the parameter combinations have the highest likelihoods. Each new parameter value combination (point in chart) is the result of a random direction (north or south) and random momentum being chosen.\nThink the thickness of the regions represent the amount of probability density. Also, it requires larger and larger amounts of “momentum” to reach the regions of lower probability density.\nThis chart (aka trace plot) represents one chain with 19 samples (points) from the joint posterior\n\n\nHamiltonian differential equations are what drives this algorithm. Actually runs a physics simulation. The king’s car is a frictionless particle moving along a gradient surface. The combination of parameter values is the particle’s multi-dimensional position.\nRequires continuous parameters\nSteps (see pg 282 for the gradient math and endnote 143 for moar math detail)\n\nCalculate the negative log-probability of every combination of parameter values. (aka elevation of towns, top part of Bayes equation)\nCalculate the gradient at the current position. This is the solution to the partial derivative of the equation that computes the negative log-probability for a given position.\n\nHyperparameters (usually chosen for you)\n\nLeapfrog Steps (L)\n\nEach path between parameter combinations (towns) is made up of leapfrog steps. This value is the number of leapfrog steps in each path. The larger this value, the longer the paths.\n\nStep Size (ε)\n\nThe size of each leapfrog step\nDetermines granularity of the simulation. Small step sizes means the particles (king’s car) in the simulation can turn sharply. Large step sizes means desirable points can be overshot.\nadapt_delta parameter\n\nthe target average proposal acceptance probability in the adaptation, which is used to determine the step size during warmup\nA higher desired acceptance probability (closer to 1) reduces the the step size.\nA smaller step size means that it will require more steps to explore the posterior distribution.\n\n\nTree-depth\n\nNumber of leapfrog steps to take in each iteration\nA too-small maximum tree-depth only affects efficiency\n\nExploration will be slower and the autocorrelation higher (effective sample size lower) than if the maximum tree-depth were set higher\n\n\n\nLecture 8 has a really nice animation of the HMC sampling process with leapfrog steps, stepsize, etc.\n\nLeft: Gaussian posterior; Right: posterior from 2 parameters of a hierarchical model\nBlue segments are the leapfrog steps\n\nThere can be many steps between sampled values\n\nThe length of each blue segment is the step size\nBlack circles are the sampled parameter values\n\nU-turn problem\n\nWhen L and ε are too large, parameter value combinations can be sampled close to each other. This leads to correlated samples which is bad (see Gibbs sampling above and Chapter 6 multicollinearity).\n\nLeads to having paths where that lead up the gradient, u-turns, and back down the gradient to the same region of the posterior where the path started.\n\nSolution\n\nwarm-up - rstan conducts a warm-up phase to determine appropriate an L and ε\n\nThis simulation tuning usually takes longer than the actual HMC sampling\n\nNUTS (No U-turn Samplers): It guesses the shape of the posterior. Using this guess, it can determine when the path is turning back towards the previous region and then adapts the L value.\n\nStan uses NUTS2\n\n\n\nSampling Diagnostics\n\nVehtari, Gelman (2019) paper on this ESS and Rhat, https://arxiv.org/abs/1903.08008?\n** HMC is easier to diagnose bad chains than Metropolis or Gibbs.\n\nGibbs doesn’t use gradients, so it doesn’t notice some issues that a Hamiltonian engine will\n\nWith Gibbs you won’t get informative warnings when sampling goes bad\n\n\nEffective Sample Size (ESS)\n\nMeasures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample.\n\nMarkov chains are typically autocorrelated, so that sequential samples are not entirely independent.\nMore autocorrelation means fewer effective number of samples.\nAutocorrelated samples provide similar information, so a chain with autocorrelation is less efficient\n\nlarger is better (for all ESS metrics)\nn_eff in {rethinking} precis output\n\nTells you how many samples the chain would have if there was 0 autocorrelation between samples in the chain\nAlmost always smaller than the number of samples specified in the model function\n\nbrms returns Bulk_ESS and Tail_ESS in the model output summary\n\nChains don’t uniformly converge across the parameter space\nBulk_ESS - effective sample size around the bulk of the posterior (i.e. around the mean or median) (same as McElreath’s n_eff)\n\n“assesses how well the centre of the distribution is resolved”\n\ni.e. measures how well HMC sampled the posterior around the bulk of the distribution in order to determine its shape.\n\nWhen value is much lower than the actual number of iterations (minus warmup) of your chains, it means the chains are inefficient, but possibly still okay\n\nIn the Example, 2000 total samples with 1000 of those used for warm-up which is brms default. 4 chains x 1000 samples = 4000 post-warm-up samples. So for each parameter, the ESS should be around that or above\n\n\nTail_ESS - effective sample size in the tails of the posterior\n\ni.e. measures how well HMC sampled the posterior in the tails of the distribution in order to determine their shape.\nestimates the Monte Carlo Standard Error (MCSE) for samples in posterior tails\nNo idea what’s good here.\nIf you get warnings, taking more samples usually helps\n\n\n\nRhat - Gelman-Rubin convergence diagnostic\n\nestimate of the convergence of Markov chains to the target distribution\n\nChecks if the start and end of each chain explores the same region\nChecks that independent chains explore the same region\n\nRatio of variances\n\nAs total variance among all chains shrinks to the average variance within chains, R-hat approaches 1\nIf converges, Rhat = 1+\nIf value is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples.\n\nIf you draw more iterations, it could be fine, or it could never converge.\n\n\nThis diagnostic can fail for more complex models (i.e. bad chains even when value = 1)\n\nThey seem to be improving this diagnostic though. As of 2021, think there’s a version 5, Rhat5.\n\n\n\nTrace plot\n\n\n# plotting posteriors and trace plots\nbrms::plot(b9.1b)\n\n# plotting pretty trace plots (shown above)\npost &lt;- posterior_samples(b9.1b, add_chain = T) \nbayesplot::mcmc_trace(post[, c(1:5, 7)],  # we need to include column 7 because it contains the chain info \n              facet_args = list(ncol = 3), \n              size = .15) +\n      scale_color_pomological() +\n      labs(title = \"My custom trace plots\") +\n      theme_pomological_fancy(base_family = \"Marck Script\") +\n      theme(legend.position = c(.95, .2))\n\nAlso see issues section below for Example of bad trace plots\nEach parameter gets it’s own plot which shows all the chains together\nBe aware the some trace plots show the warm-up phase in the first half of the plot. In this case, the second half is the part that counts.\n\nTrace plots for bayesplot and brms are only showing 1000 samples, so they aren’t showing the warm-up phase.\nIn Kurz’s brms ebook, he shows code for visualizing the warm-ups with ggmcmc and ggplot\nI don’t think there’s much reason for looking at these except for seeing how quickly the chains move from their initial values to values in the posterior.\n\nShould look like a bunch of fat, lazy caterpillars\n\nStationarity\n\nEach chain hanging around a central region of high probability\nSeems very similar if not the same as the time series definition (white noise, constant mean and variance)\n\nGood mixing\n\nEach chain rapidly explores the full region (zig-zaggy).\n\nConvergence\n\nAll chains are stationary around same central region of high probability\n\n\nSince all chains displayed at once, this plot could obscure a pathology of one of the chains.\nTrank Plot (Trace Rank Plot)\n\npost &lt;- posterior_samples(b9.1b, add_chain = T)\npost %&gt;% \n  bayesplot::mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) +\n      scale_color_pomological() +\n      ggtitle(\"My custom trank plots\") +\n      coord_cartesian(ylim = c(25, NA)) +\n      theme_pomological_fancy(base_family = \"Marck Script\") +\n      theme(legend.position = c(.95, .2))\n\nAlso see issues section below for Example of bad trank plots\nRank all the samples for each individual parameter. Lowest sample gets rank = 1. Then make a histogram for each chain.\nAll histograms should look similar, overlap, and stay within the same range.\n\nIf a chain is consistently above or below the other chains, then that’s an issue\n\nSet-up values\n\nThe number of chains allows you to reduce the variance of your Monte Carlo estimator, which is what the sampling phase is for, but not the bias, which is what the warmup phase is for.\nwarm-up samples\n\nyou want to have the shortest warmup period necessary, so you can get on with real sampling. But on the other hand, more warmup can mean more efficient sampling.\nBut if you are having trouble, you might try increasing the warmup. If not, you might try reducing it.\nmore complicated the model, the more warm-up samples you need.\n\npost-warmup samples\n\nIf all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do\nif you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many more\nIn most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases.\nFor highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. Stan will sometimes warn you about “tail ESS,” which means the effective sample size in the tails of the posterior. In those cases, it is nervous about the quality of extreme intervals, like 95%. Sampling more usually helps.\n\nChains\n\nWhen debugging a model, use only 1 chain\nDebugging\n\nUse 1 chain\n\nSome error messages don’t display unless you use only one chain.\nWill fail with more than one chain, but the reason may not be displayed.\n\n\nChain validation\n\n3 or 4\nMake sure sure autocorrelation within the sampling isn’t an issue. (i.e. lazy caterpillars, etc.)\nWhen you run multiple Markov chains, each with different starting positions, and see that all of them end up in the same region of parameter space, it provides a check that the sampling is working correctly (Also see Starting Values in Ch 4)\n\nFinal Run\n\nOnly need 1 chain\n\nGood since within chain parallelization is now possible with brms\n\nExample:\n\nWe find out we need 1000 warmup samples and about 9000 real samples in total.\nEquivalent settings\n\nwarmup=1000 and iter=10000\n3 chains, with warmup=1000 and iter=4000\n\n3 chains repeats warm-up, which takes longer than the actual sampling\nOption 1 is faster if you have within chain parallelization\nOption 2 is faster if you can only parallelize the chains themselves\n\n\n\n\nIssues\n\nAlso see Bayes &gt;&gt; Troubleshooting HMC\nWild, wandering chain\n\nMost often caused by flat priors with not enough data\nOccurs when there are broad, flat regions of the posterior density.\nErratically samples extremely positive and extremely negative parameter values\nExample\n\npriors: alpha ~ dnorm( 0 , 1000 ) , sigma ~ dexp( 0.0001 )\n\n\n\n\n## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be careful when analysing\n## the results! We recommend running more iterations and/or setting stronger priors.\n## Warning: There were 393 divergent transitions after warmup. Increasing adapt_delta above 0.8 may\n## help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n\nIn the trace plots (post-warm up, right half), the chains seem to drift around and spike occasionally to extreme values in the thousands.\nIn the trank plots, a lot of the time, one chain is way up and another way down\n\nPairs plot shows the divergent transitions\n\nbrms::pairs(b9.2, \n      np = brms::nuts_params(b9.2),\n      off_diag_args = list(size = 1/4))\n\n\n\nEach red “x” is a divergent transition\nSometimes there are patterns to these values that gives a hint what the problem is. (But evidently not here)\nDivergent Transition - a rejected proposed parameter value in the posterior during the sampling process\n\nToo many DTs could indicate a poor exploration of the posterior by the sampling algorithm and possibly biased estimates.\n\nIf the DTs are happening in the same region of the posterior then that region isn’t being sampled by the HMC algorithm\n\nIf there are “steep” areas in the posterior, these areas can break the sampling process resulting in a “bad” proposed parameter value.\n\n\nSolution:\n\nadjust priors from flat to weakly informative: alpha ~ dnorm( 1 , 10 ) , sigma ~ dexp( 1 )\n\nNow values like 30 million are no longer as equally plausible as small values like 1 or 2.\n\nNeed more data\nIncrease adapt_delta closer to 1 (default: 0.8)\nReparameterize the model\n\nNon-identifiable parameters\n\nLikely caused by highly correlated predictors\nOften, a model that is very slow to sample is under-identified.\nExample (bad) priors: a1 ~ dnorm( 0 , 1000 ), a2 ~ dnorm( 0 , 1000 ) \nSolution\n\nweakly informative priors again: a1 ~ dnorm( 0 , 10 ), a2 ~ dnorm( 0 , 10 )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-10---glm-concepts",
    "href": "qmd/statistical-rethinking.html#chapter-10---glm-concepts",
    "title": "1  Statistical Rethinking",
    "section": "1.9 Chapter 10 - GLM Concepts",
    "text": "1.9 Chapter 10 - GLM Concepts\n\nThe principle of maximum entropy provides an empirically successful way to choose likelihood functions. Information entropy is essentially a measure of the number of ways a distribution can arise, according to stated assumptions. By choosing the distribution with the biggest information entropy, we thereby choose a distribution that obeys the constraints on outcome variables, without importing additional assumptions. Generalized linear models arise naturally from this approach, as extensions of the linear models in previous chapters.\nThe maximum entropy distribution is the one with the greatest information entropy (i.e. log number of ways per event) and is the most plausible distribution.\n\nNo guarantee that this is the best probability distribution for the real problem you are analyzing. But there is a guarantee that no other distribution more conservatively reflects your assumptions.\nmaximum entropy also provides a way to generate a more informative prior that embodies the background information, while assuming as little else as possible.\n\nBayesian inference can be seen as producing a posterior distribution that is most similar to the prior distribution as possible, while remaining logically consistent with the stated information (i.e. the data).\nGaussian\n\nA perfectly uniform distribution would have infinite variance, in fact. So the variance constraint is actually a severe constraint, forcing the high-probability portion of the distribution to a small area around the mean.\nThe Gaussian distribution gets its shape by being as spread out as possible for a distribution with fixed variance.\nThe Gaussian distribution is the most conservative distribution for a continuous outcome variable with finite variance.\n\nThe mean µ doesn’t matter here, because entropy doesn’t depend upon location, just shape.\n\n\nBinomial\n\nBinomial distribution has the largest entropy of any distribution that satisfies these constraints:\n\nonly two unordered events (i.e. dichotomous)\nconstant expected value (i.e. exp_val = sum(prob*num_events))\n\nIf only two un-ordered outcomes are possible and you think the process generating them is invariant in time—so that the expected value remains constant at each combination of predictor values— then the distribution that is most conservative is the binomial.\n\nExponential\n\nconstrained to be zero or positive\nfundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events λ, or the average displacement λ −1 .\nThis distribution is the core of survival and event history analysis\n\nGamma\n\nconstrained to be zero or positive\nlike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g. age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nshape parameter k and a scale parameter θ\nAlso used in survival\n\nPoisson\n\nCount distribution like binomial\nIf the number of trials n is very large (and usually unknown) and the probability of a success p is very small, then a binomial distribution converges to a Poisson distribution with an expected rate of events per unit time of λ = np.\nused for counts that never get close to any theoretical maximum\nsingle parameter, the rate of events λ\n\nLink functions\n\nGeneralized linear models need a link function, because rarely is there a “µ”, a parameter describing the average outcome, and rarely are parameters unbounded in both directions, like µ is.\nlogit link\n\nmaps a parameter that is defined as a probability mass, and therefore constrained to lie between zero and one, onto a linear model that can take on any real value.\nyi ∼ Binomial(n, pi) logit(pi) = α + βxi\n\nlogit(pi) = log (pi / (1 − pi))\n\nThe logit is the log-odds of an event, and the “odds” of an event are just the probability it happens divided by the probability it does not happen\n\n\nThis transformation compresses the geometry far from zero (sigmoid shape), such that a unit change on the linear scale means less and less change on the probability scale\nInverse logit (logistic) pi = exp(α + βxi) / (1 + exp(α + βxi))\n\nlog link\n\nmaps a parameter that is defined over only positive real values onto a linear model\n\nNegative values get mapped to [0,1] and positive values get mapped to &gt; 1.\n\nAn increase of one unit on the log scale means an increase of an order of magnitude on the untransformed scale.\nthe log link effectively assumes is that the parameter’s value is the exponentiation of the linear model.\nyi ∼ Normal(µ, σi) log(σi) = α + βxi\n\ninverse: σi = exp(α + βxi)\n\nsolves the problem of constraining the parameter to be positive, it may also create a problem when the model is asked to predict well outside the range of data used to fit it.\n\nEventually there’s a ceiling or change from the exponential behavior.\n\ne.g. the property damage caused by a hurricane may be approximately exponentially related to wind speed for smaller storms. But for very big storms, damage may be capped by the fact that everything gets destroyed.\n\n\n\n\nAside (Histomancy): at most what a Gaussian likelihood assumes is not that the aggregated data look Gaussian, but rather that the residuals, after fitting the model, look Gaussian. So for Example the combined histogram of male and female body weights is certainly not Gaussian. But it is (approximately) a mixture of Gaussian distributions. So after conditioning on sex, the residuals may be quite normal. Other times, people decide not to use a Poisson model, because the variance of the aggregate outcome exceeds its mean (see Chapter 11). But again, at most what a Poisson likelihood assumes is that the variance equals the mean after conditioning on predictors.\nOmitted Variable Bias\n\ncan be worse in GLMs, because even a variable that isn’t technically a confounder can bias inference, once we have a link function. The reason is that the ceiling and floor effects can distort estimates by suppressing the causal influence of a variable\n\ne.g. A unit change in a predictor variable no longer produces a constant change in the mean of the outcome variable. Instead, a unit change in xi may produce a larger or smaller change in the probability pi , depending upon how far from zero the log-odds are.\n\nSuppose for Example that two variables X and Z independently influence a binary outcome Y. If either X and Z is large enough, then Y = 1. Both variables are sufficient causes of Y. Now if we don’t measure Z but only X, we might consistently underestimate the causal effect of X. Why? Because Z is sufficient for Y to equal 1, and we didn’t measure Z. So there are cases in the data where X is small but Y = 1. These cases imply X does not influence Y very strongly, but only because we are ignoring Z. This phenomenon doesn’t occur in ordinary linear regression, because independent causes just contribute to the mean. There are no ceiling or floor effects (in theory).\n\nWhen using link functions, each parameter represents a relatAive difference on the scale of the linear model, ignoring other parameters, while we are really interested in absolute differences in outcomes that must incorporate all parameters\nCan’t use any information criteria prediction metrics to compare models with different likelihood functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/statistical-rethinking.html#chapter-11-binomial-and-poisson",
    "href": "qmd/statistical-rethinking.html#chapter-11-binomial-and-poisson",
    "title": "1  Statistical Rethinking",
    "section": "1.10 Chapter 11 Binomial and Poisson",
    "text": "1.10 Chapter 11 Binomial and Poisson\n\nMisc\n\nFor glms, all predictors moderate each other.\n\ni.e. if any one predictor as outsized influence on a prediction, the effect of the other predictors with be significantly smaller.\n\nRobust models: Beta-binomial, Gamma-Poisson (aka Negative Binomial)\n\nAlmost always better to use these types of distributions rather than the typical Binomial and Poisson distributions\nMultilevel models also have a robustness quality even without using these robust distributions.\n\n\nLogistic Regression models a 0/1 outcome and data is at the case level\n\nExample: Acceptance, A; Gender, G; Department, D Ai ~ Bernoulli(pi) logit(pi) = α[Gi, Di]\n\nBinomial Regression models the counts of a Bernoulli variable that have been aggregated by some group variable(s)\n\nExample: Acceptance counts that have been aggregated by department and gender Ai ~ Binomial(Ni, pi) logit(pi) = α[Gi, Di]\n\nParameter estimates are the same no matter whether you use Logistic or Binomial Regression\n\nIf you want to use PSIS or WAIC to compare models, use a 0/1 logistic model and NOT a Binomial regression model\n\nExample: 0/1 logistic regression\n\nObjective: Measure prosocial behavior in chimps.\nExperiment: A chimp is at a long table with a lever on the left side and the right side of table. When a lever is pulled, the chimp gets a plate of food. Depending on the lever that’s pulled, a plate of food also goes to the opposite end of the table. Treatment alternates which lever sends food to other end of the table and whether or not there’s another chimp at the far end of the table. Prosocial behavior is defined as the chimp pulling the lever that also sends food to other side of table when another chimp is present.\n\nEach chimp is exposed to each of the 4 treatments 18 times\n\n\n\n    condition prosoc_left treatment  n\n1        0          0        1    126\n2        0          1        2    126\n3        1          0        3    126\n4        1          1        4    126\n\ncondition is whether there’s a second chimp at the end of the table\nprosoc_left is whether the lever that delivers food to both chimps is on the left side of the table\ntreatment is a unique combination of condition and prosoc_left\nSpecification\n\nEach “actor” is a chimp who has to pull a lever\n\nPriors\n\nWith logistic regression, flat Normal priors aren’t priors with a high sd. (See Bayes, priors for more details)\n\nModel\n\ndata(chimpanzees, package = \"rethinking\")\nd &lt;- chimpanzees  %&gt;%\n        mutate(actor = factor(actor),\n              # trick to create treatment variable for the unique combo of two binary variables (could've used an ifelse, factor combo)\n              treatment = factor(1 + prosoc_left + 2 * condition),\n              # needed for visuals\n              labels = factor(treatment,\n                              levels = 1:4,\n                              labels = c(\"r/n\", \"l/n\", \"r/p\", \"l/p\"))\nrm(chimpanzees)\n\nb11.4 &lt;- \n  brm(data = d, \n      family = binomial,\n      bf(pulled_left | trials(1) ~ a + b,\n        a ~ 0 + actor, \n        b ~ 0 + treatment,\n        nl = TRUE),\n      prior = c(prior(normal(0, 1.5), nlpar = a),\n                prior(normal(0, 0.5), nlpar = b)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.04\")\n\nposterior_summary(b11.4)\n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## a_actor1        -0.46      0.34    -1.11    0.20 1.00    1301    2137\n## a_actor2        3.88      0.74    2.54    5.46 1.00    3918    2735\n## a_actor3        -0.75      0.33    -1.41    -0.11 1.00    1349    2298\n## a_actor4        -0.76      0.33    -1.38    -0.10 1.00    1443    2531\n## a_actor5        -0.45      0.33    -1.07    0.23 1.00    1432    2274\n## a_actor6        0.46      0.33    -0.16    1.12 1.00    1370    2222\n## a_actor7        1.95      0.42    1.15    2.80 1.00    1829    2515\n## b_treatment1    -0.03      0.28    -0.60    0.52 1.00    1196    2175\n## b_treatment2    0.48      0.29    -0.08    1.03 1.00    1231    2117\n## b_treatment3    -0.38      0.29    -0.95    0.19 1.00    1248    1966\n## b_treatment4    0.37      0.28    -0.20    0.92 1.00    1141    1950\n\nLogistic and not binomial regression because\n\nCase-level data (i.e. NOT aggregated in counts by group variables)\nThe “1” in “|trials(1)” says that this is case-level data\n\nIncluding a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the  n = 1 in the model specification (above) for the outcome variable.\n\n\nThe rest of the brms specification is standard for having two categorial explanatory variables (see Ch.5 &gt;&gt; categoricals &gt;&gt; multiple nominal)\n\ni.e. ingredients for Logistic Regression: family = Binomial and binary_outcome|trials(1)\n\nThere’s a normal glm syntax available too where family = Bernouli(logit); see Model Building, brms &gt;&gt; Logistic (haven’t tried to replicate this model with it yet)\n\n\nIf you want to use prior_samples() function to retrieve the prior samples you need a prior statement for each level of treatment. (See My Appendix &gt;&gt; {brms} syntax for details)\nActor Effects\n\nlibrary(tidybayes) \npost &lt;- posterior_samples(b11.4)\npost %&gt;% \n  pivot_longer(contains(\"actor\")) %&gt;%\n  mutate(probability = inv_logit_scaled(value),\n        actor      = factor(str_remove(name, \"b_a_actor\"),\n                              levels = 7:1)) %&gt;% \n\nggplot(aes(x = probability, y = actor)) +\ngeom_vline(xintercept = .5, color = wes_palette(\"Moonrise2\")[1], linetype = 3) +\nstat_pointinterval(.width = .95, size = 1/2,\n                color = wes_palette(\"Moonrise2\")[4]) +\nscale_x_continuous(expression(alpha[actor]), limits = 0:1) +\nylab(NULL) +\ntheme(axis.ticks.y = element_blank())\n\nThe effect is a measure of handedness for each actor.\n\n1, 3, 4, and 5—show a preference for the right lever (right handedness), since the predicted probability measures probability of pulling the left lever\n\nBeing chimp 1 decreases the average probability of choosing the left lever by 38.6% (output of estimate after inputting to inverse logit function)\n\n\ninv_logit_scaled returns the effects on the probability scale (aka absolute effects, see below)\nTreatment Effects\n\n# lever/partner_present\ntx &lt;- c(\"R/N\", \"L/N\", \"R/P\", \"L/P\")\npost %&gt;% \n  select(contains(\"treatment\")) %&gt;% \n  set_names(\"R/N\",\"L/N\",\"R/P\",\"L/P\") %&gt;% \n  pivot_longer(everything()) %&gt;%\n  mutate(probability = inv_logit_scaled(value),\n        treatment  = factor(name, levels = tx)) %&gt;% \n  mutate(treatment = fct_rev(treatment)) %&gt;% \n\n  ggplot(aes(x = value, y = treatment)) +\n  geom_vline(xintercept = 0, color = wes_palette(\"Moonrise2\")[2], linetype = 3) +\n  stat_pointinterval(.width = .95, size = 1/2,\n                    color = wes_palette(\"Moonrise2\")[4]) +\n  labs(x = expression(beta[treatment]),\n      y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\nWe are looking for evidence that the chimpanzees choose the prosocial option more when a partner is present.\n\nThis implies comparing the first row with the third row and the second row with the fourth row.\n\nYou can probably see already that there isn’t much evidence of prosocial intention in these data.\nContrasts\n\npost %&gt;% \n  mutate(db13 = b_b_treatment1 - b_b_treatment3,\n        db24 = b_b_treatment2 - b_b_treatment4) %&gt;% \n  pivot_longer(db13:db24) %&gt;%\n  mutate(diffs = factor(name, levels = c(\"db24\", \"db13\"))) %&gt;% \n\n  ggplot(aes(x = value, y = diffs)) +\n  geom_vline(xintercept = 0, color = wes_palette(\"Moonrise2\")[2], linetype = 3) +\n  stat_pointinterval(.width = .95, size = 1/2,\n                    color = wes_palette(\"Moonrise2\")[4]) +\n  labs(x = \"difference\",\n      y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\ndb13 is the difference between no-partner/partner treatments when the prosocial option was on the right.\n\nSo if there is evidence of more prosocial choice when partner is present, this will show up here as a larger difference, consistent with pulling right more when partner is present.\nWeak evidence that individuals pulled left more when the partner was absent, but the compatibility interval is quite wide.\n\ndb24 is the same difference, but for when the prosocial option was on the left.\n\nNow negative differences would be consistent with more prosocial choice when partner is present.\nClearly that is not the case. If anything, individuals chose prosocial more when partner was absent.\n\nOverall, there isn’t any compelling evidence of prosocial choice in this experiment.\nPosterior Prediction Check (Orange = No Partner Present; Blue = Partner Present)\n\nObserved Proportions is the proportion of left pulls for each chimp for each treatment in the sample data\nPosterior Predictions gets predictions from the model from a dataset with distinct combos of actor, treatment, prosoc_left from the sample data.\nWe don’t want an exact match—that would mean overfitting. But if the posterior predictions generally follow the same pattern as the observed proportions, then it’s a reasonable fit and we didn’t misspecify something.\n\nAlso, we would like to understand how the model sees the data and learn from any anomalies.\n\nInterpretation\n\nThe observed proportions themselves show additional variation—some of the actors possibly respond more to the treatments than others do. (Some line slopes steep)\nThe model (posterior predictions) expects almost no change when adding a partner.  (lines nearly horizontal)\n\nPoint estimates for actors 1 and 7 don’t have the same trends as the observed data\nError bars are wide enough to leave open the possibility of seeing the same trends as in the observed data.\n\n\n\n\n# Observed Proportions\np1 &lt;-\n  d %&gt;%\n  group_by(actor, treatment) %&gt;%\n  summarise(proportion = mean(pulled_left)) %&gt;% \n  left_join(d %&gt;% distinct(actor, treatment, labels, condition, prosoc_left),\n            by = c(\"actor\", \"treatment\")) %&gt;% \n  mutate(condition = factor(condition)) %&gt;% \n  ggplot(aes(x = labels, y = proportion)) +\n  geom_hline(yintercept = .5, color = wes_palette(\"Moonrise2\")[3]) +\n  geom_line(aes(group = prosoc_left),\n            size = 1/4, color = wes_palette(\"Moonrise2\")[4]) +\n  geom_point(aes(color = condition),\n            size = 2.5, show.legend = F) + \n  labs(subtitle = \"observed proportions\")\n\n# Posterior Predictions\nnd &lt;- \n  d %&gt;% \n  distinct(actor, treatment, labels, condition, prosoc_left)\np2 &lt;-\n  fitted(b11.4,\n        newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  mutate(condition = factor(condition)) %&gt;%   \n  ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = .5, color = wes_palette(\"Moonrise2\")[3]) +\n  geom_line(aes(group = prosoc_left),\n            size = 1/4, color = wes_palette(\"Moonrise2\")[4]) +\n  geom_pointrange(aes(color = condition),\n                  fatten = 2.5, show.legend = F) + \n  labs(subtitle = \"posterior predictions\")\n# combine the two ggplots\nlibrary(patchwork)\n(p1 / p2) &\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[c(2:1)]) &\n  scale_y_continuous(\"proportion left lever\", \n                    breaks = c(0, .5, 1), limits = c(0, 1)) &\n  xlab(NULL) &\n  theme(axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill = alpha(\"white\", 1/10), size = 0)) &\n  facet_wrap(~ actor, nrow = 1, labeller = label_both)\n\nConclusion\n\nMost of the variation in predictions comes from the actor intercepts. Handedness seems to be the big story of this experiment. (see Actor Effects)\nWith this much difference between the data and the model, we might consider a model that allows each unique actor to have unique treatment parameters (i.e. multi-level model).\n\nAbsolute Effects: The effect of a (counter-factual) change in predictor value (type of treatment) has on the outcome (probability of an event)\n\nRate that the event happens in the world (i.e. change in outcome event probability)\nBase rate matters\n\nExample (fictional)\n\n1/1000 women develop blood clots (base rate), and an experiment shows that taking a pill raises the risk by 200% (relative risk)!! But that’s actually only 3/1000 now. So the absolute risk was only raised by 0.002.\n\n\n\nRelative Effects: The effect of a (counter-factual) change in predictor value (type of treatment) has on the outcome (odds of an event)\n\nContrast differences on the log-odds scale (raw output) while holding all other predictors constant\nRelative effects can be big while absolute effects are actually small\n\ni.e. unimportant things can seem super important on the relative scale\n\nNeeded to make causal inferences\nRelative measures measures like proportional odds is they ignore what you might think of as the reference or the baseline.\nCan be conditionally very important, when baseline rates of predictor variables change.\n\nExample: Conditional on being in the ocean, sharks are much more dangerous than bees. So in this situation the relative risk is more important than the absolute risk.\n\nThe absolute risk says that bees are more dangerous to humans since the base rate for death by bee is larger because humans spend much more time on land with bees than in the ocean with sharks.\n\n\n\n\n# Difference in treatment 4 and treatment 2\nposterior_samples(b11.4) %&gt;%  # model b11.4 is from the 1st [Example]{.ribbon-highlight}\n  mutate(proportional_odds = exp(b_b_treatment4 - b_b_treatment2)) %&gt;% \n  tidybayes::mean_qi(proportional_odds)\n\n##  proportional_odds    .lower  .upper .width .point .interval\n## 1        0.9315351 0.5183732 1.536444  0.95  mean        qi\n\nOn average, the switch from treatment 2 to treatment 4 multiples the odds of pulling the left lever by 0.93, an 7% reduction in odds.\n\nWhatever the odds of pulling the left lever were before, by switching from treatment 2 to treatment 4 you expect to reduce the odds of the pulling the left lever by 7%\n\nThis is what is meant by proportional odds. The new odds are calculated by taking the old odds and multiplying them by the proportional odds, which is 0.93 in this Example.\nExample: Binomial regression (balanced number of trials)\n\nbalanced - meaning all chimps had 18 trials for each treatment\n\n\n# aggregate counts: number of times each chimp pulled left lever for each treatment\nd_aggregated &lt;-\n  d %&gt;%\n  group_by(treatment, actor, side, cond) %&gt;%\n  summarise(left_pulls = sum(pulled_left)) %&gt;% \n  ungroup()\n# Note outcome parameter value for n\nb11.6 &lt;- \n  brm(data = d_aggregated, \n      family = binomial,\n      bf(left_pulls | trials(18) ~ a + b,\n        a ~ 0 + actor, \n        b ~ 0 + treatment,\n        nl = TRUE),\n      prior = c(prior(normal(0, 1.5), nlpar = a),\n                prior(normal(0, 0.5), nlpar = b)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.06\")\n\nTo fit an aggregated binomial model with brms, we augment the  | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable in the data indexing the number of trials, n. (for further details on this syntax see 0/1 logistic regression Example above)\nComparison between logistitic and binomial model\n\nsame posterior distribution (i.e. same parameter estimates)\nPSIS (and WAIC) scores are very different\n\nWith the aggregated model, there’s no multiplicity term in the binomial likelihood equation. This makes the probabilities larger and therefore the loo::PSIS/WAIC larger (or rethinking::PSIS/WAIC smaller)\n\nHigh Pareto k values are also detected now\n\nCuz reasons… something about more observations being left out of the loo-cv calculation (pg 347)\nWAIC penalty terms also affected\n\n** If you want to use PSIS or WAIC to compare models, use a 0/1 logistic model and NOT an aggregated logistic regression model **\n\nExample: Binomial regression (unbalanced number of trials)\n\nQuestion: What are the average probabilities of admission for females and males, across all departments?\nData\n\ngraduate school applications to 6 different academic departments at UC Berkeley.\nThe admit column indicates the number offered admission and the reject column indicates the opposite decision.\nThe applications column is just the sum of admit and reject.\nEach application has a 0 or 1 outcome for admission, but since these outcomes have been aggregated by department and gender, there are only 12 rows.\n\n\n\ndata(UCBadmit, package = \"rethinking\")\n##    dept applicant.gender admit reject applications\n## 1    A            male  512    313          825\n## 2    A          female    89    19          108\n## 3    B            male  353    207          560\n## 4    B          female    17      8          25\n## 5    C            male  120    205          325\n## 6    C          female  202    391          593\n## 7    D            male  138    279          417\n## 8    D          female  131    244          375\n## 9    E            male    53    138          191\n## 10    E          female    94    299          393\n## 11    F            male    22    351          373\n## 12    F          female    24    317          341\nd &lt;- UCBadmit  %&gt;%\n        mutate(gid  = factor(applicant.gender, levels = c(\"male\", \"female\")),\n              case = factor(1:n()))\nrm(UCBadmit)\n\nSpecification\n\nSame flat prior used in 0/1 logistic regression Example\n\nModel\n\nb11.7 &lt;-\n  brm(data = d, \n      family = binomial,\n      admit | trials(applications) ~ 0 + gid,\n      prior(normal(0, 1.5), class = b),\n      iter = 2000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.07\")\n\nSince this dataset is unbalanced, we use a variable, “applications,” for the number of trials, Ni, for each case.\nResults\n\nposterior_samples(b11.7) %&gt;%\n        # relative effect \n  mutate(diff_a = b_gidmale - b_gidfemale,\n        # absolute effect\n        diff_p = inv_logit_scaled(b_gidmale) - inv_logit_scaled(b_gidfemale)) %&gt;% \n  pivot_longer(contains(\"diff\")) %&gt;% \n  group_by(name) %&gt;%   \n  tidybayes::mean_qi(value, .width = .89)\n\n## # A tibble: 2 x 7\n##  name  value .lower .upper .width .point .interval\n##  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n## 1 diff_a 0.609  0.504  0.715  0.89 mean  qi       \n## 2 diff_p 0.141  0.118  0.165  0.89 mean  qi\n\nThe log-odds difference is certainly positive, corresponding to a higher probability of admission for male applicants.\nAlso see conclusion of next Example\nExample: Binomial regression (unbalanced number of trials)\n\nDescription: Same data as previous Example\nQuestion: “What is the average difference in probability of admission between females and males within departments?”\nSpecification\n\nIt may not answer the whether there’s gender discrimination in admittance (i.e. Is there a direct causal influence of gender on admission)\n\nAcademic ability could influence department and Admittance.\nIf so, then Department is a collider now and conditioning on it would open path GDUA\nData simulation code for this DAG is in R &gt;&gt; Code &gt;&gt; Data Simulation &gt;&gt; sim-pipe-with-confounder-UCB-Admissions.R\n\nAlso see 2022 lecture 10 video for discussion of this model\nAlso see\n\n\n\nModel (non-confounded DAG)\n\n\nb11.8 &lt;-\n  brm(data = d, \n      family = binomial,\n      bf(admit | trials(applications) ~ a + d,\n        a ~ 0 + gid, \n        d ~ 0 + dept,\n        nl = TRUE),\n      prior = c(prior(normal(0, 1.5), nlpar = a),\n                prior(normal(0, 1.5), nlpar = d)),\n      iter = 4000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.08\")\n\nInterpretation\n\n# summary\nprint(b11.8)\n\n## Population-Level Effects: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## a_gidmale      -0.56      0.53    -1.64    0.45 1.00    1014    1353\n## a_gidfemale    -0.46      0.53    -1.55    0.56 1.00    1013    1399\n## d_deptA        1.14      0.53    0.12    2.22 1.00    1020    1390\n## d_deptB        1.10      0.54    0.08    2.19 1.00    1024    1431\n## d_deptC        -0.12      0.53    -1.15    0.97 1.00    1024    1402\n## d_deptD        -0.15      0.53    -1.17    0.93 1.00    1022    1447\n## d_deptE        -0.59      0.54    -1.63    0.50 1.00    1038    1495\n## d_deptF        -2.15      0.54    -3.19    -1.06 1.00    1057    1566\n\nDept: departments with log-odds greater than 0 indicates a department accepts over half of their applicants\nGender: Both genders have log-odds (relative scale) under zero but now males are slightly less likely to be accepted than females (i.e. lower log-odds)\nRelative and Absolute Effects\n\nposterior_samples(b11.8) %&gt;% \n        # relative effect\n  mutate(diff_a = b_a_gidmale - b_a_gidfemale,\n        # absolute effect\n        diff_p = inv_logit_scaled(b_a_gidmale) - inv_logit_scaled(b_a_gidfemale)) %&gt;% \n  pivot_longer(contains(\"diff\")) %&gt;% \n  group_by(name) %&gt;% \n  tidybayes::mean_qi(value, .width = .89)\n\n## # A tibble: 2 x 7\n##  name    value  .lower  .upper .width .point .interval\n##  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n## 1 diff_a -0.0968 -0.227  0.0320    0.89 mean  qi       \n## 2 diff_p -0.0215 -0.0514 0.00693  0.89 mean  qi\n\nConditioning on Department causes the gender effect seen in the previous Example to disappear.\n\nOn the absolute scale, males are now 2% less likely to be admitted than females but CIs do overlap zero.\nConditioning on Department matters because the rates of admission vary a lot across departments. Furthermore, women and men applied to different departments.\n\nPosterior Predictions\n\npredict(b11.8) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(d) %&gt;%   \n  ggplot(aes(x = case, y = admit / applications)) +\n  # model predictions\n      geom_pointrange(aes(y = Estimate / applications,\n                          ymin = Q2.5 / applications ,\n                          ymax = Q97.5 / applications),\n                      color = wes_palette(\"Moonrise2\")[1],\n                      shape = 1, alpha = 1/3) +\n      # observed proportions\n      geom_point(color = wes_palette(\"Moonrise2\")[2]) +\n      geom_line(aes(group = dept),\n                color = wes_palette(\"Moonrise2\")[2]) +\n      geom_text(data = text,\n                aes(y = admit, label = dept),\n                color = wes_palette(\"Moonrise2\")[2],\n                family = \"serif\") +\n      scale_y_continuous(\"Proportion admitted\", limits = 0:1) +\n      labs(title = \"Posterior validation check\",\n          subtitle = \"Though imperfect, this model is a big improvement\") +\n      theme(axis.ticks.x = element_blank())\n\nA-F are the departments. The left dot are males and the right dot are females. Observed are filled in dots. Model predictions are open dots with bars\nThe model fits the data pretty well. Error bar doesn’t cover the observed proportion for dept A-Female though.\nSome variation in admission probability between groups of depts (e.g {A,B} and {C,D,E})\nFemales in these data tended not to apply to departments like A and B, which had high overall admission rates. Instead they applied in large numbers to departments like F, which admitted less than 10% of applicants.\nExample: UCB Admissions Binomial Regression (lecture 9 video)\n\nA: Accepted into Grad School\nG: Gender, D: Department\nTotal Effect\n\nA ~ G\n\n\n\npacman::p_load(ggplot2, dplyr, rstanarm, marginaleffects, ggdist)\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;% \n  mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\ntot_mod &lt;- stan_glm(\n  cbind(admit, reject) ~ 0 + applicant.gender,\n  prior = normal(0,1),\n  data = ucb, \n  family = binomial \n)\nsummary(tot_mod)\n                        mean  sd  10%  50%  90%\napplicant.gendermale  -0.2    0.0 -0.3  -0.2  -0.2 \napplicant.genderfemale -0.8    0.1 -0.9  -0.8  -0.8\n\nFYI: You get these same estimates whether you include an intercept or not. This way saves you the calculations to get the estimates for both categories.\nThese values are NOT interpretable except in saying that being female decreases the probability of being accepted more than being male.\nMarginal Effects\n\npreds &lt;- predictions(tot_mod) %&gt;% posteriordraws()\n\nggplot(preds, aes(x = draw, fill = applicant.gender)) +\n  ggdist::stat_halfeye() + \n  scale_fill_manual(values=c(\"#195198\", \"#BD9865\")) +\n  labs(x = \"Predicted Probability of Admittance\", y = \"\", fill = \"Gender\") + \n  theme_minimal()\n\nplot_cap(tot_mod, condition = \"applicant.gender\") +\n    labs(x = \"Gender\", y = \"Admittance Probability\")\n\nSince we’re dealing with categorical predictors, these “marginal effects” are actually predicted means. (see Regression, Interactions  &gt;&gt; OLS &gt;&gt; categorical:categorical for details)\nInterpretations:\n\nFrom the density plot, we see there is no overlap in densities, so there is very likely a credible difference between female and male probabilities of acceptance, but we must examine the contrast to be sure.\nError bar plot has the same interpretation\n\nContrast: Female - Male\n\nmarginaleffects(tot_mod, variables = \"applicant.gender\") %&gt;% tidy()\n      type            term      contrast  estimate  conf.low  conf.high\n1 response applicant.gender female - male -0.1413067 -0.1704179 -0.1124712\n\nmfx_tot &lt;- marginaleffects(tot_mod, variables = \"applicant.gender\") %&gt;% \n  posteriordraws()\nggplot(mfx_tot, aes(x = draw)) +\n  ggdist::stat_halfeye(fill = \"red\") + \n  labs(x = \"female-male\", y = \"Admittance Probability\") +\n  theme_minimal()\n\nThe average probability contrast is about -0.14\nThe point on the bar at the bottom of the density is the median with bars for 1sd (66%) and 2sd (95%)\nInterpretation\n\nAs expected, none of the contrast distribution includes zero, so there is a credible difference in acceptance probability between makes and females\nFemales are around 14% less likely to be accepted into a Berkeley graduate school on average.\n\nDirect effect\n\nA ~ G + D\n{rstanarm} using a binomial model\n\n\ndir_mod &lt;- stan_glm(\n  cbind(admit, reject) ~ 0 + applicant.gender:dept,\n  prior = normal(0,1),\n  data = UCBadmit, \n  family = binomial \n)\nsummary(dir_mod)\n                              mean  sd  10%  50%  90%\napplicant.genderfemale:deptA  1.5    0.2  1.2  1.5  1.8 \napplicant.gendermale:deptA    0.5    0.1  0.4  0.5  0.6 \napplicant.genderfemale:deptB  0.7    0.4  0.2  0.7  1.2 \napplicant.gendermale:deptB    0.5    0.1  0.4  0.5  0.6 \napplicant.genderfemale:deptC -0.7    0.1 -0.8  -0.7  -0.5 \napplicant.gendermale:deptC  -0.5    0.1 -0.7  -0.5  -0.4 \napplicant.genderfemale:deptD -0.6    0.1 -0.8  -0.6  -0.5 \napplicant.gendermale:deptD  -0.7    0.1 -0.8  -0.7  -0.6 \napplicant.genderfemale:deptE -1.1    0.1 -1.3  -1.1  -1.0 \napplicant.gendermale:deptE  -0.9    0.2 -1.1  -0.9  -0.7 \napplicant.genderfemale:deptF -2.5    0.2 -2.7  -2.5  -2.2 \napplicant.gendermale:deptF  -2.7    0.2 -2.9  -2.7  -2.4\n\n{brms} using a logistic model\n\nbrm_mod &lt;- brm(data = ucb_01,\n              family = bernoulli,\n              bf(admitted ~ 0 + gd,\n                  # this is the interaction \n                  gd ~ (0 + gender) : (0 + dept), \n                  nl = TRUE), \n              prior = prior(normal(0,1), nlpar = gd),\n              iter = 2000, warmup = 1000, cores = 3, chains = 3,\n              seed = 10, backend = \"cmdstanr\")\nbrm_mod\n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngd_genderfemale:deptA    1.47      0.23    1.04    1.95 1.00    6267    2276\ngd_gendermale:deptA      0.49      0.07    0.35    0.64 1.00    5800    1898\ngd_genderfemale:deptB    0.66      0.39    -0.09    1.43 1.00    5720    2340\ngd_gendermale:deptB      0.53      0.09    0.36    0.70 1.00    6734    2394\ngd_genderfemale:deptC    -0.66      0.09    -0.83    -0.49 1.00    6449    2245\ngd_gendermale:deptC      -0.53      0.12    -0.76    -0.29 1.00    5986    2060\ngd_genderfemale:deptD    -0.62      0.11    -0.83    -0.41 1.00    6639    2076\ngd_gendermale:deptD      -0.70      0.10    -0.90    -0.50 1.00    6293    2152\ngd_genderfemale:deptE    -1.14      0.12    -1.39    -0.91 1.00    6204    2150\ngd_gendermale:deptE      -0.94      0.16    -1.26    -0.63 1.00    6170    2344\ngd_genderfemale:deptF    -2.49      0.19    -2.88    -2.12 1.00    6092    2470\ngd_gendermale:deptF      -2.66      0.20    -3.06    -2.29 1.00    6773    2114\n\nContrasts:  Female - Male by Department\n\ncomparisons(dir_mod, \n            variables = \"applicant.gender\", \n            newdata = datagrid(dept = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")))\n  rowid    type              group        term contrast_applicant.gender  comparison    conf.low  conf.high applicant.gender dept\n1    1 response main_marginaleffect interaction            female - male  0.19139730  0.11079248 0.26401039            male    A\n2    2 response main_marginaleffect interaction            female - male  0.02630166 -0.15667488 0.18740243            male    B\n3    3 response main_marginaleffect interaction            female - male -0.02932193 -0.09613419 0.03571516            male    C\n4    4 response main_marginaleffect interaction            female - male  0.01633091 -0.04930494 0.08555060            male    D\n5    5 response main_marginaleffect interaction            female - male -0.03958714 -0.11517937 0.03433742            male    E\n6    6 response main_marginaleffect interaction            female - male  0.01116004 -0.02477051 0.04855381            male    F\n\nInterpretations:\n\nDepartment A shows a credible advantage towards females while the other department distributions hover around 0.\n\nVisualize the contrast distributions\n\ncomp_dir &lt;- comparisons(dir_mod, \n                        variables = \"applicant.gender\", \n                        newdata = datagrid(dept = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"))) %&gt;% \n  posteriordraws()\nggplot(comp_dir, aes(x = draw, y = dept)) +\n  ggdist::stat_halfeye(aes(\n    fill_ramp = stat(cut_cdf_qi(\n      cdf, \n      .width = c(.5, .8, .95),\n      labels = scales::percent_format()\n    ))\n  ), fill = \"#7c1998\") + \n  scale_fill_ramp_discrete(range = c(1, 0.2), na.translate = FALSE) +\n  geom_vline(xintercept = 0, color = \"gray85\", linetype = \"dashed\") +\n  labs(x = \"Probability of Acceptance\", fill_ramp = \"Interval\") +\n  theme_minimal()\n\nMost of the department distributions strattle zero except for department A.\n\nThe meaty part (i.e. 50%) of Depts F, D, and B distributions contain 0\n\nThe fact that there are distributions mostly on either sided of 0 is due the heterogeneity of the data (i.e. varying acceptance and application rates of departments)\nDept B is pretty flat which could mean fewer applicants\nCounterfactual direct effect of changing gender (intervention)\n\nfemale_counter &lt;- predictions(dir_mod, newdata = datagrid(applicant.gender = \"female\", \n                                                          grid.type = \"counterfactual\")) %&gt;% \n  posteriordraws()\nmale_counter &lt;- predictions(dir_mod, newdata = datagrid(applicant.gender = \"male\", \n                                                          grid.type = \"counterfactual\")) %&gt;% \n  posteriordraws()\ncounter_dat &lt;- tibble(female_acceptance = female_counter$draw,\n                      male_acceptance = male_counter$draw) %&gt;% \n  mutate(fm_contrast = female_acceptance - male_acceptance)\n\nggplot(counter_dat, aes(x = fm_contrast)) +\n  ggdist::stat_halfeye(fill = \"#982a19\") + \n  geom_vline(xintercept = 0, color = \"gray85\", linetype = \"dashed\") +\n  geom_text(data = data.frame(x = c(-0.2, 0.2),\n                              y = c(0.85, 0.85),\n                              label = c(\"Male Adv\", \"Female Adv\")),\n            mapping = aes(x = x, y = y,\n                          label = label),\n            size = 4.8, angle = 0L,\n            lineheight = 1L, hjust = 0.25,\n            vjust = 0.5, colour = \"black\", fontface = \"bold\",\n            inherit.aes = FALSE, show.legend = FALSE) +\n  labs(x = \"Female-Male\", y = \"\") + \n  theme_minimal()\n\nThis is the expected posterior distribution of changing the (perceived) gender from female to male\n\nThe left figure, is his distribution from the lecture. He used his simulated model with only 2 departments and then used counterfactual data of all 6 depts to get predictions and contrasts. In terms of answering the question, I don’t think his model applies since it’s not representative of the data.\n\nInterpretation\n\nThe median of the bi-modal distribution is close to 0, so there isn’t a credible average direct effect detected. Yet, visually it seems like the female advantaged side does contain a majority of the mass.\n\nThe average direct effect is calculated by marginalizing across departments\n\nThe fact that it’s bimodal is due to the heterogeneity of the data. (i.e. the different acceptance and application rates of each department)\n\nUCB Admissions Conclusion\n\n** See\n\nCenter for health equity for descriptions of direct and indirect discrimination\n2022 Lecture 9 video\n\nHe also describes how a proper analysis of this phenomenon would be conducted which includes sampling the applicant pool and ranking candidates to collect data in order to deal potential confounders to admission and department (e.g. latent variables like talent and ability)\n\nThis is also mentioned in the Specification section of this Example\n\n\n\nThere is no direct discrimation detected (effect with conditioning on department, A ~ G + D).\n\nOn average, there is no discrimination detected by application reviewers within departments.\nIf this effect was detected, it would be the worst one if you’re the college. It would indicate people in the college are actively discriminating during the application process.\nIn the lecture video when generating fake data, he shows that different departmental acceptance rates for each gender can result in this type of discrimination effect\n\nThere might be indirect discrimination (aka structural discrimination) \n\nIf this effect is detected, it might be the worst one if you’re a woman since the cause is likely something that can’t be easily fixed (e.g. by firing someone).\nTo figure this out you have to calculate the Average Causal Mediation Effect (ACME)\n\nSee Causal Inference &gt;&gt; Mediation Analysis for Examples and formulas.\nRemember that this is NOT Admissions ~ Gender. This would the total effect which is both paths.\nIn Lecture 9 video, he uses an interaction. So if you model it that way, it complicates the ACME formula some.\n\nSince we measured a significant total effect (A ~ G) but no direct effect (A ~ G + D), it seems reasonable that there could be indirect discrimination.\n\ni.e. discrimination that leads 1 gender category or another not to apply to a particular department\n\nIf there is structural discrimination, the reasons could be: \n\nFemales and males may apply disproportionately to different departments\n\nFrom the data, we see that females apply to particular departments much more than others and more to departments with higher base rejection rates. Therefore, they got turned down more.\n\nIf females applied more evenly across departments, the discrimination effect should disappear.\n\nMaybe something societal that makes females less comfortable in certain subjects. For Example, girls being told that girls aren’t as good at math as boys so they end up applying to less technical departments more.\n\nVariation among departmental base rates of acceptance\n\nSome departments may receive more funding which leads to accepting more students and a higher acceptance rate.\n\nHe shows in Lecture 9 2022 video how variation in both of these aspects (at the same time) result in this type of discrimination effect.\n\n\nOver-parameterization\n\nThis is discovered either by a correlation pairs plot check of the parameters’ posteriors or if there’s a non-identifiability error during the fit. (Also see Ch 6 &gt;&gt; Multicollinearity)\n\n\n\nlibrary(GGally)\nposterior_samples(b11.8) %&gt;% \n  select(-lp__) %&gt;% \n  set_names(c(\"alpha[male]\", \"alpha[female]\", str_c(\"delta[\", LETTERS[1:6], \"]\"))) %&gt;%\n  ggpairs(upper = list(continuous = my_upper),\n          diag  = list(continuous = my_diag),\n          lower = list(continuous = my_lower),\n          labeller = \"label_parsed\") +\n  labs(title = \"Model: b11.8\",\n      subtitle = \"The parameters are strongly correlated.\") +\n  theme(strip.text = element_text(size = 11))\n\nThen, we know that the parameter standard errors are inflated, because due to the correlation, there are many combinations of the gender and department parameters that can match the data. Prediction aren’t biased though. (see Posterior Predictions plot)\nIt it isn’t a violation of any statistical principle. The only limitation is whether the algorithm we use to approximate the posterior can handle the high correlations (i.e. no non-identifiability error) which in this case it can.\nPoisson\n\nPoisson is a special case of the Binomial Distribution\n\nThe expected value of a binomial distribution is just Np, and its variance is Np(1 − p).\nBut when N is very large and p is very small, then expected value and variance are approximately the same. The shape of this distribution is the Poisson\n\nExample\n\nDescription\n\nDataset on primitive civilizations in Oceania.\n\n“Tools” are counts of unique tools that get developed in the civilization.\n“Contact” is how much contact those civilizations had with the outside world.\n\n\n\n\n\ndata(Kline, package = \"rethinking\")\nd &lt;- Kline %&gt;%\n    mutate(log_pop_std = (log(population) - mean(log(population))) / sd(log(population)),\n          cid = contact)\nrm(Kline)\n\nVariables\n\ntotal_tools (outcome): discrete\nlog_pop_std (predictor): numeric; standardized log population\ncontact rate (predictor): categorical\n\nHypotheses\n\nThe number of tools increases with the log population size\n\nlogging a predictor says that the order of magnitude of the population is what matters, not the absolute size of it.\n\nThe number of tools increases with the contact rate among islands.\nThe impact of population on tool counts is moderated by high contact\n\nSpecification (count ~ continuous * categorical)\nModel\n\n# cat * cont interaction model\nb11.10 &lt;-\n  brm(data = d, \n      family = poisson,\n      bf(total_tools ~ a + b * log_pop_std,\n        a + b ~ 0 + cid,\n        nl = TRUE),\n      prior = c(prior(normal(3, 0.5), nlpar = a),\n                prior(normal(0, 0.2), nlpar = b)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.10\")\nb11.10\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cidhigh    3.61      0.07    3.46    3.75 1.00    3925    2583\na_cidlow      3.32      0.09    3.15    3.48 1.00    3324    3084\nb_cidhigh    0.20      0.16    -0.13    0.51 1.00    3638    2865\nb_cidlow      0.38      0.05    0.28    0.48 1.00    3700    2878\n\nformula: a + b ~ 0 + cid is a shortcut that can be used instead of separate a and b statements (see Ch. 8 cont * cat interaction Example)\npriors: no separate prior specifications for each level\n\nSeparate specifications only required if we want to use prior_samples (see My Appendix &gt;&gt; {brms} syntax for more details)\n\nFitted vs Predictor\n\ncultures &lt;- c(\"Hawaii\", \"Tonga\", \"Trobriand\", \"Yap\")\nlibrary(ggrepel)\nnd &lt;-\n  distinct(d, cid) %&gt;% \n  expand(cid, \n        log_pop_std = seq(from = -4.5, to = 2.5, length.out = 100))\nf &lt;- \n  fitted(b11.10,\n        newdata = nd,\n        probs = c(.055, .945)) %&gt;%\n  data.frame() %&gt;%\n  bind_cols(nd)\np1 &lt;-\n  f %&gt;%\n  ggplot(aes(x = log_pop_std, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, size = 1/2) +\n  geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics),\n            aes(y = total_tools, size = pareto_k),\n            alpha = 4/5) +\n  geom_text_repel(data = \n                    bind_cols(d, b11.10$criteria$loo$diagnostics) %&gt;% \n                    filter(culture %in% cultures) %&gt;% \n                    mutate(label = str_c(culture, \" (\", round(pareto_k, digits = 2), \")\")),\n                  aes(y = total_tools, label = label), \n                  size = 3, seed = 11, color = \"black\", family = \"Times\") +\n  labs(x = \"log population (std)\",\n      y = \"total tools\") +\n  coord_cartesian(xlim = range(b11.10$data$log_pop_std),\n                  ylim = c(0, 80))\n\np2 &lt;-\n  f %&gt;%\n  mutate(population = exp((log_pop_std * sd(log(d$population))) + mean(log(d$population)))) %&gt;% \n  ggplot(aes(x = population, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, size = 1/2) +\n  geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics),\n            aes(y = total_tools, size = pareto_k),\n            alpha = 4/5) +\n  scale_x_continuous(\"population\", breaks = c(0, 50000, 150000, 250000)) +\n  ylab(\"total tools\") +\n  coord_cartesian(xlim = range(d$population),\n                  ylim = c(0, 80))\n\n(p1 | p2) &\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[1:2]) &\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[1:2]) &\n  scale_size(range = c(2, 5)) &\n  theme(legend.position = \"none\")\n\nOrange low contact and blue is high contact; Pareto K-values shown in parentheses\nHawaii is highly influential according to it’s k-value (left graph), and it can also be seen when population has been back-transformed (right graph)\n\nReason being that it has extreme population size and the most tools\n\nInterpretation\n\nThe trend for societies with high contact (solid) is higher than the trend for societies with low contact (dashed) when population size is low\nThe means cross one another at high population sizes.\n\nmodel is actually saying it has no idea where the trend for high contact societies goes at high population sizes, because there are no high population size societies with high contact. There is only low-contact Hawaii. But it is still a silly pattern that we know shouldn’t happen. A counter-factual Hawaii with the same population size but high contact should theoretically have at least as many tools as the real Hawaii. It shouldn’t have fewer\n\nLow contact fitted values have a y-intercept at around 20 which is impossible since you can’t have 0 population (x-axis) and have tools\n\nThe fitted line should go through (0,0)\n\n\nImprovements\n\nUse a negative binomial model (aka Gamma-Poisson)\nUse a more principled scientific model (see 2022 Lecture video for details, it’s just a scientific nonlinear model though)\n\nExample: Counts with varying rates\n\nWhere tau is the offset or exposure (time, area, etc).\nDescription: Two monasteries produce manuscripts but record the number of manuscripts they complete on different time scales. One monastery records daily manuscripts and the other monastery records on a weekly basis.\nVariables:\n\ny (outcome): discrete; manuscript count\nlog_days (offset): numeric\nmonastery (predictor): binary\n\nSo log_days will have a different value (i.e. different rate) depending on the value of the monastery indicator.\n\n\n\n\nset.seed(11)\n# old monastery: 1.5 manuscript/day\nnum_days &lt;- 30\ny &lt;- rpois(num_days, lambda = 1.5)\n\n# new monastary: 0.5 manuscripts/day converted to weekly by multiplying by 7\nnum_weeks &lt;- 4\ny_new &lt;- rpois(num_weeks, lambda = 0.5 * 7)\n\nd &lt;- \n  tibble(y        = c(y, y_new), \n        days      = rep(c(1, 7), times = c(num_days, num_weeks)),  # this is the exposure\n        monastery = rep(0:1, times = c(num_days, num_weeks))) %&gt;%\n  mutate(log_days = log(days))\n\nSpecification\nModel\n\nb11.12 &lt;-\n  brm(data = d, \n      family = poisson,\n      y ~ 1 + offset(log_days) + monastery,\n      prior = c(prior(normal(0, 1), class = Intercept),\n                prior(normal(0, 1), class = b)),\n      iter = 2000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.12\")\n\nprint(b11.12)\n## Population-Level Effects: \n##          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.01      0.18    -0.38    0.33 1.00    2350    1804\n## monastery    -0.89      0.33    -1.56    -0.26 1.00    2443    2306\n\nbrm fixes the value for the offset parameter therefore there is no estimate for it.\nInterpretation \n\nposterior_samples(b11.12) %&gt;%\n  mutate(lambda_old = exp(b_Intercept),\n        lambda_new = exp(b_Intercept + b_monastery)) %&gt;%\n  pivot_longer(contains(\"lambda\")) %&gt;% \n  mutate(name = factor(name, levels = c(\"lambda_old\", \"lambda_new\"))) %&gt;%\n  group_by(name) %&gt;%\n  tidybayes::mean_hdi(value, .width = .89) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n##  name      value .lower .upper .width .point .interval\n##  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n## 1 lambda_old  1      0.69  1.25  0.89 mean  hdi     \n## 2 lambda_new  0.42  0.23  0.6    0.89 mean  hdi\n\nPosterior mean manuscript output estimate for each monastery\nOof. Not a terribly good model. Simulated data was generated using a λ = 1.5 for the old monastery and λ = 0.5 for the new monastery. So the true value is outside the 89% CIs for the old monastery but a not-so-bad of an estimate for the new monastery.\n\nIt’s only a month’s worth of data. Probably shouldn’t be too hard on the model.\n\nAnyways, we estimate that the new monastary outputs less than half the daily output of the old monastery. (Note: both estimates on the daily scale)\nMultinomial and categorical models\n\naka Categorical Rregression or Maximum Entropy Classifier\nThe likelihood is tricky because as the event types multiply, so too, do your modeling choices\n\nexplicit apppoach: The first is based directly on the multinomial likelihood and uses a generalization of the logit link\ntransforms the multinomial likelihood into a series of Poisson likelihoods\n\nUses a multinomial logit (aka softmax) link\n\nTakes a vector of scores, one for each of K event types, and computes the probability of a particular type of event K (also see results section)\n\nScores, si, are the parameters that are estimated by the multinomial model\nK is the number of levels of the categorical outcome variable\n\n\nin a multinomial (or categorical) GLM, you need K − 1 linear models for K types of events. One of the outcome values is chosen as a “pivot” (i.e. reference category) and the others are modeled relative to it.\nIn each the K − 1 linear models, you can use any predictors and parameters you like—they don’t have to be the same, and there are often good reasons for them to be different\n1.11 There are two basic cases:\n(1) Predictors matched to outcomes\n\n-   predictors have different values for different values the outcome, and\n-   useful when each type of event has its own quantitative traits, and you want to estimate the association between those traits and the probability each type of event appears in the data\n\n\nPredictors matched to observations\n\n\nparameters are distinct for each value of the outcome\nuseful when you are interested instead in features of some entity that produces each event, whatever type it turns out to be\n\n\n1.12 Example: Multinomial Logistic Regression\n(1) Predictors matched to outcomes\n\nDescription\n\nModeling the choice of career for a number of young adults.\nData is at the case level like a logistic regression\ncareer (outcome): 1, 2, or 3 which represent 3 different career categories\n\nHow the model is fit (makes a little more since after looking at how the data is simulated)\n\n3 different careers, each with its own income trait (i.e. career_income).\nThese traits are used to assign a score to each type of event.\nThen when the model is fit to the data, one of these scores is held constant, and the other two scores are estimated, using the known income traits.\n\n\ncareer_income (predictor): 1, 2, or 5, income “traits”\n\n? One of the relevant predictor variables is expected income. estimate the impact of the income trait on the probability a career. a different income value multiplies the parameter in each linear model\n\n\nSimulate Data\n\n\n\n# simulate career choices among 500 individuals\n\nn      &lt;- 500          # number of individuals\nincome &lt;- c(1, 2, 5)    # expected income of each career\nscore  &lt;- 0.5 * income  # scores for each career, based on income, [0.5 1.0 2.5]\n# Convert scores to probabilities\np &lt;- rethinking::softmax(score[1], score[2], score[3]) # 0.09962365 0.16425163 0.73612472\n\n# Simulate career choice\n# outcome career holds event type values, not counts\ncareer &lt;- rep(NA, n)  # empty vector of choices for each individual \nset.seed(34302)\n# sample chosen career for each individual\nfor(i in 1:n) career[i] &lt;- sample(1:3, size = 1, prob = p)\n\n# assign income according to career, n = 500\nd &lt;-\n  tibble(career = career) %&gt;% \n  mutate(career_income = ifelse(career == 3, 5, career))\nhead(d)\n  career career_income\n  &lt;int&gt;        &lt;dbl&gt;\n1      3            5\n2      3            5\n3      3            5\n4      3            5\n5      3            5\n\nThe probabilities, p, is the outcome that we’re modeling.\nModel (intercept-only)\n\n# Outcome categorical variable has k = 3 levels. We fit k-1 models. Hence the 2 intercept priors\n# intercept model \nget_prior(data = d, \n          # refcat sets the reference category to the 3rd level\n          family = categorical(link = logit, refcat = 3), \n          # just an intercept model\n          career ~ 1) \n##                prior    class coef group resp dpar nlpar bound  source \n##                (flat) Intercept                                  default \n##  student_t(3, 3, 2.5) Intercept                  mu1            default \n##  student_t(3, 3, 2.5) Intercept                  mu2            default\n\nb11.13io &lt;- \n  brm(data = d,       \n      # refcat sets the reference category to the 3rd level\n      family = categorical(link = logit, refcat = 3), \n      career ~ 1, \n      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), \n                prior(normal(0, 1), class = Intercept, dpar = mu2)), \n      iter = 2000, warmup = 1000, cores = 4, chains = 4, \n      seed = 11, backend - \"cmdstanr\",\n      file = \"fits/b11.13io\")\n\nb11.13io\n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## mu1_Intercept    -2.01      0.15    -2.30    -1.73 1.00    3324    2631\n## mu2_Intercept    -1.53      0.12    -1.77    -1.29 1.00    2993    2768\n\n** For alternative brms syntaxes in fitting this model see Model Building, brms\nas of brms 2.12.0, “specifying global priors for regression coefficients in categorical models is deprecated.” Meaning — if we want to use the same prior for both, we need to use the dpar argument for each\n\nThe name of the distributional parameters (dpar) in multinomial models are “mu”\n\nThe reference level, refcat ,  was set to the 3rd level so that level is set to 0 and the scores for the levels 1 and 2 are shown\n\nDefault reference level is level 1.\n\nThese are the score values that we’d get if we centered all the scores by level 3’s score value in the data simulation (i.e. x - 2.5)\n\nNothing to infer from these. We want the probabilities which are in the next section.\n\nResults\n\nfitted(b11.13io)[1, , ] %&gt;% \n  round(digits = 2) %&gt;% \n  t()\n##          Estimate Est.Error Q2.5 Q97.5\n## P(Y = 1)    0.10      0.01 0.08  0.13\n## P(Y = 2)    0.16      0.02 0.13  0.19\n## P(Y = 3)    0.74      0.02 0.70  0.77\n\n# manually using the softmax function\nposterior_samples(b11.13io) %&gt;% \n  # The reference level (level 3) score is zero since all the scores have been centered at its value.\n  mutate(b_mu3_Intercept = 0) %&gt;% \n  mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),\n        p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),\n        p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) %&gt;% \n  pivot_longer(p1:p3) %&gt;% \n  group_by(name) %&gt;% \n  tidybayes::mean_qi(value) %&gt;% \n  mutate_if(is.double, round, digits = 2)\n##  name  value .lower .upper .width .point .interval\n##  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n## 1 p1    0.1    0.08  0.13  0.95 mean  qi       \n## 2 p2    0.16  0.13  0.19  0.95 mean  qi       \n## 3 p3    0.74  0.7    0.77  0.95 mean  qi\n\nMean probabilities for each category level of the outcome variable\nAverage probability for a young adult to choose career == 1 is 10%\nModel (with predictor)\n\nb11.13d &lt;-\n  brm(data = d, \n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n        nlf(mu1 ~ a1 + b * 1),\n        nlf(mu2 ~ a2 + b * 2),\n        a1 + a2 + b ~ 1),\n      prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n                prior(normal(0, 1), class = b, nlpar = a2),\n                prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)),\n      iter = 2000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11, backend = \"cmdstanr\",\n      control = list(adapt_delta = .99),\n      file = \"fits/b11.13d\")\n\nfixef(b11.13d) %&gt;% round(digits = 2)\n##              Estimate Est.Error  Q2.5 Q97.5\n## a1_Intercept    -2.14      0.19 -2.53 -1.80\n## a2_Intercept    -1.79      0.25 -2.39 -1.40\n## b_Intercept      0.13      0.11  0.00  0.42\n\nKurz hardcoded the “career_income” variable values into the “nlf” formulas where b*1 is essentially β*income_career = 1 for career 1 (i.e mu1) and b*2 is β*income_career = 2 for career 2 (i.e. mu2).\n“lb” arg in the prior functions sets the lower bound of the parameter to 0\nHe has a1 and a2 classes as b instead of Intercept for some reason\n\nMy Appendix\n\nThoughts about the components in Bayes theorem (from Chapter 4)\n\nThe likelihood, prior, and posterior densities are probability densities each with an area = 1. Looking at the marble tables it looks like the individual posterior probabilities sum to 1. So, the sum (we’re talking densities so probably this “sum” = integration actually) of all the products of the multiiplication of the prior and likelihood densities must not have an area = 1. Therefore, the denominator (i.e. sum of products) then standardizes these products so the posterior density does have an area of 1.\nWould this make the posterior a joint density of parameter combinations (aka conjectures)?  It does (pg 96)\n\nGetting a handle on characteristics of masking and multicollinearity\n\nThe Examples seem quite similar because of the correlation structures between the variables. This is just me brainstorming. tldr in Take away section\nMasking is in Ch. 5 and Multicollinearity in Ch. 6\nMasking Example\n\ncorrelations\n\npred_var1 and pred_var1 are (strongly) positively correlated\npred_var1 is (mildly) negatively correlated with the outcome\npred_var2 is (mildly) positively correlated with the outcome\n\nmultivariate vs bivariate effects\n\nthe magnitude of each predictor’s effect is increased\nsd increased slightly but the sd/effect ratio decreases since the effect increased\n\n\nMulticollinearity Example\n\ncorrelations\n\nsame but predictors are (strongly) negatively correlated with each other and (strongly) correlated with the outcome\n\nmultivariate vs bivariate effects\n\nmagnitude is decreased \nsd increases\n\n\nInitial thoughts\n\nmasking:\n\nmultivariate uncovers effect;\n\nmulticollinearity:\n\nmultivariate covers effect.\npredictions are fine\n\nbut PIs inflated?\n\n\nstrength of correlation of predictors with outcome was mild then strong\nstrength of correlation of predictors with each other was strong in both\n\nDifferences between multicollinearity and masking in Examples\n\nset-up\n\nsign of correlation between predictors\n\nmulticollinearity: negative\nmasking: positive\n\nstrength of correlation of predictors with outcome\n\nmulticollinearity: strong\nmasking: mild\n\n\nresults\n\ndirection of the change in effect size\n\nmulticollinearity: decrease\nmasking: increase\n\nsd\n\nmulticollinearity: greatly increased\nmasking: slightly increased\n\n\n\nissues\n\nmulticollinearity:\n\nstandard errors are large which affects inference\neffect shrinkage could cause sign flips\nPIs inflated?\n\nmasking:\n\nMaybe this becomes a source of confusion if doing robustness checks?\n\n\n“solution”\n\nmulticollinearity:\n\ndomain knowledge — is there a 3rd variable that influences both of the collinear variables\n\nnot supposed to automatically drop a variable\nregression with outcome and this third variable and drop the mediators\n\n\nmasking:\n\nBoth variables need to be included in the regression\n\n\nTake away\n\nInference\n\nmasking\n\nunless there’s some sort of variable reduction necessary and there’s a masking relationship with the variable of interest and another variable and the “other” one gets removed, I don’t think this is an issue\nMaybe this becomes a source of confusion if doing robustness checks\n\nmulticollinearity\n\ninference impossible\n\nstd devs will blow-up\npoint estimate signs may flip as compared to a bivariate regression\n\n\n\nPrediction\n\nmasking\n\nShouldn’t matter with model selection. If a group of masked variables is important to prediction, they should all get selected.\n\nMaybe possible some combination of masked variables and other variables has an opposite effect and one of the masked variables gets dropped. But then the masked group wasn’t terribly important to begin with.  Could explain variations in Variable Importance with different models. Prediction is all and not variable importance though so still not a big issue.\n\n\nmulticollinearity\n\nPIs may be inflated\n\n\nOther\n\nmasking: I think it’s just a phenomenon that’s rarely an issue\nmulticollinearity: important in both but will be evident in inference; not so much in prediction\n\n\nQuestions\n\nif the sign of the collinearity were positive in the multicollinearity Example, would the effects increase?\nAs the strength of the collinearity increases, does sd gradually increase or is there some kind of threshold? (r &gt; .90)\nIs there a sort of linear path from masking to full multicollinearity?\n\nwould need to figure out if correlation signs matter, pred vs pred and pred vs outcome\ncombinations of strong and week corr by one and both preds\ngradation from weak to strong\nproject with animation?\n\n\n\nThe Haunted DAG\n\nsection 6.3.2\nMost texts use only 3 variables when introducing Simpson’s Paradox but McElreath uses 4 here\n\nSimpson’s Paradox is a statistical phenomenon where adding conditioning variables changes the effect and interpretation of the variable of interest. The variables that are causing the change are adding backdoor paths. So, the scientific question and its DAG determine causality. An Example of why you just can’t keep adding control variables.\n\nVariables\n\nC is the outcome variable, Child’s education level\nG is grandparent’s education level\nP is parent’s education level\nU is the neighborhood quality with values: good or bad\n\n\nInitial thoughts on the dag\n\nPUC: a fork, so it’s path is open unless U is conditioned upon, U is the confounder\nGPC: a pipe, conditioning on P would block the path from G through P to C and leave only G to C. P is the confounder here.\nGPUC: Adjusting for P closes P to C; adjusting for U opens P to C\n\nP is a collider\n\n\nSim data (think magnitudes)\n\nG to C is given effect = 0\nG to P is given effect = 1\nP to C is given effect = 1\nU to C and P is given effect = 2\n\nHis first regression C ~ a + G + P\n\nG estimate ~ (-1) and P estimate ~ 2\n\n\n\n      mean  sd  5.5% 94.5%\na    -0.12 0.10 -0.28  0.04\nb_PC  1.79 0.04  1.72  1.86\nb_GC -0.84 0.11 -1.01 -0.67\nsigma 1.41 0.07  1.30  1.52\n\nThe reason is unobserved effect of U on P and C creates illusion that G’s influence on C is negative (when it’s actually 0).\nFig  explains how P induces the negative effect\n\nfilled dots are parent with education levels within the 45 and 65 percentile. The rest of the info is annotated in the plot\n2 parents with average education but one having a G with below 0 education and the other having above 0 education will necessarily live in different neighborhoods\n\nif you split the plot into G’s below 0 and above 0, then for the most part, blue filled dots are on the left and black filled dots on the right.\n\nRegressing a line through this subpopulation of parents gives the negative estimated effect between G and C.\ni.e. we held parents education at it’s average (standardized var) and the relationship between G and C is negative (i.e. 1st regression). By coloring this subpopulation by U, we see WHY it’s negative and the effect of U on P by the distinct groups of parents into different neighborhoods.\n\nHis second regression C ~ a + G + P + U\n\nall the effects match the sim data\n\n\n      mean  sd  5.5% 94.5%\na    -0.12 0.07 -0.24 -0.01\nb_PC  1.01 0.07  0.91  1.12\nb_GC -0.04 0.10 -0.20  0.11\nb_U  2.00 0.15  1.76  2.23\nsigma 1.02 0.05  0.94  1.10\n\nTakeaway\n\n1st reg - Not conditioning on U\n\nblocked the true effect between P and C\nWithout conditioning on U but conditioning on the collider P:\n\nall U’s influence on C was channeled through P which inflated P’s estimated effect\nP produced a spurious association between G and C which is seen in G’s estimated negative effect\n\n\n\n2nd reg - conditioning on U\n\nU’s association between P and C taken into account\n\ni.e. the path between P and C is unblocked which equals true effect\n\nP is no longer a collider and conditioning on P blocks G’s influence on C THROUGH P which allows only G’s true effect on C to be estimated.\n\n\n\n{brms} syntax and functions\n\nFunctions\n\nposterior_summary  just gives the estimates, errors, CIs in a matrix object\n\narg: brms model object\nWith {posterior}\n\n\n\nas_draws_rvars(brms_fit) %&gt;%   \n    summarize_draws()\nvariable mean median   sd mad   q5 q95  rhat ess_bulk ess_tail \nlp__       -38.56  -38.20 1.30 1.02  -41.09  -37.21    1    1880    2642 \nalpha_c         9.32 9.32 0.14 0.14 9.09 9.55    1    3364    2436 \nbeta         0.02 0.02 0.01 0.01 0.01 0.03    1    3864    2525 \nsigma         1.12 1.11 0.10 0.10 0.97 1.29    1    3014    2776\n\nDocs\nCan also compute CIs for convergence statistics (see Diagnostics, Bayes)\nposterior_samples (deprecated) extracts samples from the posterior for each parameter\n\nUse posterior::as_draws_* instead (matrix, df, rvars, etc.)\nextracts samples HMC chains\nalso outputs the log posterior, lp__ . For details:\n\nhttps://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2\nhttps://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient \n\n\nfitted gives the fitted values (same as SR link)\n\nbrms: “Expected Values of the Posterior Predictive Distribution”\nnewdata + summary = F takes new data and interpolates from the model.\nGenerates the 95% CIs for the fitted line by default\n\nColumn names are Q2.5 and Q97.5\nUse arg probs = c(.25, .75) to get different CI quantiles\n\n\npredict / posterior_predict - gives predicted values and PIs by default; “simulated values” (same as SR sim)\n\nbrms: “Draws from the Posterior Predictive Distribution”\nGenerates the 95% PIs for the predictions by default\n\nColumn names are Q2.5 and Q97.5\n\nFor details on all these predict function, also see\n\nHeiss article\nmc-stan forum post\n\nposterior_linpred  - predicted values from the non-transformed left side of the equation\n\nFor OLS, this output is the same as “posterior_predict”\nFor GLM, e.g. logistic regression w/logit link, outputs predictions in logits (aka log-odds)\n\nposterior_epred - draws of the expected value/mean of the posterior predictive distribution\n\nOutputs a distribution of means which shows the amount of uncertainty around the mean (doc)\nExample: Heiss notebook, uses add_epred_draws\n\n\n\npriors &lt;- c(prior(normal(60, 10), class = Intercept),\n            prior(lognormal(0, 1), class = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\nheight_weight_lognormal &lt;- brm(\n  bf(weight ~ 1 + height_z),\n  data = d,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, cores = 4, seed = BAYES_SEED\n)\ndraws_posterior_epred &lt;- tibble(height_z = seq(min(d$height_z), max(d$height_z), length.out = 100)) %&gt;%  # new data\n  add_epred_draws(height_weight_lognormal, ndraws = 50) %&gt;% \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\nggplot() +\n  geom_point(data = d, aes(x = height, y = weight), alpha = 0.5, size = 1) +\n  geom_line(data = draws_posterior_epred,\n            aes(x = height_unscaled, y = .epred, group = .draw), alpha = 0.2, color = clrs[6]) +\n  coord_cartesian(ylim = c(30, 65))\n\nmean_hdi - computes hdmi\n\nargs: posterior samples, .width = alpha value\n\ninits argument is for starting values in brms (Ch 4)\n\nEither “random” or “0”. If inits is “random” (the default), Stan will randomly generate initial values for parameters.\n\nIf “0”,  then all parameters are initialized to zero.\n“random” randomly selects the starting points from a uniform distribution ranging from -2 to 2\n\nThis option is sometimes useful for certain families, as it happens that default (“random”) inits cause samples to be essentially constant.\nWhen everything goes well, the MCMC chains will all have traversed from their starting values to sampling probabilistically from the posterior distribution once they have emerged from the warmup phase. Sometimes the chains get stuck around their stating values and continue to linger there, even after you have terminated the warmup period. When this happens, you’ll end up with samples that are still tainted by their starting values and are not yet representative of the posterior distribution.\n\nsetting inits = “0” is worth a try, if chains do not behave well.\nAlternatively, inits can be a list of lists containing the initial values, or a function (or function name) generating initial values. The latter options are mainly implemented for internal testing but are available to users if necessary. If specifying initial values using a list or a function then currently the parameter names must correspond to the names used in the generated Stan code (not the names used in R).\n\n\nFormula notation\n\nIncluding a | (bar) on the left side of a formula indicates we have extra supplementary information about our criterion\n\nChapters used: 11 (logistic and aggregated binomial regression)\n\nnl = TRUE means that nonlinear notation is being used\n\nChapters used: 11\n\n“0 + intercept”\n\nFor non-centered priors\nWhen you set your priors, brms applies them under the assumption your predictors are centered. If they’re not, consider the `0 + Intercept` syntax.\nKurz discussion\n\n\nIf all you want to do is fit the models, you wouldn’t have to add a separate prior() statement for each level of treatment. You could have just included a single line, prior(normal(0, 0.5), nlpar = b), that did not include a coef argument. The problem with this approach is we’d only get one column for treatment when using the prior_samples() function to retrieve the prior samples. To get separate columns for the prior samples of each of the levels of treatment, you need to take the verbose approach, above.\n\nprior = c(prior(normal(0, 1.5), nlpar = a),\n          prior(normal(0, 0.5), nlpar = b, coef = treatment1),\n          prior(normal(0, 0.5), nlpar = b, coef = treatment2),\n          prior(normal(0, 0.5), nlpar = b, coef = treatment3),\n          prior(normal(0, 0.5), nlpar = b, coef = treatment4)),\n{rethinking} functions\n\nextract_samples - gets samples from this multi-dimensional posterior\n\nargs: model, number of samples\n\nlink - generates a posterior distribution of expected values (same as brms fitted)\n\nThink it’s like a R predict function except it generates 2000 (default) predictions for each row of new data\nContinuous outcome - means\nBinary outcome - probabilities of an event\nargs: model, data\n\nprecis - model summary (mean, sd, CI for parameters)\n\nargs: model\n\nquap - Model fitting function that uses quadratic approximation to calculate the posterior\nsim - simulates data (same as brms predict)\n\nThink it’s like a R bootstrap function\ne.g. for a prediction interval\nargs: model, data\n\nStan code Examples\n\npg 343 logistic regression\npg 368-9 multinomial logistic regression\n\n\nBayes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Rethinking</span>"
    ]
  },
  {
    "objectID": "qmd/glossary.html",
    "href": "qmd/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Credible Interval - General Bayesian term that is interchangeable with confidence interval. Instead an interval of probability density or mass, it’s based on an interval of the posterior probability. If choice of interval (percentile or hdpi) affects inferences being made, then also report entire posterior distribution.\nDummy Data are simulated data (aka fake data) to take the place of real data.\nHighest Posterior Density Interval (HPDI) - The narrowest interval containing the specified probability mass. Guaranteed to have the value with the highest posterior probability.\nThe relative number of ways that a value p can produce the data is usually called a likelihood.\n\nIt is derived by the enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data (i.e. paths_consistent_with_data / total_paths).\nAs a model component, the likelihood is a function that gives the probability of an observation given a parameter value (conjecture)\n\n“How likely your sample data is out of all sample data of the same length?”\n\n\nMaximum a posteriori (MAP) - value with the highest posterior probability, aka mode of the posterior.\nA conjectured proportion of blue marbles, p, is usually called a parameter value. It’s just a way of indexing possible explanations of the data\n\nHere p, proportion of surface water (See example below), is the unknown parameter, but the conjecture could also be other things like sample size, treatment effect, group variation, etc.\nThere can also be multiple unknown parameters for the likelihood to consider.\nEvery parameter must have a corresponding prior probability assigned to it.\n\nThe new, updated relative plausibility of a specific p is called the posterior probability.\nThe set of estimates, aka relative plausibilities of different parameter values, aka posterior probabilities, conditional on the data — is known as the posterior distribution or posterior density (e.g. \\(Pr(p | N, W)\\)).\nThe prior plausibility of any specific p is usually called the prior probability.\n\nA distribution initial plausibilities for every value of a parameter\nExpresses prior knowledge about a parameter and constrains estimates to reasonable ranges\nUnless there’s already strong evidence for using a particular prior, multiple priors should be tried to see how sensitive the estimates are to the choice of a prior\nExample where the prior is a probability distribution for the parameter:\n\np is distributed Uniformly between 0 and 1, (i.e. each conjecture is equally likely), \\(p \\sim \\mbox{Uniform}(0, 1)\\)\n\n\n\nRegularizing priors - See Weakly Informative Priors\nWeakly Informative priors - conservative; guards against inferences of strong association - Mathematically equivalent to penalized likelihood",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "section": "",
    "text": "Attribution-NonCommercial 4.0 International"
  },
  {
    "objectID": "LICENSE.html#attribution-noncommercial-4.0-international",
    "href": "LICENSE.html#attribution-noncommercial-4.0-international",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n\nConsiderations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors.\nConsiderations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor’s permission is not necessary for any reason–for example, because of any applicable exception or limitation to copyright–then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "section": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "text": "Creative Commons Attribution-NonCommercial 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  }
]